{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-enginering of the definition of HMM for Address Segmentation\n",
    "- dataset_v4 directory for data\n",
    "- contry and countryCode in dataset are not used, skip in split_good_bad_dataset function\n",
    "- corrected the row 10330 in input dataset wholedataset_withshuffle.csv. AUTOSTRADA ROMA FIUMICINO becomes AUTOSTRADA ROMA-FIUMICINO to be coherent with the segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from hmmlearn.hmm import MultinomialHMM\n",
    "from hmm_library import HMM\n",
    "from hmm import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input_files has 3 represents 3 different ways to write each address in the same line\n",
    "# this function generates 3 lines for each input line\n",
    "def prepare_dataset(input_file, output_file):\n",
    "    '''\n",
    "    Service function to manage the new data format provided by DoDifferent,\n",
    "    splits each line in the input_file in 3 lines: \n",
    "    \n",
    "    r1;r2;r3;other fields\n",
    "    \n",
    "    in input files becomes\n",
    "    \n",
    "    r1;other fields\n",
    "    r2;other fields\n",
    "    r3;other fields\n",
    "    \n",
    "    into the output_file\n",
    "    ''' \n",
    "    if not os.path.exists(input_file):\n",
    "        print(\"Input dataset file not present\", input_file)\n",
    "        return\n",
    "    \n",
    "    of = open(output_file, 'w')\n",
    "    for line in open(input_file):\n",
    "        elements = line.split(';')\n",
    "        of.write(';'.join([elements[0]] + elements[3:]))\n",
    "        of.write(';'.join([elements[1]] + elements[3:]))\n",
    "        of.write(';'.join([elements[2]] + elements[3:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prepare_dataset('dataset_v4/wholedataset_withshuffle.csv', 'dataset_v4/wholedataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# index2tag dictionary reflects the data structure defined in Definizione Tracciato 01_00.xlsx\n",
    "# only records with returnCode=OK will be used for training (think about learning from negative examples in future)\n",
    "# the tags will be used as internal states of the HMM\n",
    "# country and countryCode will be discarded in the training set as they are fixed in Italy\n",
    "\n",
    "index2tag = { 0:'inputFullAddress', 1:'streetName', 2:'streetType', 3:'houseNumber', 4:'postalCode', \n",
    "              5:'country', 6:'region', 7:'province', 8:'city', 9:'area', 10:'countryCode', 11:'regionCode',\n",
    "              12:'provCode', 13:'cityCode', 14:'areaCode', 15:'returnCode'}\n",
    "\n",
    "# search_priority is a map used to give priority to tags in the processing of each word of the inputFullAddress\n",
    "# keeping the order of the columns in the input file (don't want to reorder the columns)\n",
    "# basically to follow the priority order: area, city, prov, region\n",
    "# 15:15 is necessary to avoid bug with case limit in the loop\n",
    "search_priority = { 0:0, 1:1, 2:2, 3:3, 4:4, 5:9, 6:8, 7:7, 8:6, 9:5, 10:14, 11:13, 12:12, 13:11, 14:10, 15:15 }\n",
    "\n",
    "# output_file is the output in the format needed for processing in HMM\n",
    "# check return code OK-KO has been added in this function\n",
    "# ISSUE TO SOLVE: ROMA RM AUTOSTRADA ROMA AEROPORTO DI FIUMICINO .....\n",
    "def split_good_bad_dataset(input_file, good_file, bad_file, output_file):\n",
    "    '''\n",
    "    Service function to create the output_file in the format needed for the training of HMM\n",
    "    the original input_file is split in 2 files: good_file and bad_file\n",
    "    the good_file contains the rows that have been used to generate the output_file\n",
    "    the bad_file contains the rows not usable for training\n",
    "    further analysis could expand the good_file (and the output_file) including more records\n",
    "    '''\n",
    "    if not os.path.exists(input_file):\n",
    "        print(\"Input dataset file not present\", input_file)\n",
    "        return\n",
    "        \n",
    "    count_bad = 0\n",
    "    count_good = 0\n",
    "    \n",
    "    gf = open(good_file, 'w')\n",
    "    bf = open(bad_file, 'w')\n",
    "    of = open(output_file, 'w')\n",
    "    \n",
    "    for line in open(input_file):\n",
    "        elements = line.split(';')\n",
    "        \n",
    "        # malformed record\n",
    "        if len(elements) < 16:\n",
    "            count_bad += 1\n",
    "            bf.write(line)\n",
    "            continue\n",
    "        \n",
    "        # only records with returnCode OK can be used for training\n",
    "        if elements[15] != 'OK':\n",
    "            count_bad += 1\n",
    "            bf.write(line)\n",
    "            continue\n",
    "        \n",
    "        inputFullAddressAsList = re.split(r'\\s+', elements[0].strip())\n",
    "        n_tokens = len(inputFullAddressAsList)\n",
    "        \n",
    "        buffer_to_write = ''\n",
    "        for i in range(n_tokens):\n",
    "            token = inputFullAddressAsList[i]\n",
    "            for j in range(1, 16):\n",
    "                k = search_priority[j]\n",
    "                # avoid country (index 5) and countryCode (index 10)\n",
    "                if (k != 5) and (k != 10) and (token in re.split(r'\\s+', elements[k].strip())):\n",
    "                    buffer_to_write = buffer_to_write + token + ' ' + index2tag[k] + '\\n'\n",
    "                    break\n",
    "            if j == 15:\n",
    "                count_bad += 1\n",
    "                bf.write(line)\n",
    "                break\n",
    "            elif i == n_tokens-1:\n",
    "                of.write(buffer_to_write + '\\n')\n",
    "                gf.write(line)\n",
    "                count_good += 1\n",
    "    \n",
    "    \n",
    "    print('Number of bad lines', count_bad)\n",
    "    print('Number of good lines', count_good)\n",
    "    \n",
    "    gf.close()\n",
    "    bf.close()\n",
    "    of.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bad lines 73330\n",
      "Number of good lines 298670\n"
     ]
    }
   ],
   "source": [
    "split_good_bad_dataset('dataset_v4/wholedataset.csv', 'dataset_v4/training_RM_OK_good.csv', 'dataset_v4/training_RM_OK_bad.csv', \n",
    "                       'dataset_v4/training_hmm.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_dataset(filename, split_sequences=False):\n",
    "    if not os.path.exists(filename):\n",
    "        print(\"Input dataset file not present\", filename)\n",
    "        exit()\n",
    "\n",
    "    word2idx = {}\n",
    "    tag2idx = {}\n",
    "    word_idx = 0\n",
    "    tag_idx = 0\n",
    "    X = []\n",
    "    Y = []\n",
    "    currentX = []\n",
    "    currentY = []\n",
    "    \n",
    "    # placeholder to manage unknown words in actual addresses for prediction\n",
    "    word2idx['UNK'] = word_idx\n",
    "    word_idx += 1\n",
    "    \n",
    "    # 4 placeholders to manage 5 cypher numbers (e.g. ZIP codes) in input textual address\n",
    "    for i in range(4):\n",
    "        word2idx['N5_' + str(i)] = word_idx\n",
    "        word_idx += 1\n",
    "        \n",
    "    # 4 placeholders to manage other numbers (e.g. ZIP codes) in input textual address\n",
    "    for i in range(4):\n",
    "        word2idx['N_' + str(i)] = word_idx\n",
    "        word_idx += 1\n",
    "    \n",
    "    number_5cypher = 0 # counter to instanciate N5 placeholders found in the actual sequence\n",
    "    number_other = 0 # counter to instanciate N placeholders found in the actual sequence\n",
    "    \n",
    "    for line in open(filename):\n",
    "        line = line.rstrip()\n",
    "        if line:\n",
    "            r = line.split()\n",
    "            \n",
    "            word, tag = r\n",
    "            \n",
    "            # F.P. decided to manage lowerized\n",
    "            word = word.lower()\n",
    "    \n",
    "            # F.P. added to manage numbers\n",
    "            if re.match(r'^\\d{5}$', word):\n",
    "                word = 'N5_' + str(number_5cypher)\n",
    "                number_5cypher += 1\n",
    "            elif re.match(r'^\\d+$', word):\n",
    "                word = 'N_' + str(number_other)\n",
    "                number_other += 1\n",
    "    \n",
    "            if word not in word2idx:\n",
    "                word2idx[word] = word_idx\n",
    "                word_idx += 1\n",
    "            currentX.append(word2idx[word])\n",
    "            \n",
    "            if tag not in tag2idx:\n",
    "                tag2idx[tag] = tag_idx\n",
    "                tag_idx += 1\n",
    "            currentY.append(tag2idx[tag])\n",
    "            \n",
    "        elif split_sequences:\n",
    "            X.append(currentX)\n",
    "            Y.append(currentY)\n",
    "            currentX = []\n",
    "            currentY = []\n",
    "            number_5cypher = 0 #F.P. reinitializes counters\n",
    "            number_other = 0\n",
    "\n",
    "    if not split_sequences:\n",
    "        X = currentX\n",
    "        Y = currentY\n",
    "        \n",
    "    print('The TAG dictionary is')\n",
    "    print(tag2idx)\n",
    "\n",
    "    return X, Y, word2idx, tag2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(T, Y):\n",
    "    # inputs are lists of lists\n",
    "    n_correct = 0\n",
    "    n_total = 0\n",
    "    for t, y in zip(T, Y):\n",
    "        n_correct += np.sum(t == y)\n",
    "        n_total += len(y)\n",
    "        \n",
    "    print('N correct =', n_correct)\n",
    "    print('N total =', n_total)\n",
    "    return float(n_correct) / n_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def total_f1_score(T, Y):\n",
    "    # inputs are lists of lists\n",
    "    T = np.concatenate(T)\n",
    "    Y = np.concatenate(Y)\n",
    "    print('T =', T)\n",
    "    print('Y =', Y)\n",
    "    print('Detailed f1: ', f1_score(T, Y, average=None))\n",
    "    \n",
    "    return f1_score(T, Y, average=None).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def total_recall_score(T, Y):\n",
    "    # inputs are lists of lists\n",
    "    T = np.concatenate(T)\n",
    "    Y = np.concatenate(Y)\n",
    "    print('T =', T)\n",
    "    print('Y =', Y)\n",
    "    print('Detailed Recall: ', recall_score(T, Y, average=None))\n",
    "    return recall_score(T, Y, average=None).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def total_f1_kfold(T, Y):\n",
    "    # inputs are lists of lists\n",
    "    T = np.concatenate(T)\n",
    "    Y = np.concatenate(Y)\n",
    "    print('T =', T)\n",
    "    print('Y =', Y)\n",
    "    print('Detailed f1: ', f1_score(T, Y, average=None))\n",
    "    \n",
    "    return f1_score(T, Y, average=None).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def total_precision_score(T, Y):\n",
    "    # inputs are lists of lists\n",
    "    T = np.concatenate(T)\n",
    "    Y = np.concatenate(Y)\n",
    "    print('T =', T)\n",
    "    print('Y =', Y)\n",
    "    print('Detailed precision: ', precision_score(T, Y, average=None))\n",
    "    return precision_score(T, Y, average=None).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(\"Test Accuracy: {}\".format(accuracy(Y, X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_hmm_model_kfold(smoothing=1e-3):\n",
    "    X, Y, word2idx, tag2idx = load_dataset('dataset_v4/training_hmm.csv', split_sequences=True)\n",
    "    X, Y = shuffle(X, Y)\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    print('Samples in whole dataset =', len(X), len(Y))\n",
    "    V = len(word2idx)\n",
    "    print('V =', V)\n",
    "    print()\n",
    "\n",
    "    # find hidden state transition matrix and pi\n",
    "    M = len(tag2idx)\n",
    "    print('M =', M)\n",
    "    \n",
    "    A = np.ones((M, M))#*smoothing # add-one smoothing\n",
    "    pi = np.zeros(M)\n",
    "    \n",
    "    # observation matrix\n",
    "    B = np.ones((M, V))*smoothing # add-one smoothing\n",
    "    #print(B)\n",
    "    kf = KFold(n_splits = 5, shuffle = True)\n",
    "    sum_acc_train = 0\n",
    "    sum_acc_test = 0\n",
    "    sum_f1_score_train = 0\n",
    "    sum_f1_score_test = 0\n",
    "    counter = 0\n",
    "    for train_ind, test_ind in kf.split(X):\n",
    "        Xtrain, Xtest = X[train_ind], X[test_ind]\n",
    "        Ytrain, Ytest = Y[train_ind], Y[test_ind]\n",
    "        \n",
    "        for y in Ytrain:\n",
    "            pi[y[0]] += 1\n",
    "            for i in range(len(y)-1):\n",
    "                A[y[i], y[i+1]] += 1\n",
    "        \n",
    "        A /= A.sum(axis=1, keepdims=True)\n",
    "        pi /= pi.sum()\n",
    "        \n",
    "        \n",
    "        for x, y in zip(Xtrain, Ytrain):\n",
    "            for xi, yi in zip(x, y):\n",
    "                B[yi, xi] += 1    # Do Absolute discounting here \n",
    "            #else:\n",
    "            #    B[yi, xi] += 0.01\n",
    "        B /= B.sum(axis=1, keepdims=True)\n",
    "        hmm = MultinomialHMM(M, verbose = False, n_iter = 1, tol = .1)\n",
    "        hmm.startprob_ = pi\n",
    "        hmm.transmat_ = A\n",
    "        hmm.emissionprob_ = B\n",
    "        \n",
    "        Ptrain = []\n",
    "        for x in Xtrain:\n",
    "            p = (hmm.decode(np.array([x]).T))[1] # F.P. this method can be used to tag a new address\n",
    "            Ptrain.append(p)\n",
    "            #print(len(Ptrain))\n",
    "        Ptest = []\n",
    "        for x in Xtest:\n",
    "            p = (hmm.decode(np.array([x]).T))[1]\n",
    "            Ptest.append(p)\n",
    "        # print results\n",
    "        sum_acc_train += accuracy(Ytrain, Ptrain)\n",
    "        sum_acc_test += accuracy(Ytest, Ptest)\n",
    "        sum_f1_score_train += total_f1_score(Ytrain, Ptrain)\n",
    "        sum_f1_score_test += total_f1_score(Ytest, Ptest)\n",
    "        counter += 1\n",
    "        print(\"RUN NOW IS: {} OF 10\".format(counter))\n",
    "    print(\"train accuracy: {}\".format(sum_acc_train/5))\n",
    "    print(\"test accuracy: {}\".format(sum_acc_test/5))\n",
    "    print(\"train F1_score: {}\".format(sum_f1_score_train/5))\n",
    "    print(\"test F1_score: {}\".format(sum_f1_score_test/5))\n",
    "    \n",
    "    return hmm, word2idx, tag2idx      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "Samples in whole dataset = 298670 298670\n",
      "V = 13745\n",
      "\n",
      "M = 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gift/anaconda3/lib/python3.6/site-packages/hmmlearn/base.py:451: RuntimeWarning: divide by zero encountered in log\n",
      "  n_samples, n_components, np.log(self.startprob_),\n"
     ]
    }
   ],
   "source": [
    "address_segmentator, word2idx, tag2idx = create_hmm_model_kfold()\n",
    "#address_segmentator, word2idx, tag2idx = create_hmm_model_kfold_sm(smoothing=1e-1)\n",
    "#model, word2idx, tag2idx = load_dataset('dataset_v4/training_hmm.csv', split_sequences=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9932\n"
     ]
    }
   ],
   "source": [
    "c1 = 0\n",
    "c2 = 0\n",
    "for x, y in zip(X, Y):\n",
    "    #print(4)\n",
    "    x = set(x)\n",
    "    y = set(y)\n",
    "    if(len(x-y) == 0):\n",
    "        c1 = c1+1\n",
    "    else:\n",
    "        c2 = c2+1\n",
    "print(c1/len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#address_segmentator, word2idx, tag2idx = create_hmm_model()\n",
    "#print(word2idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tag2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def segment(model, word2index, tag2index, address):\n",
    "    \n",
    "    index2tag = {}\n",
    "    for tag in tag2index:\n",
    "        index2tag[tag2index[tag]] = tag\n",
    "    \n",
    "    number_5cypher = 0 # counter to instanciate N5 placeholders found in the actual sequence\n",
    "    number_other = 0 # counter to instanciate N placeholders found in the actual sequence\n",
    "    \n",
    "    address_words = re.split(r'\\s+', address.strip())\n",
    "\n",
    "    x = []\n",
    "    for word in address_words:\n",
    "        word = word.lower()\n",
    "        if word in word2index:\n",
    "            x.append(word2index[word])\n",
    "        elif re.match(r'^\\d{5}$', word):\n",
    "            x.append(word2index['N5_' + str(number_5cypher)])\n",
    "            number_5cypher +=1\n",
    "        elif re.match(r'^\\d+$', word):\n",
    "            x.append(word2index['N_' + str(number_other)])\n",
    "            number_other += 1\n",
    "        else:     \n",
    "            x.append(word2index['UNK'])\n",
    "    \n",
    "    print(x)\n",
    "    \n",
    "    state_sequence = (model.decode(np.array([x]).T))[1]\n",
    "    print(state_sequence)\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        print(address_words[i], index2tag[state_sequence[i]])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['via', 'delle', 'archeologia', '29', '00133', 'RM']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split(r'\\s+', 'via delle archeologia 29 00133 RM'.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 50, 0, 5, 1, 13]\n",
      "[1 2 6 0 3 5]\n",
      "via streetType\n",
      "del streetName\n",
      "archeologia area\n",
      "29 houseNumber\n",
      "00133 postalCode\n",
      "RM provCode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gift/anaconda3/lib/python3.6/site-packages/hmmlearn/base.py:451: RuntimeWarning: divide by zero encountered in log\n",
      "  n_samples, n_components, np.log(self.startprob_),\n"
     ]
    }
   ],
   "source": [
    "segment(address_segmentator, word2idx, tag2idx, 'via del archeologia 29 00133 RM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_v4/address_segmentator.pkl']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(address_segmentator, 'model_v4/address_segmentator.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_v4/word2idx.pkl']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(word2idx, 'model_v4/word2idx.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_v4/tag2idx.pkl']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(tag2idx, 'model_v4/tag2idx.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-28 13:01:36,109 - __main__ - INFO - Address Segmentator: creating endpoints\n",
      "2018-11-28 13:01:36,110 - __main__ - INFO - Address segmentator: loading config\n",
      "2018-11-28 13:01:36,293 - __main__ - INFO - Address Segmentator: starting\n",
      "2018-11-28 13:01:36,453 - werkzeug - INFO -  * Running on http://127.0.0.1:5002/ (Press CTRL+C to quit)\n",
      "2018-11-28 13:01:36,479 - werkzeug - INFO -  * Restarting with stat\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gift/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import codecs\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# logging object is shared between module, the first initialization is effective\n",
    "logging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from flask import Flask, jsonify, request\n",
    "\n",
    "LOG_FILE = './Address_Segmentator.log' # Not used yet\n",
    "\n",
    "application = Flask(__name__)\n",
    "\n",
    "def inference():\n",
    "  logger.info('Address segmentator: loading config')\n",
    "\n",
    "  address_segmentator = joblib.load('model_v4/address_segmentator.pkl')\n",
    "  word2index = joblib.load('model_v4/word2idx.pkl')\n",
    "  tag2index = joblib.load('model_v4/tag2idx.pkl')\n",
    "\n",
    "  index2tag = {}\n",
    "  for tag in tag2index:\n",
    "    index2tag[tag2index[tag]] = tag\n",
    "\n",
    "  def segment(address):\n",
    "    number_5cypher = 0 # counter to instanciate N5 placeholders found in the actual sequence\n",
    "    number_other = 0 # counter to instanciate N placeholders found in the actual sequence\n",
    "    address_words = re.split(r'\\s+', address.strip())\n",
    "\n",
    "    x = []\n",
    "    for word in address_words:\n",
    "      word = word.lower()\n",
    "      if word in word2index:\n",
    "        x.append(word2index[word])\n",
    "      elif re.match(r'^\\d{5}$', word):\n",
    "        x.append(word2index['N5_' + str(number_5cypher)])\n",
    "        number_5cypher +=1\n",
    "      elif re.match(r'^\\d+$', word):\n",
    "        x.append(word2index['N_' + str(number_other)])\n",
    "        number_other += 1\n",
    "      else:\n",
    "        x.append(word2index['UNK'])\n",
    "\n",
    "    state_sequence = (address_segmentator.decode(np.array([x]).T))[1]\n",
    "\n",
    "    segmented_address = {}\n",
    "    for i in range(len(x)):\n",
    "      segmented_address[index2tag[state_sequence[i]]] = segmented_address.get(index2tag[state_sequence[i]], '') + address_words[i] + ' '\n",
    "\n",
    "    return segmented_address\n",
    "\n",
    "  @application.route('/decode/v0.0', methods=['GET'])\n",
    "  def decode():\n",
    "    query = str(request.args.get('query')) # str convert None as string\n",
    "    \n",
    "    return jsonify(segment(query))\n",
    "\n",
    "  @application.route('/nl2sql/v0.0/set_log_level', methods=['GET'])\n",
    "  def set_log_level():\n",
    "    log_level = request.args.get('log_level')\n",
    "    if log_level == 'INFO':\n",
    "      logger.setLevel(logging.INFO)\n",
    "      return jsonify('Nl2sql: log level set to INFO')\n",
    "    elif log_level == 'DEBUG':\n",
    "      logger.setLevel(logging.DEBUG)\n",
    "      return jsonify('Nl2sql: log level set to DEBUG')\n",
    "\n",
    "    return jsonify('Nl2sql: unsupported or missing log level')\n",
    "\n",
    "logger.info('Address Segmentator: creating endpoints')\n",
    "inference()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  logger.info('Address Segmentator: starting')\n",
    "  application.run(host='127.0.0.1', port=5002, debug=True, use_reloader=True)\n",
    "  #application.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "total_series_length = 298670\n",
    "truncated_backprop_length = 15\n",
    "state_size = 4\n",
    "num_classes = 7\n",
    "echo_step = 3\n",
    "batch_size = 5\n",
    "num_batches = total_series_length//batch_size//truncated_backprop_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generateData():\n",
    "    X, Y, word2idx, tag2idx = load_dataset('dataset_v4/training_hmm.csv', split_sequences=True)\n",
    "    \n",
    "    X =np.array([x[0] for x in X])\n",
    "    \n",
    "    Y = np.array([x[0] for x in Y])\n",
    "    X = X.reshape((batch_size, -1))  # The first index changing slowest, subseries as rows\n",
    "    Y = Y.reshape((batch_size, -1)) \n",
    "    return (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "X, Y = generateData()\n",
    "print(len(Y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder_44:0\", shape=(5, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "batchX_placeholder = tf.placeholder(tf.float32, [batch_size, truncated_backprop_length])\n",
    "batchY_placeholder = tf.placeholder(tf.int32, [batch_size, truncated_backprop_length])\n",
    "\n",
    "init_state = tf.placeholder(tf.float32, [batch_size, state_size])\n",
    "print(init_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(np.random.rand(state_size+1, state_size), dtype=tf.float32)\n",
    "b = tf.Variable(np.zeros((1,state_size)), dtype=tf.float32)\n",
    "\n",
    "W2 = tf.Variable(np.random.rand(state_size, num_classes),dtype=tf.float32)\n",
    "b2 = tf.Variable(np.zeros((1,num_classes)), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Unpack columns\n",
    "inputs_series = tf.unstack(batchX_placeholder, axis=1)\n",
    "labels_series = tf.unstack(batchY_placeholder, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "current_state = init_state\n",
    "states_series = []\n",
    "for current_input in inputs_series:\n",
    "    current_input = tf.reshape(current_input, [batch_size, 1])\n",
    "    input_and_state_concatenated = tf.concat([current_input, current_state], 1)  # Increasing number of columns\n",
    "\n",
    "    next_state = tf.tanh(tf.matmul(input_and_state_concatenated, W) + b)  # Broadcasted addition\n",
    "    states_series.append(next_state)\n",
    "    current_state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits_series = [tf.matmul(state, W2) + b2 for state in states_series] #Broadcasted addition\n",
    "predictions_series = [tf.nn.softmax(logits) for logits in logits_series]\n",
    "\n",
    "losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels) for logits, labels in zip(logits_series,labels_series)]\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "\n",
    "train_step = tf.train.AdagradOptimizer(0.3).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot(loss_list, predictions_series, batchX, batchY):\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.cla()\n",
    "    plt.plot(loss_list)\n",
    "\n",
    "    for batch_series_idx in range(5):\n",
    "        one_hot_output_series = np.array(predictions_series)[:, batch_series_idx, :]\n",
    "        single_output_series = np.array([(1 if out[0] < 0.5 else 0) for out in one_hot_output_series])\n",
    "\n",
    "        plt.subplot(2, 3, batch_series_idx + 2)\n",
    "        plt.cla()\n",
    "        plt.axis([0, truncated_backprop_length, 0, 2])\n",
    "        left_offset = range(truncated_backprop_length)\n",
    "        plt.bar(left_offset, batchX[batch_series_idx, :], width=1, color=\"blue\")\n",
    "        plt.bar(left_offset, batchY[batch_series_idx, :] * 0.5, width=1, color=\"red\")\n",
    "        plt.bar(left_offset, single_output_series * 0.3, width=1, color=\"green\")\n",
    "\n",
    "    plt.draw()\n",
    "plt.pause(0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8ed945f358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 0\n",
      "Step 0 Loss 1.71384\n",
      "Step 100 Loss 1.10768\n",
      "Step 200 Loss 1.09784\n",
      "Step 300 Loss 1.10796\n",
      "Step 400 Loss 1.101\n",
      "Step 500 Loss 1.10591\n",
      "Step 600 Loss 1.38184\n",
      "Step 700 Loss 1.10777\n",
      "Step 800 Loss 1.10117\n",
      "Step 900 Loss 1.1005\n",
      "Step 1000 Loss 1.10475\n",
      "Step 1100 Loss 1.1018\n",
      "Step 1200 Loss 1.09618\n",
      "Step 1300 Loss 1.09976\n",
      "Step 1400 Loss 1.1025\n",
      "Step 1500 Loss 1.10267\n",
      "Step 1600 Loss 1.09934\n",
      "Step 1700 Loss 1.10042\n",
      "Step 1800 Loss 1.10039\n",
      "Step 1900 Loss 1.10033\n",
      "Step 2000 Loss 1.09229\n",
      "Step 2100 Loss 0.421653\n",
      "Step 2200 Loss 0.507995\n",
      "Step 2300 Loss 0.481491\n",
      "Step 2400 Loss 0.367996\n",
      "Step 2500 Loss 0.282211\n",
      "Step 2600 Loss 0.505718\n",
      "Step 2700 Loss 0.425336\n",
      "Step 2800 Loss 0.263565\n",
      "Step 2900 Loss 0.319644\n",
      "Step 3000 Loss 0.489506\n",
      "Step 3100 Loss 0.22281\n",
      "Step 3200 Loss 0.235785\n",
      "Step 3300 Loss 0.465532\n",
      "Step 3400 Loss 0.140014\n",
      "Step 3500 Loss 0.261344\n",
      "Step 3600 Loss 0.163634\n",
      "Step 3700 Loss 0.491806\n",
      "Step 3800 Loss 0.0598837\n",
      "Step 3900 Loss 0.0558109\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 1\n",
      "Step 0 Loss 0.940465\n",
      "Step 100 Loss 0.0509575\n",
      "Step 200 Loss 0.0474712\n",
      "Step 300 Loss 0.0789259\n",
      "Step 400 Loss 0.0418395\n",
      "Step 500 Loss 0.257564\n",
      "Step 600 Loss 0.547385\n",
      "Step 700 Loss 0.230052\n",
      "Step 800 Loss 0.361834\n",
      "Step 900 Loss 0.193972\n",
      "Step 1000 Loss 0.194284\n",
      "Step 1100 Loss 0.0946697\n",
      "Step 1200 Loss 0.0647254\n",
      "Step 1300 Loss 0.0939346\n",
      "Step 1400 Loss 0.185857\n",
      "Step 1500 Loss 0.0197637\n",
      "Step 1600 Loss 0.0147676\n",
      "Step 1700 Loss 0.0121702\n",
      "Step 1800 Loss 0.0098117\n",
      "Step 1900 Loss 0.0192289\n",
      "Step 2000 Loss 0.25666\n",
      "Step 2100 Loss 0.0122334\n",
      "Step 2200 Loss 0.0128112\n",
      "Step 2300 Loss 0.0115431\n",
      "Step 2400 Loss 0.0379171\n",
      "Step 2500 Loss 0.300855\n",
      "Step 2600 Loss 0.259544\n",
      "Step 2700 Loss 0.190914\n",
      "Step 2800 Loss 0.0177494\n",
      "Step 2900 Loss 0.0151896\n",
      "Step 3000 Loss 0.211777\n",
      "Step 3100 Loss 0.185238\n",
      "Step 3200 Loss 0.2043\n",
      "Step 3300 Loss 0.358665\n",
      "Step 3400 Loss 0.0218063\n",
      "Step 3500 Loss 0.26485\n",
      "Step 3600 Loss 0.086732\n",
      "Step 3700 Loss 0.32251\n",
      "Step 3800 Loss 0.0169932\n",
      "Step 3900 Loss 0.0137031\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 2\n",
      "Step 0 Loss 0.544375\n",
      "Step 100 Loss 0.0182632\n",
      "Step 200 Loss 0.0131719\n",
      "Step 300 Loss 0.0110903\n",
      "Step 400 Loss 0.0095919\n",
      "Step 500 Loss 0.298547\n",
      "Step 600 Loss 0.371188\n",
      "Step 700 Loss 0.121246\n",
      "Step 800 Loss 0.227242\n",
      "Step 900 Loss 0.18642\n",
      "Step 1000 Loss 0.0193017\n",
      "Step 1100 Loss 0.0110607\n",
      "Step 1200 Loss 0.00829529\n",
      "Step 1300 Loss 0.00729686\n",
      "Step 1400 Loss 0.186468\n",
      "Step 1500 Loss 0.0176855\n",
      "Step 1600 Loss 0.00949815\n",
      "Step 1700 Loss 0.00820062\n",
      "Step 1800 Loss 0.00613235\n",
      "Step 1900 Loss 0.00539711\n",
      "Step 2000 Loss 0.00570101\n",
      "Step 2100 Loss 0.00466082\n",
      "Step 2200 Loss 0.00497831\n",
      "Step 2300 Loss 0.00544156\n",
      "Step 2400 Loss 0.00581117\n",
      "Step 2500 Loss 0.120987\n",
      "Step 2600 Loss 0.18565\n",
      "Step 2700 Loss 0.134374\n",
      "Step 2800 Loss 0.0104762\n",
      "Step 2900 Loss 0.0140749\n",
      "Step 3000 Loss 0.173617\n",
      "Step 3100 Loss 0.0059755\n",
      "Step 3200 Loss 0.189689\n",
      "Step 3300 Loss 0.348877\n",
      "Step 3400 Loss 0.0175813\n",
      "Step 3500 Loss 0.297671\n",
      "Step 3600 Loss 0.0793154\n",
      "Step 3700 Loss 0.313217\n",
      "Step 3800 Loss 0.012239\n",
      "Step 3900 Loss 0.0100249\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 3\n",
      "Step 0 Loss 0.468629\n",
      "Step 100 Loss 0.0143763\n",
      "Step 200 Loss 0.0108945\n",
      "Step 300 Loss 0.00811351\n",
      "Step 400 Loss 0.0071724\n",
      "Step 500 Loss 0.317841\n",
      "Step 600 Loss 0.365632\n",
      "Step 700 Loss 0.112674\n",
      "Step 800 Loss 0.218184\n",
      "Step 900 Loss 0.185155\n",
      "Step 1000 Loss 0.0175663\n",
      "Step 1100 Loss 0.00925924\n",
      "Step 1200 Loss 0.00632119\n",
      "Step 1300 Loss 0.00568188\n",
      "Step 1400 Loss 0.185851\n",
      "Step 1500 Loss 0.0180613\n",
      "Step 1600 Loss 0.00897569\n",
      "Step 1700 Loss 0.00759484\n",
      "Step 1800 Loss 0.00532561\n",
      "Step 1900 Loss 0.00457287\n",
      "Step 2000 Loss 0.00497691\n",
      "Step 2100 Loss 0.0037677\n",
      "Step 2200 Loss 0.0044364\n",
      "Step 2300 Loss 0.00483384\n",
      "Step 2400 Loss 0.00528357\n",
      "Step 2500 Loss 0.118849\n",
      "Step 2600 Loss 0.184395\n",
      "Step 2700 Loss 0.136171\n",
      "Step 2800 Loss 0.00995003\n",
      "Step 2900 Loss 0.0137816\n",
      "Step 3000 Loss 0.171701\n",
      "Step 3100 Loss 0.00557515\n",
      "Step 3200 Loss 0.193443\n",
      "Step 3300 Loss 0.34295\n",
      "Step 3400 Loss 0.0172742\n",
      "Step 3500 Loss 0.301651\n",
      "Step 3600 Loss 0.0795578\n",
      "Step 3700 Loss 0.298672\n",
      "Step 3800 Loss 0.0120434\n",
      "Step 3900 Loss 0.0100189\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 4\n",
      "Step 0 Loss 0.526595\n",
      "Step 100 Loss 0.0118276\n",
      "Step 200 Loss 0.0099877\n",
      "Step 300 Loss 0.0075219\n",
      "Step 400 Loss 0.00708857\n",
      "Step 500 Loss 0.318821\n",
      "Step 600 Loss 0.374115\n",
      "Step 700 Loss 0.110817\n",
      "Step 800 Loss 0.215852\n",
      "Step 900 Loss 0.185151\n",
      "Step 1000 Loss 0.0179292\n",
      "Step 1100 Loss 0.00915522\n",
      "Step 1200 Loss 0.0061404\n",
      "Step 1300 Loss 0.00558854\n",
      "Step 1400 Loss 0.185843\n",
      "Step 1500 Loss 0.0189156\n",
      "Step 1600 Loss 0.009196\n",
      "Step 1700 Loss 0.00773475\n",
      "Step 1800 Loss 0.00534711\n",
      "Step 1900 Loss 0.00454415\n",
      "Step 2000 Loss 0.00494711\n",
      "Step 2100 Loss 0.0036207\n",
      "Step 2200 Loss 0.00438207\n",
      "Step 2300 Loss 0.00472586\n",
      "Step 2400 Loss 0.00514102\n",
      "Step 2500 Loss 0.118999\n",
      "Step 2600 Loss 0.185475\n",
      "Step 2700 Loss 0.139348\n",
      "Step 2800 Loss 0.0097012\n",
      "Step 2900 Loss 0.0135219\n",
      "Step 3000 Loss 0.17065\n",
      "Step 3100 Loss 0.00551796\n",
      "Step 3200 Loss 0.197191\n",
      "Step 3300 Loss 0.339531\n",
      "Step 3400 Loss 0.0178565\n",
      "Step 3500 Loss 0.301994\n",
      "Step 3600 Loss 0.0800442\n",
      "Step 3700 Loss 0.306011\n",
      "Step 3800 Loss 0.0122283\n",
      "Step 3900 Loss 0.0102039\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 5\n",
      "Step 0 Loss 0.54748\n",
      "Step 100 Loss 0.0110772\n",
      "Step 200 Loss 0.00973149\n",
      "Step 300 Loss 0.0071154\n",
      "Step 400 Loss 0.00704485\n",
      "Step 500 Loss 0.319949\n",
      "Step 600 Loss 0.380933\n",
      "Step 700 Loss 0.109641\n",
      "Step 800 Loss 0.214813\n",
      "Step 900 Loss 0.185547\n",
      "Step 1000 Loss 0.0185002\n",
      "Step 1100 Loss 0.00932705\n",
      "Step 1200 Loss 0.00619414\n",
      "Step 1300 Loss 0.00566844\n",
      "Step 1400 Loss 0.186154\n",
      "Step 1500 Loss 0.0197501\n",
      "Step 1600 Loss 0.00951044\n",
      "Step 1700 Loss 0.00797202\n",
      "Step 1800 Loss 0.00549899\n",
      "Step 1900 Loss 0.0046347\n",
      "Step 2000 Loss 0.00499669\n",
      "Step 2100 Loss 0.00361667\n",
      "Step 2200 Loss 0.00441718\n",
      "Step 2300 Loss 0.00472647\n",
      "Step 2400 Loss 0.00510881\n",
      "Step 2500 Loss 0.119109\n",
      "Step 2600 Loss 0.186562\n",
      "Step 2700 Loss 0.142343\n",
      "Step 2800 Loss 0.00948408\n",
      "Step 2900 Loss 0.0133051\n",
      "Step 3000 Loss 0.170105\n",
      "Step 3100 Loss 0.00552052\n",
      "Step 3200 Loss 0.201043\n",
      "Step 3300 Loss 0.336595\n",
      "Step 3400 Loss 0.0184691\n",
      "Step 3500 Loss 0.30234\n",
      "Step 3600 Loss 0.0805249\n",
      "Step 3700 Loss 0.311241\n",
      "Step 3800 Loss 0.012441\n",
      "Step 3900 Loss 0.0103873\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 6\n",
      "Step 0 Loss 0.554412\n",
      "Step 100 Loss 0.0109076\n",
      "Step 200 Loss 0.00969515\n",
      "Step 300 Loss 0.00693314\n",
      "Step 400 Loss 0.0070691\n",
      "Step 500 Loss 0.321578\n",
      "Step 600 Loss 0.385649\n",
      "Step 700 Loss 0.108714\n",
      "Step 800 Loss 0.214079\n",
      "Step 900 Loss 0.186048\n",
      "Step 1000 Loss 0.0190495\n",
      "Step 1100 Loss 0.0095829\n",
      "Step 1200 Loss 0.00632673\n",
      "Step 1300 Loss 0.00581714\n",
      "Step 1400 Loss 0.186658\n",
      "Step 1500 Loss 0.0205228\n",
      "Step 1600 Loss 0.00984806\n",
      "Step 1700 Loss 0.00823716\n",
      "Step 1800 Loss 0.00569019\n",
      "Step 1900 Loss 0.00476905\n",
      "Step 2000 Loss 0.00508812\n",
      "Step 2100 Loss 0.00365982\n",
      "Step 2200 Loss 0.00448746\n",
      "Step 2300 Loss 0.0047671\n",
      "Step 2400 Loss 0.00512313\n",
      "Step 2500 Loss 0.119179\n",
      "Step 2600 Loss 0.187499\n",
      "Step 2700 Loss 0.145025\n",
      "Step 2800 Loss 0.00928751\n",
      "Step 2900 Loss 0.0131292\n",
      "Step 3000 Loss 0.169811\n",
      "Step 3100 Loss 0.00554387\n",
      "Step 3200 Loss 0.204793\n",
      "Step 3300 Loss 0.333929\n",
      "Step 3400 Loss 0.0190228\n",
      "Step 3500 Loss 0.302663\n",
      "Step 3600 Loss 0.0809628\n",
      "Step 3700 Loss 0.314531\n",
      "Step 3800 Loss 0.0126427\n",
      "Step 3900 Loss 0.0105544\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 7\n",
      "Step 0 Loss 0.561283\n",
      "Step 100 Loss 0.0108561\n",
      "Step 200 Loss 0.00970634\n",
      "Step 300 Loss 0.00683768\n",
      "Step 400 Loss 0.00711666\n",
      "Step 500 Loss 0.322632\n",
      "Step 600 Loss 0.389386\n",
      "Step 700 Loss 0.107983\n",
      "Step 800 Loss 0.21359\n",
      "Step 900 Loss 0.186578\n",
      "Step 1000 Loss 0.0195556\n",
      "Step 1100 Loss 0.00986216\n",
      "Step 1200 Loss 0.00649447\n",
      "Step 1300 Loss 0.00599133\n",
      "Step 1400 Loss 0.187278\n",
      "Step 1500 Loss 0.0212217\n",
      "Step 1600 Loss 0.0101863\n",
      "Step 1700 Loss 0.00850875\n",
      "Step 1800 Loss 0.0058936\n",
      "Step 1900 Loss 0.00492038\n",
      "Step 2000 Loss 0.00520024\n",
      "Step 2100 Loss 0.00372624\n",
      "Step 2200 Loss 0.00457458\n",
      "Step 2300 Loss 0.00482663\n",
      "Step 2400 Loss 0.00516036\n",
      "Step 2500 Loss 0.119194\n",
      "Step 2600 Loss 0.188277\n",
      "Step 2700 Loss 0.147384\n",
      "Step 2800 Loss 0.00911259\n",
      "Step 2900 Loss 0.0129915\n",
      "Step 3000 Loss 0.169637\n",
      "Step 3100 Loss 0.00557716\n",
      "Step 3200 Loss 0.208335\n",
      "Step 3300 Loss 0.331607\n",
      "Step 3400 Loss 0.0195153\n",
      "Step 3500 Loss 0.302905\n",
      "Step 3600 Loss 0.0813608\n",
      "Step 3700 Loss 0.31654\n",
      "Step 3800 Loss 0.0128274\n",
      "Step 3900 Loss 0.0107076\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 8\n",
      "Step 0 Loss 0.567458\n",
      "Step 100 Loss 0.0108551\n",
      "Step 200 Loss 0.0097411\n",
      "Step 300 Loss 0.00679084\n",
      "Step 400 Loss 0.00717568\n",
      "Step 500 Loss 0.323581\n",
      "Step 600 Loss 0.392425\n",
      "Step 700 Loss 0.107387\n",
      "Step 800 Loss 0.213232\n",
      "Step 900 Loss 0.187106\n",
      "Step 1000 Loss 0.0200168\n",
      "Step 1100 Loss 0.0101468\n",
      "Step 1200 Loss 0.00667766\n",
      "Step 1300 Loss 0.00617305\n",
      "Step 1400 Loss 0.187974\n",
      "Step 1500 Loss 0.0218489\n",
      "Step 1600 Loss 0.0105161\n",
      "Step 1700 Loss 0.00877928\n",
      "Step 1800 Loss 0.00609894\n",
      "Step 1900 Loss 0.00507731\n",
      "Step 2000 Loss 0.00532217\n",
      "Step 2100 Loss 0.00380441\n",
      "Step 2200 Loss 0.00467122\n",
      "Step 2300 Loss 0.0048965\n",
      "Step 2400 Loss 0.00520979\n",
      "Step 2500 Loss 0.119124\n",
      "Step 2600 Loss 0.188927\n",
      "Step 2700 Loss 0.149452\n",
      "Step 2800 Loss 0.00896026\n",
      "Step 2900 Loss 0.0128928\n",
      "Step 3000 Loss 0.169531\n",
      "Step 3100 Loss 0.00561548\n",
      "Step 3200 Loss 0.211632\n",
      "Step 3300 Loss 0.329647\n",
      "Step 3400 Loss 0.0199547\n",
      "Step 3500 Loss 0.303073\n",
      "Step 3600 Loss 0.0817257\n",
      "Step 3700 Loss 0.317857\n",
      "Step 3800 Loss 0.0129974\n",
      "Step 3900 Loss 0.0108511\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 9\n",
      "Step 0 Loss 0.573113\n",
      "Step 100 Loss 0.0108703\n",
      "Step 200 Loss 0.00978088\n",
      "Step 300 Loss 0.00676391\n",
      "Step 400 Loss 0.00723943\n",
      "Step 500 Loss 0.324157\n",
      "Step 600 Loss 0.395045\n",
      "Step 700 Loss 0.106896\n",
      "Step 800 Loss 0.212998\n",
      "Step 900 Loss 0.187622\n",
      "Step 1000 Loss 0.0204399\n",
      "Step 1100 Loss 0.0104092\n",
      "Step 1200 Loss 0.00685654\n",
      "Step 1300 Loss 0.00634938\n",
      "Step 1400 Loss 0.188716\n",
      "Step 1500 Loss 0.0224001\n",
      "Step 1600 Loss 0.010826\n",
      "Step 1700 Loss 0.00903549\n",
      "Step 1800 Loss 0.00629959\n",
      "Step 1900 Loss 0.0052318\n",
      "Step 2000 Loss 0.00544217\n",
      "Step 2100 Loss 0.00388478\n",
      "Step 2200 Loss 0.00476777\n",
      "Step 2300 Loss 0.00496892\n",
      "Step 2400 Loss 0.00526275\n",
      "Step 2500 Loss 0.119018\n",
      "Step 2600 Loss 0.189436\n",
      "Step 2700 Loss 0.151255\n",
      "Step 2800 Loss 0.00882487\n",
      "Step 2900 Loss 0.0128078\n",
      "Step 3000 Loss 0.169439\n",
      "Step 3100 Loss 0.0056544\n",
      "Step 3200 Loss 0.214679\n",
      "Step 3300 Loss 0.328212\n",
      "Step 3400 Loss 0.0203472\n",
      "Step 3500 Loss 0.303178\n",
      "Step 3600 Loss 0.0820642\n",
      "Step 3700 Loss 0.318736\n",
      "Step 3800 Loss 0.0131536\n",
      "Step 3900 Loss 0.0109852\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 10\n",
      "Step 0 Loss 0.578725\n",
      "Step 100 Loss 0.0108921\n",
      "Step 200 Loss 0.00982204\n",
      "Step 300 Loss 0.00675234\n",
      "Step 400 Loss 0.00730483\n",
      "Step 500 Loss 0.3265\n",
      "Step 600 Loss 0.397513\n",
      "Step 700 Loss 0.106478\n",
      "Step 800 Loss 0.212831\n",
      "Step 900 Loss 0.188132\n",
      "Step 1000 Loss 0.0208444\n",
      "Step 1100 Loss 0.0106686\n",
      "Step 1200 Loss 0.00703871\n",
      "Step 1300 Loss 0.0065236\n",
      "Step 1400 Loss 0.18949\n",
      "Step 1500 Loss 0.0228896\n",
      "Step 1600 Loss 0.0111222\n",
      "Step 1700 Loss 0.00928347\n",
      "Step 1800 Loss 0.00649556\n",
      "Step 1900 Loss 0.00538508\n",
      "Step 2000 Loss 0.00556468\n",
      "Step 2100 Loss 0.00397105\n",
      "Step 2200 Loss 0.00486596\n",
      "Step 2300 Loss 0.00504426\n",
      "Step 2400 Loss 0.00531942\n",
      "Step 2500 Loss 0.118932\n",
      "Step 2600 Loss 0.189862\n",
      "Step 2700 Loss 0.152855\n",
      "Step 2800 Loss 0.00870878\n",
      "Step 2900 Loss 0.0127373\n",
      "Step 3000 Loss 0.169361\n",
      "Step 3100 Loss 0.00569433\n",
      "Step 3200 Loss 0.21749\n",
      "Step 3300 Loss 0.32687\n",
      "Step 3400 Loss 0.0206884\n",
      "Step 3500 Loss 0.303324\n",
      "Step 3600 Loss 0.0823743\n",
      "Step 3700 Loss 0.31934\n",
      "Step 3800 Loss 0.0132988\n",
      "Step 3900 Loss 0.0111154\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 11\n",
      "Step 0 Loss 0.582303\n",
      "Step 100 Loss 0.0109211\n",
      "Step 200 Loss 0.00986792\n",
      "Step 300 Loss 0.00675849\n",
      "Step 400 Loss 0.00737063\n",
      "Step 500 Loss 0.327776\n",
      "Step 600 Loss 0.399746\n",
      "Step 700 Loss 0.106095\n",
      "Step 800 Loss 0.212706\n",
      "Step 900 Loss 0.188632\n",
      "Step 1000 Loss 0.0212069\n",
      "Step 1100 Loss 0.0108947\n",
      "Step 1200 Loss 0.00720287\n",
      "Step 1300 Loss 0.00668719\n",
      "Step 1400 Loss 0.190272\n",
      "Step 1500 Loss 0.0233161\n",
      "Step 1600 Loss 0.0113946\n",
      "Step 1700 Loss 0.00951269\n",
      "Step 1800 Loss 0.00668348\n",
      "Step 1900 Loss 0.00553171\n",
      "Step 2000 Loss 0.00567965\n",
      "Step 2100 Loss 0.00405091\n",
      "Step 2200 Loss 0.00495991\n",
      "Step 2300 Loss 0.00511774\n",
      "Step 2400 Loss 0.00537319\n",
      "Step 2500 Loss 0.118788\n",
      "Step 2600 Loss 0.190189\n",
      "Step 2700 Loss 0.154258\n",
      "Step 2800 Loss 0.00860627\n",
      "Step 2900 Loss 0.0126768\n",
      "Step 3000 Loss 0.169285\n",
      "Step 3100 Loss 0.00573305\n",
      "Step 3200 Loss 0.220072\n",
      "Step 3300 Loss 0.325913\n",
      "Step 3400 Loss 0.0209937\n",
      "Step 3500 Loss 0.303474\n",
      "Step 3600 Loss 0.0826661\n",
      "Step 3700 Loss 0.319745\n",
      "Step 3800 Loss 0.013435\n",
      "Step 3900 Loss 0.0112384\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 12\n",
      "Step 0 Loss 0.584435\n",
      "Step 100 Loss 0.0109578\n",
      "Step 200 Loss 0.00991524\n",
      "Step 300 Loss 0.00676975\n",
      "Step 400 Loss 0.00743598\n",
      "Step 500 Loss 0.3283\n",
      "Step 600 Loss 0.401654\n",
      "Step 700 Loss 0.105789\n",
      "Step 800 Loss 0.21263\n",
      "Step 900 Loss 0.189106\n",
      "Step 1000 Loss 0.0215513\n",
      "Step 1100 Loss 0.0111194\n",
      "Step 1200 Loss 0.00736313\n",
      "Step 1300 Loss 0.00685197\n",
      "Step 1400 Loss 0.191049\n",
      "Step 1500 Loss 0.0236955\n",
      "Step 1600 Loss 0.0116517\n",
      "Step 1700 Loss 0.00973546\n",
      "Step 1800 Loss 0.00686503\n",
      "Step 1900 Loss 0.00567657\n",
      "Step 2000 Loss 0.00579801\n",
      "Step 2100 Loss 0.00413001\n",
      "Step 2200 Loss 0.00505575\n",
      "Step 2300 Loss 0.00519286\n",
      "Step 2400 Loss 0.00543087\n",
      "Step 2500 Loss 0.118644\n",
      "Step 2600 Loss 0.190471\n",
      "Step 2700 Loss 0.155511\n",
      "Step 2800 Loss 0.00852243\n",
      "Step 2900 Loss 0.012638\n",
      "Step 3000 Loss 0.169238\n",
      "Step 3100 Loss 0.00577129\n",
      "Step 3200 Loss 0.222458\n",
      "Step 3300 Loss 0.324794\n",
      "Step 3400 Loss 0.0212503\n",
      "Step 3500 Loss 0.303715\n",
      "Step 3600 Loss 0.0829298\n",
      "Step 3700 Loss 0.320047\n",
      "Step 3800 Loss 0.01356\n",
      "Step 3900 Loss 0.0113611\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 13\n",
      "Step 0 Loss 0.585548\n",
      "Step 100 Loss 0.0109965\n",
      "Step 200 Loss 0.00996761\n",
      "Step 300 Loss 0.00678071\n",
      "Step 400 Loss 0.00750059\n",
      "Step 500 Loss 0.327968\n",
      "Step 600 Loss 0.4032\n",
      "Step 700 Loss 0.105545\n",
      "Step 800 Loss 0.212566\n",
      "Step 900 Loss 0.189546\n",
      "Step 1000 Loss 0.0218838\n",
      "Step 1100 Loss 0.0113545\n",
      "Step 1200 Loss 0.00752504\n",
      "Step 1300 Loss 0.00701917\n",
      "Step 1400 Loss 0.191814\n",
      "Step 1500 Loss 0.0240355\n",
      "Step 1600 Loss 0.011897\n",
      "Step 1700 Loss 0.00995465\n",
      "Step 1800 Loss 0.00704087\n",
      "Step 1900 Loss 0.00581953\n",
      "Step 2000 Loss 0.00591992\n",
      "Step 2100 Loss 0.00421075\n",
      "Step 2200 Loss 0.00515367\n",
      "Step 2300 Loss 0.00527008\n",
      "Step 2400 Loss 0.00549281\n",
      "Step 2500 Loss 0.1185\n",
      "Step 2600 Loss 0.190719\n",
      "Step 2700 Loss 0.156631\n",
      "Step 2800 Loss 0.00845625\n",
      "Step 2900 Loss 0.0126231\n",
      "Step 3000 Loss 0.169217\n",
      "Step 3100 Loss 0.00580896\n",
      "Step 3200 Loss 0.224666\n",
      "Step 3300 Loss 0.323249\n",
      "Step 3400 Loss 0.0214592\n",
      "Step 3500 Loss 0.304049\n",
      "Step 3600 Loss 0.0831643\n",
      "Step 3700 Loss 0.320275\n",
      "Step 3800 Loss 0.0136741\n",
      "Step 3900 Loss 0.0114835\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 14\n",
      "Step 0 Loss 0.585789\n",
      "Step 100 Loss 0.0110369\n",
      "Step 200 Loss 0.0100295\n",
      "Step 300 Loss 0.00679687\n",
      "Step 400 Loss 0.00756681\n",
      "Step 500 Loss 0.326031\n",
      "Step 600 Loss 0.404707\n",
      "Step 700 Loss 0.105316\n",
      "Step 800 Loss 0.212519\n",
      "Step 900 Loss 0.18995\n",
      "Step 1000 Loss 0.0221855\n",
      "Step 1100 Loss 0.0115644\n",
      "Step 1200 Loss 0.00767628\n",
      "Step 1300 Loss 0.00717695\n",
      "Step 1400 Loss 0.192557\n",
      "Step 1500 Loss 0.0243321\n",
      "Step 1600 Loss 0.0121245\n",
      "Step 1700 Loss 0.010158\n",
      "Step 1800 Loss 0.00720846\n",
      "Step 1900 Loss 0.00595594\n",
      "Step 2000 Loss 0.00603519\n",
      "Step 2100 Loss 0.00428642\n",
      "Step 2200 Loss 0.00524667\n",
      "Step 2300 Loss 0.00534396\n",
      "Step 2400 Loss 0.00555234\n",
      "Step 2500 Loss 0.11834\n",
      "Step 2600 Loss 0.190904\n",
      "Step 2700 Loss 0.157621\n",
      "Step 2800 Loss 0.00839952\n",
      "Step 2900 Loss 0.0126128\n",
      "Step 3000 Loss 0.169198\n",
      "Step 3100 Loss 0.00584411\n",
      "Step 3200 Loss 0.226704\n",
      "Step 3300 Loss 0.321969\n",
      "Step 3400 Loss 0.0216397\n",
      "Step 3500 Loss 0.30439\n",
      "Step 3600 Loss 0.0833813\n",
      "Step 3700 Loss 0.320482\n",
      "Step 3800 Loss 0.0137792\n",
      "Step 3900 Loss 0.0116015\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 15\n",
      "Step 0 Loss 0.586867\n",
      "Step 100 Loss 0.0110747\n",
      "Step 200 Loss 0.0100918\n",
      "Step 300 Loss 0.00680981\n",
      "Step 400 Loss 0.00763229\n",
      "Step 500 Loss 0.325603\n",
      "Step 600 Loss 0.40617\n",
      "Step 700 Loss 0.105089\n",
      "Step 800 Loss 0.212457\n",
      "Step 900 Loss 0.190308\n",
      "Step 1000 Loss 0.0224701\n",
      "Step 1100 Loss 0.0117663\n",
      "Step 1200 Loss 0.00782085\n",
      "Step 1300 Loss 0.00732997\n",
      "Step 1400 Loss 0.193271\n",
      "Step 1500 Loss 0.0245911\n",
      "Step 1600 Loss 0.0123369\n",
      "Step 1700 Loss 0.0103472\n",
      "Step 1800 Loss 0.00736879\n",
      "Step 1900 Loss 0.00608653\n",
      "Step 2000 Loss 0.0061445\n",
      "Step 2100 Loss 0.00436052\n",
      "Step 2200 Loss 0.00533474\n",
      "Step 2300 Loss 0.00541445\n",
      "Step 2400 Loss 0.00560889\n",
      "Step 2500 Loss 0.118196\n",
      "Step 2600 Loss 0.191052\n",
      "Step 2700 Loss 0.158516\n",
      "Step 2800 Loss 0.00834994\n",
      "Step 2900 Loss 0.0125999\n",
      "Step 3000 Loss 0.16917\n",
      "Step 3100 Loss 0.005877\n",
      "Step 3200 Loss 0.228584\n",
      "Step 3300 Loss 0.320974\n",
      "Step 3400 Loss 0.0217949\n",
      "Step 3500 Loss 0.304725\n",
      "Step 3600 Loss 0.0835839\n",
      "Step 3700 Loss 0.320594\n",
      "Step 3800 Loss 0.0138745\n",
      "Step 3900 Loss 0.011713\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 16\n",
      "Step 0 Loss 0.588005\n",
      "Step 100 Loss 0.0111156\n",
      "Step 200 Loss 0.0101498\n",
      "Step 300 Loss 0.00683043\n",
      "Step 400 Loss 0.0076949\n",
      "Step 500 Loss 0.32493\n",
      "Step 600 Loss 0.407567\n",
      "Step 700 Loss 0.104875\n",
      "Step 800 Loss 0.212395\n",
      "Step 900 Loss 0.190641\n",
      "Step 1000 Loss 0.0227291\n",
      "Step 1100 Loss 0.0119462\n",
      "Step 1200 Loss 0.00795485\n",
      "Step 1300 Loss 0.00747525\n",
      "Step 1400 Loss 0.193958\n",
      "Step 1500 Loss 0.0248156\n",
      "Step 1600 Loss 0.012534\n",
      "Step 1700 Loss 0.0105219\n",
      "Step 1800 Loss 0.00752188\n",
      "Step 1900 Loss 0.00621184\n",
      "Step 2000 Loss 0.0062478\n",
      "Step 2100 Loss 0.00443038\n",
      "Step 2200 Loss 0.00541742\n",
      "Step 2300 Loss 0.00548161\n",
      "Step 2400 Loss 0.00566253\n",
      "Step 2500 Loss 0.118074\n",
      "Step 2600 Loss 0.191159\n",
      "Step 2700 Loss 0.159325\n",
      "Step 2800 Loss 0.0083059\n",
      "Step 2900 Loss 0.0125816\n",
      "Step 3000 Loss 0.169132\n",
      "Step 3100 Loss 0.00590822\n",
      "Step 3200 Loss 0.230327\n",
      "Step 3300 Loss 0.320405\n",
      "Step 3400 Loss 0.0219312\n",
      "Step 3500 Loss 0.305049\n",
      "Step 3600 Loss 0.0837774\n",
      "Step 3700 Loss 0.320677\n",
      "Step 3800 Loss 0.0139632\n",
      "Step 3900 Loss 0.0118192\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 17\n",
      "Step 0 Loss 0.589781\n",
      "Step 100 Loss 0.0111567\n",
      "Step 200 Loss 0.0102021\n",
      "Step 300 Loss 0.00684877\n",
      "Step 400 Loss 0.00775432\n",
      "Step 500 Loss 0.324238\n",
      "Step 600 Loss 0.408742\n",
      "Step 700 Loss 0.104721\n",
      "Step 800 Loss 0.212433\n",
      "Step 900 Loss 0.190966\n",
      "Step 1000 Loss 0.0229824\n",
      "Step 1100 Loss 0.012152\n",
      "Step 1200 Loss 0.00808955\n",
      "Step 1300 Loss 0.00762767\n",
      "Step 1400 Loss 0.194604\n",
      "Step 1500 Loss 0.02502\n",
      "Step 1600 Loss 0.0127228\n",
      "Step 1700 Loss 0.0106964\n",
      "Step 1800 Loss 0.00767058\n",
      "Step 1900 Loss 0.00633507\n",
      "Step 2000 Loss 0.00635297\n",
      "Step 2100 Loss 0.00449873\n",
      "Step 2200 Loss 0.00550245\n",
      "Step 2300 Loss 0.00555021\n",
      "Step 2400 Loss 0.00571901\n",
      "Step 2500 Loss 0.117904\n",
      "Step 2600 Loss 0.191268\n",
      "Step 2700 Loss 0.160061\n",
      "Step 2800 Loss 0.00827339\n",
      "Step 2900 Loss 0.0125897\n",
      "Step 3000 Loss 0.169117\n",
      "Step 3100 Loss 0.00593931\n",
      "Step 3200 Loss 0.23194\n",
      "Step 3300 Loss 0.319503\n",
      "Step 3400 Loss 0.0220435\n",
      "Step 3500 Loss 0.305414\n",
      "Step 3600 Loss 0.0839526\n",
      "Step 3700 Loss 0.32075\n",
      "Step 3800 Loss 0.0140444\n",
      "Step 3900 Loss 0.0119239\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 18\n",
      "Step 0 Loss 0.590033\n",
      "Step 100 Loss 0.0112005\n",
      "Step 200 Loss 0.0102592\n",
      "Step 300 Loss 0.00687204\n",
      "Step 400 Loss 0.00781373\n",
      "Step 500 Loss 0.323832\n",
      "Step 600 Loss 0.410055\n",
      "Step 700 Loss 0.104591\n",
      "Step 800 Loss 0.212518\n",
      "Step 900 Loss 0.191308\n",
      "Step 1000 Loss 0.0232135\n",
      "Step 1100 Loss 0.0123509\n",
      "Step 1200 Loss 0.00822449\n",
      "Step 1300 Loss 0.00777554\n",
      "Step 1400 Loss 0.19523\n",
      "Step 1500 Loss 0.0252032\n",
      "Step 1600 Loss 0.0129041\n",
      "Step 1700 Loss 0.0108675\n",
      "Step 1800 Loss 0.00781461\n",
      "Step 1900 Loss 0.00645598\n",
      "Step 2000 Loss 0.00645896\n",
      "Step 2100 Loss 0.00456858\n",
      "Step 2200 Loss 0.00558808\n",
      "Step 2300 Loss 0.00561958\n",
      "Step 2400 Loss 0.00577766\n",
      "Step 2500 Loss 0.117745\n",
      "Step 2600 Loss 0.191375\n",
      "Step 2700 Loss 0.160741\n",
      "Step 2800 Loss 0.00824958\n",
      "Step 2900 Loss 0.0126097\n",
      "Step 3000 Loss 0.169116\n",
      "Step 3100 Loss 0.00597015\n",
      "Step 3200 Loss 0.233443\n",
      "Step 3300 Loss 0.318306\n",
      "Step 3400 Loss 0.0221271\n",
      "Step 3500 Loss 0.305824\n",
      "Step 3600 Loss 0.0841087\n",
      "Step 3700 Loss 0.320793\n",
      "Step 3800 Loss 0.0141175\n",
      "Step 3900 Loss 0.0120262\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 19\n",
      "Step 0 Loss 0.589232\n",
      "Step 100 Loss 0.0112444\n",
      "Step 200 Loss 0.0103194\n",
      "Step 300 Loss 0.00689725\n",
      "Step 400 Loss 0.0078727\n",
      "Step 500 Loss 0.323549\n",
      "Step 600 Loss 0.411419\n",
      "Step 700 Loss 0.104473\n",
      "Step 800 Loss 0.212632\n",
      "Step 900 Loss 0.191641\n",
      "Step 1000 Loss 0.0234375\n",
      "Step 1100 Loss 0.0125501\n",
      "Step 1200 Loss 0.0083548\n",
      "Step 1300 Loss 0.00792254\n",
      "Step 1400 Loss 0.195825\n",
      "Step 1500 Loss 0.0253651\n",
      "Step 1600 Loss 0.0130753\n",
      "Step 1700 Loss 0.0110313\n",
      "Step 1800 Loss 0.00795347\n",
      "Step 1900 Loss 0.00657383\n",
      "Step 2000 Loss 0.00656343\n",
      "Step 2100 Loss 0.00463726\n",
      "Step 2200 Loss 0.00567204\n",
      "Step 2300 Loss 0.00568818\n",
      "Step 2400 Loss 0.00583735\n",
      "Step 2500 Loss 0.117625\n",
      "Step 2600 Loss 0.191464\n",
      "Step 2700 Loss 0.161367\n",
      "Step 2800 Loss 0.00823173\n",
      "Step 2900 Loss 0.0126289\n",
      "Step 3000 Loss 0.169121\n",
      "Step 3100 Loss 0.00600001\n",
      "Step 3200 Loss 0.234851\n",
      "Step 3300 Loss 0.316934\n",
      "Step 3400 Loss 0.0221859\n",
      "Step 3500 Loss 0.306264\n",
      "Step 3600 Loss 0.0842476\n",
      "Step 3700 Loss 0.320846\n",
      "Step 3800 Loss 0.0141842\n",
      "Step 3900 Loss 0.0121257\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 20\n",
      "Step 0 Loss 0.587731\n",
      "Step 100 Loss 0.0112885\n",
      "Step 200 Loss 0.0103807\n",
      "Step 300 Loss 0.00692329\n",
      "Step 400 Loss 0.00793061\n",
      "Step 500 Loss 0.323348\n",
      "Step 600 Loss 0.412825\n",
      "Step 700 Loss 0.104371\n",
      "Step 800 Loss 0.212776\n",
      "Step 900 Loss 0.191964\n",
      "Step 1000 Loss 0.0236729\n",
      "Step 1100 Loss 0.0127545\n",
      "Step 1200 Loss 0.00848026\n",
      "Step 1300 Loss 0.00807036\n",
      "Step 1400 Loss 0.196385\n",
      "Step 1500 Loss 0.0255074\n",
      "Step 1600 Loss 0.0132359\n",
      "Step 1700 Loss 0.0111877\n",
      "Step 1800 Loss 0.00808676\n",
      "Step 1900 Loss 0.00668839\n",
      "Step 2000 Loss 0.00666639\n",
      "Step 2100 Loss 0.00470374\n",
      "Step 2200 Loss 0.00575412\n",
      "Step 2300 Loss 0.00575589\n",
      "Step 2400 Loss 0.00589774\n",
      "Step 2500 Loss 0.117548\n",
      "Step 2600 Loss 0.191538\n",
      "Step 2700 Loss 0.161948\n",
      "Step 2800 Loss 0.00821887\n",
      "Step 2900 Loss 0.0126451\n",
      "Step 3000 Loss 0.169136\n",
      "Step 3100 Loss 0.00602867\n",
      "Step 3200 Loss 0.236176\n",
      "Step 3300 Loss 0.31537\n",
      "Step 3400 Loss 0.0222223\n",
      "Step 3500 Loss 0.306733\n",
      "Step 3600 Loss 0.0843691\n",
      "Step 3700 Loss 0.320953\n",
      "Step 3800 Loss 0.0142452\n",
      "Step 3900 Loss 0.0122239\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 21\n",
      "Step 0 Loss 0.585817\n",
      "Step 100 Loss 0.0113329\n",
      "Step 200 Loss 0.0104415\n",
      "Step 300 Loss 0.00694779\n",
      "Step 400 Loss 0.00798675\n",
      "Step 500 Loss 0.323214\n",
      "Step 600 Loss 0.413914\n",
      "Step 700 Loss 0.10426\n",
      "Step 800 Loss 0.212836\n",
      "Step 900 Loss 0.192272\n",
      "Step 1000 Loss 0.0238333\n",
      "Step 1100 Loss 0.0129046\n",
      "Step 1200 Loss 0.00860138\n",
      "Step 1300 Loss 0.00819486\n",
      "Step 1400 Loss 0.19695\n",
      "Step 1500 Loss 0.0256327\n",
      "Step 1600 Loss 0.0133893\n",
      "Step 1700 Loss 0.0113358\n",
      "Step 1800 Loss 0.00821535\n",
      "Step 1900 Loss 0.0067973\n",
      "Step 2000 Loss 0.00676256\n",
      "Step 2100 Loss 0.00477007\n",
      "Step 2200 Loss 0.00583263\n",
      "Step 2300 Loss 0.00582013\n",
      "Step 2400 Loss 0.005951\n",
      "Step 2500 Loss 0.117406\n",
      "Step 2600 Loss 0.191595\n",
      "Step 2700 Loss 0.162474\n",
      "Step 2800 Loss 0.00820853\n",
      "Step 2900 Loss 0.012672\n",
      "Step 3000 Loss 0.169135\n",
      "Step 3100 Loss 0.00605709\n",
      "Step 3200 Loss 0.237398\n",
      "Step 3300 Loss 0.314245\n",
      "Step 3400 Loss 0.0222674\n",
      "Step 3500 Loss 0.307142\n",
      "Step 3600 Loss 0.084494\n",
      "Step 3700 Loss 0.320962\n",
      "Step 3800 Loss 0.0143012\n",
      "Step 3900 Loss 0.0123086\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 22\n",
      "Step 0 Loss 0.584549\n",
      "Step 100 Loss 0.011376\n",
      "Step 200 Loss 0.0104969\n",
      "Step 300 Loss 0.00697286\n",
      "Step 400 Loss 0.00804046\n",
      "Step 500 Loss 0.323088\n",
      "Step 600 Loss 0.414799\n",
      "Step 700 Loss 0.104151\n",
      "Step 800 Loss 0.212863\n",
      "Step 900 Loss 0.19257\n",
      "Step 1000 Loss 0.0239396\n",
      "Step 1100 Loss 0.0130134\n",
      "Step 1200 Loss 0.0087169\n",
      "Step 1300 Loss 0.00830235\n",
      "Step 1400 Loss 0.197511\n",
      "Step 1500 Loss 0.0257412\n",
      "Step 1600 Loss 0.0135353\n",
      "Step 1700 Loss 0.0114754\n",
      "Step 1800 Loss 0.00833972\n",
      "Step 1900 Loss 0.0069016\n",
      "Step 2000 Loss 0.00685288\n",
      "Step 2100 Loss 0.00483566\n",
      "Step 2200 Loss 0.005907\n",
      "Step 2300 Loss 0.00588131\n",
      "Step 2400 Loss 0.00599912\n",
      "Step 2500 Loss 0.117245\n",
      "Step 2600 Loss 0.191629\n",
      "Step 2700 Loss 0.162951\n",
      "Step 2800 Loss 0.00819982\n",
      "Step 2900 Loss 0.0126998\n",
      "Step 3000 Loss 0.169118\n",
      "Step 3100 Loss 0.00608494\n",
      "Step 3200 Loss 0.238531\n",
      "Step 3300 Loss 0.313621\n",
      "Step 3400 Loss 0.0223161\n",
      "Step 3500 Loss 0.307509\n",
      "Step 3600 Loss 0.0846227\n",
      "Step 3700 Loss 0.320879\n",
      "Step 3800 Loss 0.0143536\n",
      "Step 3900 Loss 0.012383\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 23\n",
      "Step 0 Loss 0.584006\n",
      "Step 100 Loss 0.0114169\n",
      "Step 200 Loss 0.010547\n",
      "Step 300 Loss 0.00699788\n",
      "Step 400 Loss 0.00809177\n",
      "Step 500 Loss 0.322931\n",
      "Step 600 Loss 0.415745\n",
      "Step 700 Loss 0.104064\n",
      "Step 800 Loss 0.212944\n",
      "Step 900 Loss 0.192869\n",
      "Step 1000 Loss 0.0240591\n",
      "Step 1100 Loss 0.0131291\n",
      "Step 1200 Loss 0.00882825\n",
      "Step 1300 Loss 0.0084116\n",
      "Step 1400 Loss 0.198044\n",
      "Step 1500 Loss 0.0258365\n",
      "Step 1600 Loss 0.0136733\n",
      "Step 1700 Loss 0.0116088\n",
      "Step 1800 Loss 0.00845999\n",
      "Step 1900 Loss 0.00700323\n",
      "Step 2000 Loss 0.00694177\n",
      "Step 2100 Loss 0.00489963\n",
      "Step 2200 Loss 0.00597986\n",
      "Step 2300 Loss 0.0059417\n",
      "Step 2400 Loss 0.00604751\n",
      "Step 2500 Loss 0.117117\n",
      "Step 2600 Loss 0.19165\n",
      "Step 2700 Loss 0.163394\n",
      "Step 2800 Loss 0.00819451\n",
      "Step 2900 Loss 0.0127244\n",
      "Step 3000 Loss 0.169102\n",
      "Step 3100 Loss 0.00611237\n",
      "Step 3200 Loss 0.2396\n",
      "Step 3300 Loss 0.313085\n",
      "Step 3400 Loss 0.0223496\n",
      "Step 3500 Loss 0.307895\n",
      "Step 3600 Loss 0.0847416\n",
      "Step 3700 Loss 0.320806\n",
      "Step 3800 Loss 0.0144022\n",
      "Step 3900 Loss 0.0124551\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 24\n",
      "Step 0 Loss 0.583075\n",
      "Step 100 Loss 0.0114577\n",
      "Step 200 Loss 0.0105961\n",
      "Step 300 Loss 0.00702165\n",
      "Step 400 Loss 0.0081417\n",
      "Step 500 Loss 0.322784\n",
      "Step 600 Loss 0.416667\n",
      "Step 700 Loss 0.103986\n",
      "Step 800 Loss 0.213036\n",
      "Step 900 Loss 0.193161\n",
      "Step 1000 Loss 0.0241781\n",
      "Step 1100 Loss 0.0132463\n",
      "Step 1200 Loss 0.00893597\n",
      "Step 1300 Loss 0.00852009\n",
      "Step 1400 Loss 0.198553\n",
      "Step 1500 Loss 0.0259201\n",
      "Step 1600 Loss 0.0138042\n",
      "Step 1700 Loss 0.0117374\n",
      "Step 1800 Loss 0.00857612\n",
      "Step 1900 Loss 0.00710229\n",
      "Step 2000 Loss 0.00702885\n",
      "Step 2100 Loss 0.00496186\n",
      "Step 2200 Loss 0.00605138\n",
      "Step 2300 Loss 0.00600117\n",
      "Step 2400 Loss 0.00609503\n",
      "Step 2500 Loss 0.116996\n",
      "Step 2600 Loss 0.191665\n",
      "Step 2700 Loss 0.163806\n",
      "Step 2800 Loss 0.00819239\n",
      "Step 2900 Loss 0.0127515\n",
      "Step 3000 Loss 0.169091\n",
      "Step 3100 Loss 0.00613932\n",
      "Step 3200 Loss 0.240605\n",
      "Step 3300 Loss 0.312641\n",
      "Step 3400 Loss 0.0223735\n",
      "Step 3500 Loss 0.308288\n",
      "Step 3600 Loss 0.0848524\n",
      "Step 3700 Loss 0.320741\n",
      "Step 3800 Loss 0.0144472\n",
      "Step 3900 Loss 0.012524\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 25\n",
      "Step 0 Loss 0.58185\n",
      "Step 100 Loss 0.011498\n",
      "Step 200 Loss 0.0106439\n",
      "Step 300 Loss 0.00704422\n",
      "Step 400 Loss 0.00819004\n",
      "Step 500 Loss 0.322643\n",
      "Step 600 Loss 0.417564\n",
      "Step 700 Loss 0.103917\n",
      "Step 800 Loss 0.213139\n",
      "Step 900 Loss 0.193446\n",
      "Step 1000 Loss 0.0242931\n",
      "Step 1100 Loss 0.0133633\n",
      "Step 1200 Loss 0.00904037\n",
      "Step 1300 Loss 0.00862706\n",
      "Step 1400 Loss 0.199039\n",
      "Step 1500 Loss 0.0259936\n",
      "Step 1600 Loss 0.0139287\n",
      "Step 1700 Loss 0.0118612\n",
      "Step 1800 Loss 0.00868863\n",
      "Step 1900 Loss 0.00719881\n",
      "Step 2000 Loss 0.0071143\n",
      "Step 2100 Loss 0.0050225\n",
      "Step 2200 Loss 0.0061216\n",
      "Step 2300 Loss 0.00605965\n",
      "Step 2400 Loss 0.0061416\n",
      "Step 2500 Loss 0.116879\n",
      "Step 2600 Loss 0.191673\n",
      "Step 2700 Loss 0.164188\n",
      "Step 2800 Loss 0.00819311\n",
      "Step 2900 Loss 0.0127807\n",
      "Step 3000 Loss 0.169084\n",
      "Step 3100 Loss 0.00616581\n",
      "Step 3200 Loss 0.241551\n",
      "Step 3300 Loss 0.312306\n",
      "Step 3400 Loss 0.0223893\n",
      "Step 3500 Loss 0.308687\n",
      "Step 3600 Loss 0.0849565\n",
      "Step 3700 Loss 0.320684\n",
      "Step 3800 Loss 0.0144889\n",
      "Step 3900 Loss 0.0125891\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 26\n",
      "Step 0 Loss 0.580369\n",
      "Step 100 Loss 0.0115375\n",
      "Step 200 Loss 0.0106905\n",
      "Step 300 Loss 0.00706578\n",
      "Step 400 Loss 0.00823706\n",
      "Step 500 Loss 0.322506\n",
      "Step 600 Loss 0.418431\n",
      "Step 700 Loss 0.103856\n",
      "Step 800 Loss 0.21325\n",
      "Step 900 Loss 0.193725\n",
      "Step 1000 Loss 0.0244017\n",
      "Step 1100 Loss 0.0134777\n",
      "Step 1200 Loss 0.00914113\n",
      "Step 1300 Loss 0.0087318\n",
      "Step 1400 Loss 0.199504\n",
      "Step 1500 Loss 0.0260581\n",
      "Step 1600 Loss 0.0140469\n",
      "Step 1700 Loss 0.0119806\n",
      "Step 1800 Loss 0.00879765\n",
      "Step 1900 Loss 0.00729253\n",
      "Step 2000 Loss 0.0071976\n",
      "Step 2100 Loss 0.00508166\n",
      "Step 2200 Loss 0.00619027\n",
      "Step 2300 Loss 0.00611706\n",
      "Step 2400 Loss 0.00618686\n",
      "Step 2500 Loss 0.116766\n",
      "Step 2600 Loss 0.191676\n",
      "Step 2700 Loss 0.164545\n",
      "Step 2800 Loss 0.00819615\n",
      "Step 2900 Loss 0.0128114\n",
      "Step 3000 Loss 0.169079\n",
      "Step 3100 Loss 0.00619194\n",
      "Step 3200 Loss 0.242443\n",
      "Step 3300 Loss 0.312094\n",
      "Step 3400 Loss 0.0223986\n",
      "Step 3500 Loss 0.309089\n",
      "Step 3600 Loss 0.0850543\n",
      "Step 3700 Loss 0.320632\n",
      "Step 3800 Loss 0.0145276\n",
      "Step 3900 Loss 0.0126509\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 27\n",
      "Step 0 Loss 0.578708\n",
      "Step 100 Loss 0.0115764\n",
      "Step 200 Loss 0.010736\n",
      "Step 300 Loss 0.00708638\n",
      "Step 400 Loss 0.00828275\n",
      "Step 500 Loss 0.32237\n",
      "Step 600 Loss 0.419266\n",
      "Step 700 Loss 0.103801\n",
      "Step 800 Loss 0.213366\n",
      "Step 900 Loss 0.193998\n",
      "Step 1000 Loss 0.0245033\n",
      "Step 1100 Loss 0.0135887\n",
      "Step 1200 Loss 0.00923874\n",
      "Step 1300 Loss 0.00883402\n",
      "Step 1400 Loss 0.19995\n",
      "Step 1500 Loss 0.026114\n",
      "Step 1600 Loss 0.0141597\n",
      "Step 1700 Loss 0.0120958\n",
      "Step 1800 Loss 0.00890337\n",
      "Step 1900 Loss 0.00738378\n",
      "Step 2000 Loss 0.00727912\n",
      "Step 2100 Loss 0.00513953\n",
      "Step 2200 Loss 0.0062576\n",
      "Step 2300 Loss 0.00617338\n",
      "Step 2400 Loss 0.00623079\n",
      "Step 2500 Loss 0.116656\n",
      "Step 2600 Loss 0.191673\n",
      "Step 2700 Loss 0.164877\n",
      "Step 2800 Loss 0.00820098\n",
      "Step 2900 Loss 0.0128433\n",
      "Step 3000 Loss 0.169076\n",
      "Step 3100 Loss 0.00621762\n",
      "Step 3200 Loss 0.243283\n",
      "Step 3300 Loss 0.312007\n",
      "Step 3400 Loss 0.0224021\n",
      "Step 3500 Loss 0.309492\n",
      "Step 3600 Loss 0.0851469\n",
      "Step 3700 Loss 0.320588\n",
      "Step 3800 Loss 0.0145638\n",
      "Step 3900 Loss 0.0127094\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 28\n",
      "Step 0 Loss 0.576904\n",
      "Step 100 Loss 0.0116146\n",
      "Step 200 Loss 0.0107803\n",
      "Step 300 Loss 0.00710642\n",
      "Step 400 Loss 0.00832736\n",
      "Step 500 Loss 0.322233\n",
      "Step 600 Loss 0.420067\n",
      "Step 700 Loss 0.103752\n",
      "Step 800 Loss 0.213485\n",
      "Step 900 Loss 0.194266\n",
      "Step 1000 Loss 0.0245975\n",
      "Step 1100 Loss 0.0136954\n",
      "Step 1200 Loss 0.00933315\n",
      "Step 1300 Loss 0.00893358\n",
      "Step 1400 Loss 0.200379\n",
      "Step 1500 Loss 0.0261628\n",
      "Step 1600 Loss 0.0142671\n",
      "Step 1700 Loss 0.0122067\n",
      "Step 1800 Loss 0.00900565\n",
      "Step 1900 Loss 0.00747267\n",
      "Step 2000 Loss 0.00735872\n",
      "Step 2100 Loss 0.00519613\n",
      "Step 2200 Loss 0.00632338\n",
      "Step 2300 Loss 0.00622874\n",
      "Step 2400 Loss 0.0062733\n",
      "Step 2500 Loss 0.11655\n",
      "Step 2600 Loss 0.191663\n",
      "Step 2700 Loss 0.165187\n",
      "Step 2800 Loss 0.00820778\n",
      "Step 2900 Loss 0.0128762\n",
      "Step 3000 Loss 0.169074\n",
      "Step 3100 Loss 0.00624299\n",
      "Step 3200 Loss 0.244075\n",
      "Step 3300 Loss 0.312038\n",
      "Step 3400 Loss 0.0224008\n",
      "Step 3500 Loss 0.309895\n",
      "Step 3600 Loss 0.0852347\n",
      "Step 3700 Loss 0.320549\n",
      "Step 3800 Loss 0.0145973\n",
      "Step 3900 Loss 0.0127646\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 29\n",
      "Step 0 Loss 0.574997\n",
      "Step 100 Loss 0.0116518\n",
      "Step 200 Loss 0.0108236\n",
      "Step 300 Loss 0.007126\n",
      "Step 400 Loss 0.00837071\n",
      "Step 500 Loss 0.322096\n",
      "Step 600 Loss 0.420834\n",
      "Step 700 Loss 0.103708\n",
      "Step 800 Loss 0.213607\n",
      "Step 900 Loss 0.194529\n",
      "Step 1000 Loss 0.024685\n",
      "Step 1100 Loss 0.0137982\n",
      "Step 1200 Loss 0.00942458\n",
      "Step 1300 Loss 0.0090306\n",
      "Step 1400 Loss 0.20079\n",
      "Step 1500 Loss 0.0262047\n",
      "Step 1600 Loss 0.0143697\n",
      "Step 1700 Loss 0.0123138\n",
      "Step 1800 Loss 0.00910512\n",
      "Step 1900 Loss 0.00755917\n",
      "Step 2000 Loss 0.00743632\n",
      "Step 2100 Loss 0.00525152\n",
      "Step 2200 Loss 0.00638797\n",
      "Step 2300 Loss 0.00628296\n",
      "Step 2400 Loss 0.00631444\n",
      "Step 2500 Loss 0.116446\n",
      "Step 2600 Loss 0.191649\n",
      "Step 2700 Loss 0.165476\n",
      "Step 2800 Loss 0.00821597\n",
      "Step 2900 Loss 0.0129094\n",
      "Step 3000 Loss 0.169072\n",
      "Step 3100 Loss 0.00626796\n",
      "Step 3200 Loss 0.244824\n",
      "Step 3300 Loss 0.312173\n",
      "Step 3400 Loss 0.0223951\n",
      "Step 3500 Loss 0.3103\n",
      "Step 3600 Loss 0.085318\n",
      "Step 3700 Loss 0.320514\n",
      "Step 3800 Loss 0.0146286\n",
      "Step 3900 Loss 0.0128167\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 30\n",
      "Step 0 Loss 0.573001\n",
      "Step 100 Loss 0.011688\n",
      "Step 200 Loss 0.0108659\n",
      "Step 300 Loss 0.00714521\n",
      "Step 400 Loss 0.00841318\n",
      "Step 500 Loss 0.321957\n",
      "Step 600 Loss 0.42157\n",
      "Step 700 Loss 0.103668\n",
      "Step 800 Loss 0.213731\n",
      "Step 900 Loss 0.194786\n",
      "Step 1000 Loss 0.0247656\n",
      "Step 1100 Loss 0.0138967\n",
      "Step 1200 Loss 0.00951309\n",
      "Step 1300 Loss 0.00912487\n",
      "Step 1400 Loss 0.201186\n",
      "Step 1500 Loss 0.0262407\n",
      "Step 1600 Loss 0.0144677\n",
      "Step 1700 Loss 0.0124172\n",
      "Step 1800 Loss 0.00920163\n",
      "Step 1900 Loss 0.00764349\n",
      "Step 2000 Loss 0.00751209\n",
      "Step 2100 Loss 0.00530564\n",
      "Step 2200 Loss 0.00645088\n",
      "Step 2300 Loss 0.00633618\n",
      "Step 2400 Loss 0.00635437\n",
      "Step 2500 Loss 0.116345\n",
      "Step 2600 Loss 0.19163\n",
      "Step 2700 Loss 0.165747\n",
      "Step 2800 Loss 0.00822534\n",
      "Step 2900 Loss 0.012943\n",
      "Step 3000 Loss 0.169071\n",
      "Step 3100 Loss 0.00629272\n",
      "Step 3200 Loss 0.245532\n",
      "Step 3300 Loss 0.312394\n",
      "Step 3400 Loss 0.0223857\n",
      "Step 3500 Loss 0.310704\n",
      "Step 3600 Loss 0.0853975\n",
      "Step 3700 Loss 0.320486\n",
      "Step 3800 Loss 0.014658\n",
      "Step 3900 Loss 0.0128659\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 31\n",
      "Step 0 Loss 0.570928\n",
      "Step 100 Loss 0.0117235\n",
      "Step 200 Loss 0.0109072\n",
      "Step 300 Loss 0.00716436\n",
      "Step 400 Loss 0.00845466\n",
      "Step 500 Loss 0.321817\n",
      "Step 600 Loss 0.422276\n",
      "Step 700 Loss 0.103632\n",
      "Step 800 Loss 0.213857\n",
      "Step 900 Loss 0.195039\n",
      "Step 1000 Loss 0.0248402\n",
      "Step 1100 Loss 0.0139913\n",
      "Step 1200 Loss 0.0095989\n",
      "Step 1300 Loss 0.00921668\n",
      "Step 1400 Loss 0.201566\n",
      "Step 1500 Loss 0.0262712\n",
      "Step 1600 Loss 0.0145612\n",
      "Step 1700 Loss 0.0125171\n",
      "Step 1800 Loss 0.00929547\n",
      "Step 1900 Loss 0.00772564\n",
      "Step 2000 Loss 0.00758606\n",
      "Step 2100 Loss 0.00535866\n",
      "Step 2200 Loss 0.00651275\n",
      "Step 2300 Loss 0.00638826\n",
      "Step 2400 Loss 0.00639313\n",
      "Step 2500 Loss 0.116247\n",
      "Step 2600 Loss 0.191608\n",
      "Step 2700 Loss 0.166001\n",
      "Step 2800 Loss 0.00823596\n",
      "Step 2900 Loss 0.012977\n",
      "Step 3000 Loss 0.16907\n",
      "Step 3100 Loss 0.0063171\n",
      "Step 3200 Loss 0.246203\n",
      "Step 3300 Loss 0.312685\n",
      "Step 3400 Loss 0.0223729\n",
      "Step 3500 Loss 0.311108\n",
      "Step 3600 Loss 0.0854735\n",
      "Step 3700 Loss 0.320462\n",
      "Step 3800 Loss 0.0146856\n",
      "Step 3900 Loss 0.0129122\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 32\n",
      "Step 0 Loss 0.568781\n",
      "Step 100 Loss 0.0117581\n",
      "Step 200 Loss 0.0109475\n",
      "Step 300 Loss 0.0071832\n",
      "Step 400 Loss 0.00849514\n",
      "Step 500 Loss 0.321677\n",
      "Step 600 Loss 0.422954\n",
      "Step 700 Loss 0.1036\n",
      "Step 800 Loss 0.213984\n",
      "Step 900 Loss 0.195288\n",
      "Step 1000 Loss 0.0249091\n",
      "Step 1100 Loss 0.0140822\n",
      "Step 1200 Loss 0.00968206\n",
      "Step 1300 Loss 0.00930616\n",
      "Step 1400 Loss 0.201933\n",
      "Step 1500 Loss 0.0262972\n",
      "Step 1600 Loss 0.014651\n",
      "Step 1700 Loss 0.0126138\n",
      "Step 1800 Loss 0.00938666\n",
      "Step 1900 Loss 0.00780579\n",
      "Step 2000 Loss 0.00765852\n",
      "Step 2100 Loss 0.00541062\n",
      "Step 2200 Loss 0.00657308\n",
      "Step 2300 Loss 0.0064396\n",
      "Step 2400 Loss 0.00643118\n",
      "Step 2500 Loss 0.116152\n",
      "Step 2600 Loss 0.191581\n",
      "Step 2700 Loss 0.166239\n",
      "Step 2800 Loss 0.00824764\n",
      "Step 2900 Loss 0.0130114\n",
      "Step 3000 Loss 0.16907\n",
      "Step 3100 Loss 0.00634119\n",
      "Step 3200 Loss 0.246837\n",
      "Step 3300 Loss 0.313029\n",
      "Step 3400 Loss 0.0223572\n",
      "Step 3500 Loss 0.311512\n",
      "Step 3600 Loss 0.0855461\n",
      "Step 3700 Loss 0.320445\n",
      "Step 3800 Loss 0.0147115\n",
      "Step 3900 Loss 0.0129559\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 33\n",
      "Step 0 Loss 0.56656\n",
      "Step 100 Loss 0.0117919\n",
      "Step 200 Loss 0.0109871\n",
      "Step 300 Loss 0.00720224\n",
      "Step 400 Loss 0.00853484\n",
      "Step 500 Loss 0.321537\n",
      "Step 600 Loss 0.423607\n",
      "Step 700 Loss 0.103571\n",
      "Step 800 Loss 0.214113\n",
      "Step 900 Loss 0.195533\n",
      "Step 1000 Loss 0.0249726\n",
      "Step 1100 Loss 0.0141697\n",
      "Step 1200 Loss 0.00976277\n",
      "Step 1300 Loss 0.00939329\n",
      "Step 1400 Loss 0.202286\n",
      "Step 1500 Loss 0.0263188\n",
      "Step 1600 Loss 0.0147369\n",
      "Step 1700 Loss 0.0127072\n",
      "Step 1800 Loss 0.00947538\n",
      "Step 1900 Loss 0.00788404\n",
      "Step 2000 Loss 0.00772928\n",
      "Step 2100 Loss 0.00546158\n",
      "Step 2200 Loss 0.00663232\n",
      "Step 2300 Loss 0.00648985\n",
      "Step 2400 Loss 0.00646818\n",
      "Step 2500 Loss 0.11606\n",
      "Step 2600 Loss 0.191552\n",
      "Step 2700 Loss 0.166464\n",
      "Step 2800 Loss 0.00825996\n",
      "Step 2900 Loss 0.0130456\n",
      "Step 3000 Loss 0.16907\n",
      "Step 3100 Loss 0.00636498\n",
      "Step 3200 Loss 0.24744\n",
      "Step 3300 Loss 0.313415\n",
      "Step 3400 Loss 0.0223388\n",
      "Step 3500 Loss 0.311915\n",
      "Step 3600 Loss 0.0856156\n",
      "Step 3700 Loss 0.320433\n",
      "Step 3800 Loss 0.0147358\n",
      "Step 3900 Loss 0.012997\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 34\n",
      "Step 0 Loss 0.564272\n",
      "Step 100 Loss 0.0118248\n",
      "Step 200 Loss 0.0110259\n",
      "Step 300 Loss 0.00722136\n",
      "Step 400 Loss 0.00857384\n",
      "Step 500 Loss 0.321397\n",
      "Step 600 Loss 0.424235\n",
      "Step 700 Loss 0.103546\n",
      "Step 800 Loss 0.214243\n",
      "Step 900 Loss 0.195774\n",
      "Step 1000 Loss 0.0250308\n",
      "Step 1100 Loss 0.0142537\n",
      "Step 1200 Loss 0.0098409\n",
      "Step 1300 Loss 0.00947838\n",
      "Step 1400 Loss 0.202626\n",
      "Step 1500 Loss 0.0263366\n",
      "Step 1600 Loss 0.0148193\n",
      "Step 1700 Loss 0.0127979\n",
      "Step 1800 Loss 0.00956186\n",
      "Step 1900 Loss 0.00796042\n",
      "Step 2000 Loss 0.00779848\n",
      "Step 2100 Loss 0.00551168\n",
      "Step 2200 Loss 0.00669054\n",
      "Step 2300 Loss 0.0065393\n",
      "Step 2400 Loss 0.00650453\n",
      "Step 2500 Loss 0.11597\n",
      "Step 2600 Loss 0.191521\n",
      "Step 2700 Loss 0.166675\n",
      "Step 2800 Loss 0.00827312\n",
      "Step 2900 Loss 0.01308\n",
      "Step 3000 Loss 0.16907\n",
      "Step 3100 Loss 0.00638851\n",
      "Step 3200 Loss 0.248012\n",
      "Step 3300 Loss 0.313829\n",
      "Step 3400 Loss 0.0223182\n",
      "Step 3500 Loss 0.312317\n",
      "Step 3600 Loss 0.0856821\n",
      "Step 3700 Loss 0.320428\n",
      "Step 3800 Loss 0.0147589\n",
      "Step 3900 Loss 0.0130356\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 35\n",
      "Step 0 Loss 0.561915\n",
      "Step 100 Loss 0.0118566\n",
      "Step 200 Loss 0.0110638\n",
      "Step 300 Loss 0.00724034\n",
      "Step 400 Loss 0.00861198\n",
      "Step 500 Loss 0.32126\n",
      "Step 600 Loss 0.42484\n",
      "Step 700 Loss 0.103523\n",
      "Step 800 Loss 0.214376\n",
      "Step 900 Loss 0.196012\n",
      "Step 1000 Loss 0.0250845\n",
      "Step 1100 Loss 0.0143347\n",
      "Step 1200 Loss 0.00991682\n",
      "Step 1300 Loss 0.00956124\n",
      "Step 1400 Loss 0.202955\n",
      "Step 1500 Loss 0.0263506\n",
      "Step 1600 Loss 0.0148982\n",
      "Step 1700 Loss 0.0128857\n",
      "Step 1800 Loss 0.00964604\n",
      "Step 1900 Loss 0.00803509\n",
      "Step 2000 Loss 0.0078663\n",
      "Step 2100 Loss 0.00556075\n",
      "Step 2200 Loss 0.00674759\n",
      "Step 2300 Loss 0.00658792\n",
      "Step 2400 Loss 0.00654029\n",
      "Step 2500 Loss 0.115882\n",
      "Step 2600 Loss 0.191487\n",
      "Step 2700 Loss 0.166874\n",
      "Step 2800 Loss 0.0082871\n",
      "Step 2900 Loss 0.0131146\n",
      "Step 3000 Loss 0.169071\n",
      "Step 3100 Loss 0.00641181\n",
      "Step 3200 Loss 0.248555\n",
      "Step 3300 Loss 0.314264\n",
      "Step 3400 Loss 0.0222953\n",
      "Step 3500 Loss 0.312719\n",
      "Step 3600 Loss 0.0857456\n",
      "Step 3700 Loss 0.320428\n",
      "Step 3800 Loss 0.0147805\n",
      "Step 3900 Loss 0.0130718\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 36\n",
      "Step 0 Loss 0.559475\n",
      "Step 100 Loss 0.0118877\n",
      "Step 200 Loss 0.0111011\n",
      "Step 300 Loss 0.00725952\n",
      "Step 400 Loss 0.00864942\n",
      "Step 500 Loss 0.321124\n",
      "Step 600 Loss 0.425425\n",
      "Step 700 Loss 0.103503\n",
      "Step 800 Loss 0.214509\n",
      "Step 900 Loss 0.196246\n",
      "Step 1000 Loss 0.0251336\n",
      "Step 1100 Loss 0.0144127\n",
      "Step 1200 Loss 0.00999043\n",
      "Step 1300 Loss 0.00964229\n",
      "Step 1400 Loss 0.203271\n",
      "Step 1500 Loss 0.0263617\n",
      "Step 1600 Loss 0.014974\n",
      "Step 1700 Loss 0.012971\n",
      "Step 1800 Loss 0.00972802\n",
      "Step 1900 Loss 0.00810795\n",
      "Step 2000 Loss 0.00793273\n",
      "Step 2100 Loss 0.00560877\n",
      "Step 2200 Loss 0.00680363\n",
      "Step 2300 Loss 0.00663568\n",
      "Step 2400 Loss 0.00657557\n",
      "Step 2500 Loss 0.115797\n",
      "Step 2600 Loss 0.191452\n",
      "Step 2700 Loss 0.167062\n",
      "Step 2800 Loss 0.00830159\n",
      "Step 2900 Loss 0.013149\n",
      "Step 3000 Loss 0.169073\n",
      "Step 3100 Loss 0.00643478\n",
      "Step 3200 Loss 0.249073\n",
      "Step 3300 Loss 0.314713\n",
      "Step 3400 Loss 0.0222708\n",
      "Step 3500 Loss 0.31312\n",
      "Step 3600 Loss 0.0858061\n",
      "Step 3700 Loss 0.320435\n",
      "Step 3800 Loss 0.0148011\n",
      "Step 3900 Loss 0.0131056\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 37\n",
      "Step 0 Loss 0.556958\n",
      "Step 100 Loss 0.0119179\n",
      "Step 200 Loss 0.0111377\n",
      "Step 300 Loss 0.00727856\n",
      "Step 400 Loss 0.00868611\n",
      "Step 500 Loss 0.320991\n",
      "Step 600 Loss 0.425991\n",
      "Step 700 Loss 0.103486\n",
      "Step 800 Loss 0.214643\n",
      "Step 900 Loss 0.196477\n",
      "Step 1000 Loss 0.0251785\n",
      "Step 1100 Loss 0.014488\n",
      "Step 1200 Loss 0.0100619\n",
      "Step 1300 Loss 0.00972136\n",
      "Step 1400 Loss 0.203576\n",
      "Step 1500 Loss 0.0263701\n",
      "Step 1600 Loss 0.015047\n",
      "Step 1700 Loss 0.0130538\n",
      "Step 1800 Loss 0.00980782\n",
      "Step 1900 Loss 0.00817931\n",
      "Step 2000 Loss 0.00799784\n",
      "Step 2100 Loss 0.00565571\n",
      "Step 2200 Loss 0.0068586\n",
      "Step 2300 Loss 0.00668267\n",
      "Step 2400 Loss 0.00661028\n",
      "Step 2500 Loss 0.115714\n",
      "Step 2600 Loss 0.191415\n",
      "Step 2700 Loss 0.167239\n",
      "Step 2800 Loss 0.00831665\n",
      "Step 2900 Loss 0.0131838\n",
      "Step 3000 Loss 0.169075\n",
      "Step 3100 Loss 0.00645752\n",
      "Step 3200 Loss 0.249567\n",
      "Step 3300 Loss 0.31517\n",
      "Step 3400 Loss 0.0222448\n",
      "Step 3500 Loss 0.313521\n",
      "Step 3600 Loss 0.0858634\n",
      "Step 3700 Loss 0.32045\n",
      "Step 3800 Loss 0.0148205\n",
      "Step 3900 Loss 0.0131369\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 38\n",
      "Step 0 Loss 0.55435\n",
      "Step 100 Loss 0.0119471\n",
      "Step 200 Loss 0.0111738\n",
      "Step 300 Loss 0.00729738\n",
      "Step 400 Loss 0.00872219\n",
      "Step 500 Loss 0.320864\n",
      "Step 600 Loss 0.426541\n",
      "Step 700 Loss 0.10347\n",
      "Step 800 Loss 0.214778\n",
      "Step 900 Loss 0.196704\n",
      "Step 1000 Loss 0.0252192\n",
      "Step 1100 Loss 0.0145606\n",
      "Step 1200 Loss 0.0101312\n",
      "Step 1300 Loss 0.00979879\n",
      "Step 1400 Loss 0.203871\n",
      "Step 1500 Loss 0.0263759\n",
      "Step 1600 Loss 0.0151168\n",
      "Step 1700 Loss 0.013134\n",
      "Step 1800 Loss 0.00988557\n",
      "Step 1900 Loss 0.00824914\n",
      "Step 2000 Loss 0.00806183\n",
      "Step 2100 Loss 0.00570142\n",
      "Step 2200 Loss 0.0069127\n",
      "Step 2300 Loss 0.00672894\n",
      "Step 2400 Loss 0.00664446\n",
      "Step 2500 Loss 0.115633\n",
      "Step 2600 Loss 0.191378\n",
      "Step 2700 Loss 0.167408\n",
      "Step 2800 Loss 0.00833211\n",
      "Step 2900 Loss 0.0132185\n",
      "Step 3000 Loss 0.169078\n",
      "Step 3100 Loss 0.00647987\n",
      "Step 3200 Loss 0.250037\n",
      "Step 3300 Loss 0.315631\n",
      "Step 3400 Loss 0.0222181\n",
      "Step 3500 Loss 0.31392\n",
      "Step 3600 Loss 0.085917\n",
      "Step 3700 Loss 0.320477\n",
      "Step 3800 Loss 0.014839\n",
      "Step 3900 Loss 0.0131652\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 39\n",
      "Step 0 Loss 0.551644\n",
      "Step 100 Loss 0.0119754\n",
      "Step 200 Loss 0.0112095\n",
      "Step 300 Loss 0.00731555\n",
      "Step 400 Loss 0.00875768\n",
      "Step 500 Loss 0.320744\n",
      "Step 600 Loss 0.427077\n",
      "Step 700 Loss 0.103457\n",
      "Step 800 Loss 0.214914\n",
      "Step 900 Loss 0.196929\n",
      "Step 1000 Loss 0.025256\n",
      "Step 1100 Loss 0.0146307\n",
      "Step 1200 Loss 0.0101983\n",
      "Step 1300 Loss 0.00987434\n",
      "Step 1400 Loss 0.204156\n",
      "Step 1500 Loss 0.0263795\n",
      "Step 1600 Loss 0.0151839\n",
      "Step 1700 Loss 0.0132121\n",
      "Step 1800 Loss 0.0099616\n",
      "Step 1900 Loss 0.00831747\n",
      "Step 2000 Loss 0.00812461\n",
      "Step 2100 Loss 0.00574593\n",
      "Step 2200 Loss 0.00696599\n",
      "Step 2300 Loss 0.00677457\n",
      "Step 2400 Loss 0.00667841\n",
      "Step 2500 Loss 0.115551\n",
      "Step 2600 Loss 0.19134\n",
      "Step 2700 Loss 0.167567\n",
      "Step 2800 Loss 0.0083483\n",
      "Step 2900 Loss 0.013254\n",
      "Step 3000 Loss 0.169082\n",
      "Step 3100 Loss 0.00650219\n",
      "Step 3200 Loss 0.250484\n",
      "Step 3300 Loss 0.316091\n",
      "Step 3400 Loss 0.0221911\n",
      "Step 3500 Loss 0.314321\n",
      "Step 3600 Loss 0.0859661\n",
      "Step 3700 Loss 0.320514\n",
      "Step 3800 Loss 0.0148565\n",
      "Step 3900 Loss 0.013191\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 40\n",
      "Step 0 Loss 0.548778\n",
      "Step 100 Loss 0.0120025\n",
      "Step 200 Loss 0.011245\n",
      "Step 300 Loss 0.00733319\n",
      "Step 400 Loss 0.00879261\n",
      "Step 500 Loss 0.320631\n",
      "Step 600 Loss 0.427601\n",
      "Step 700 Loss 0.103446\n",
      "Step 800 Loss 0.21505\n",
      "Step 900 Loss 0.19715\n",
      "Step 1000 Loss 0.025289\n",
      "Step 1100 Loss 0.0146983\n",
      "Step 1200 Loss 0.0102635\n",
      "Step 1300 Loss 0.00994831\n",
      "Step 1400 Loss 0.204431\n",
      "Step 1500 Loss 0.0263813\n",
      "Step 1600 Loss 0.0152487\n",
      "Step 1700 Loss 0.0132884\n",
      "Step 1800 Loss 0.0100358\n",
      "Step 1900 Loss 0.00838438\n",
      "Step 2000 Loss 0.00818649\n",
      "Step 2100 Loss 0.00578959\n",
      "Step 2200 Loss 0.00701871\n",
      "Step 2300 Loss 0.00681962\n",
      "Step 2400 Loss 0.00671205\n",
      "Step 2500 Loss 0.11547\n",
      "Step 2600 Loss 0.191302\n",
      "Step 2700 Loss 0.167718\n",
      "Step 2800 Loss 0.00836503\n",
      "Step 2900 Loss 0.01329\n",
      "Step 3000 Loss 0.169087\n",
      "Step 3100 Loss 0.0065243\n",
      "Step 3200 Loss 0.250911\n",
      "Step 3300 Loss 0.316548\n",
      "Step 3400 Loss 0.0221629\n",
      "Step 3500 Loss 0.314727\n",
      "Step 3600 Loss 0.0860092\n",
      "Step 3700 Loss 0.320559\n",
      "Step 3800 Loss 0.0148727\n",
      "Step 3900 Loss 0.0132145\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 41\n",
      "Step 0 Loss 0.545651\n",
      "Step 100 Loss 0.012028\n",
      "Step 200 Loss 0.0112807\n",
      "Step 300 Loss 0.0073505\n",
      "Step 400 Loss 0.00882708\n",
      "Step 500 Loss 0.320526\n",
      "Step 600 Loss 0.428115\n",
      "Step 700 Loss 0.103437\n",
      "Step 800 Loss 0.215187\n",
      "Step 900 Loss 0.197369\n",
      "Step 1000 Loss 0.0253185\n",
      "Step 1100 Loss 0.0147639\n",
      "Step 1200 Loss 0.0103271\n",
      "Step 1300 Loss 0.0100207\n",
      "Step 1400 Loss 0.204697\n",
      "Step 1500 Loss 0.0263815\n",
      "Step 1600 Loss 0.0153113\n",
      "Step 1700 Loss 0.0133632\n",
      "Step 1800 Loss 0.0101084\n",
      "Step 1900 Loss 0.00845015\n",
      "Step 2000 Loss 0.00824767\n",
      "Step 2100 Loss 0.00583296\n",
      "Step 2200 Loss 0.00707097\n",
      "Step 2300 Loss 0.00686449\n",
      "Step 2400 Loss 0.00674573\n",
      "Step 2500 Loss 0.115388\n",
      "Step 2600 Loss 0.191265\n",
      "Step 2700 Loss 0.167862\n",
      "Step 2800 Loss 0.00838253\n",
      "Step 2900 Loss 0.0133275\n",
      "Step 3000 Loss 0.169093\n",
      "Step 3100 Loss 0.00654619\n",
      "Step 3200 Loss 0.25132\n",
      "Step 3300 Loss 0.317002\n",
      "Step 3400 Loss 0.0221333\n",
      "Step 3500 Loss 0.315147\n",
      "Step 3600 Loss 0.0860438\n",
      "Step 3700 Loss 0.32061\n",
      "Step 3800 Loss 0.0148862\n",
      "Step 3900 Loss 0.0132353\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 42\n",
      "Step 0 Loss 0.542123\n",
      "Step 100 Loss 0.0120508\n",
      "Step 200 Loss 0.0113165\n",
      "Step 300 Loss 0.0073677\n",
      "Step 400 Loss 0.00886124\n",
      "Step 500 Loss 0.320431\n",
      "Step 600 Loss 0.42862\n",
      "Step 700 Loss 0.10343\n",
      "Step 800 Loss 0.215326\n",
      "Step 900 Loss 0.197587\n",
      "Step 1000 Loss 0.0253448\n",
      "Step 1100 Loss 0.0148279\n",
      "Step 1200 Loss 0.0103899\n",
      "Step 1300 Loss 0.0100916\n",
      "Step 1400 Loss 0.204956\n",
      "Step 1500 Loss 0.0263808\n",
      "Step 1600 Loss 0.015372\n",
      "Step 1700 Loss 0.0134368\n",
      "Step 1800 Loss 0.0101794\n",
      "Step 1900 Loss 0.00851473\n",
      "Step 2000 Loss 0.00830848\n",
      "Step 2100 Loss 0.0058762\n",
      "Step 2200 Loss 0.00712309\n",
      "Step 2300 Loss 0.00690905\n",
      "Step 2400 Loss 0.00677937\n",
      "Step 2500 Loss 0.115305\n",
      "Step 2600 Loss 0.19123\n",
      "Step 2700 Loss 0.167999\n",
      "Step 2800 Loss 0.0084009\n",
      "Step 2900 Loss 0.0133668\n",
      "Step 3000 Loss 0.169103\n",
      "Step 3100 Loss 0.0065681\n",
      "Step 3200 Loss 0.251711\n",
      "Step 3300 Loss 0.317448\n",
      "Step 3400 Loss 0.0221013\n",
      "Step 3500 Loss 0.315601\n",
      "Step 3600 Loss 0.0860657\n",
      "Step 3700 Loss 0.320665\n",
      "Step 3800 Loss 0.0148954\n",
      "Step 3900 Loss 0.013254\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 43\n",
      "Step 0 Loss 0.537919\n",
      "Step 100 Loss 0.0120696\n",
      "Step 200 Loss 0.0113532\n",
      "Step 300 Loss 0.00738448\n",
      "Step 400 Loss 0.00889498\n",
      "Step 500 Loss 0.320355\n",
      "Step 600 Loss 0.429127\n",
      "Step 700 Loss 0.103426\n",
      "Step 800 Loss 0.215469\n",
      "Step 900 Loss 0.197805\n",
      "Step 1000 Loss 0.0253681\n",
      "Step 1100 Loss 0.0148912\n",
      "Step 1200 Loss 0.0104519\n",
      "Step 1300 Loss 0.0101616\n",
      "Step 1400 Loss 0.205206\n",
      "Step 1500 Loss 0.0263799\n",
      "Step 1600 Loss 0.0154316\n",
      "Step 1700 Loss 0.01351\n",
      "Step 1800 Loss 0.0102491\n",
      "Step 1900 Loss 0.00857866\n",
      "Step 2000 Loss 0.00836946\n",
      "Step 2100 Loss 0.00591962\n",
      "Step 2200 Loss 0.0071757\n",
      "Step 2300 Loss 0.0069539\n",
      "Step 2400 Loss 0.00681355\n",
      "Step 2500 Loss 0.115219\n",
      "Step 2600 Loss 0.191199\n",
      "Step 2700 Loss 0.168129\n",
      "Step 2800 Loss 0.00842071\n",
      "Step 2900 Loss 0.0134098\n",
      "Step 3000 Loss 0.169118\n",
      "Step 3100 Loss 0.00658993\n",
      "Step 3200 Loss 0.252084\n",
      "Step 3300 Loss 0.317883\n",
      "Step 3400 Loss 0.0220658\n",
      "Step 3500 Loss 0.316144\n",
      "Step 3600 Loss 0.0860661\n",
      "Step 3700 Loss 0.320707\n",
      "Step 3800 Loss 0.0148941\n",
      "Step 3900 Loss 0.0132713\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 44\n",
      "Step 0 Loss 0.532435\n",
      "Step 100 Loss 0.01208\n",
      "Step 200 Loss 0.0113909\n",
      "Step 300 Loss 0.00740018\n",
      "Step 400 Loss 0.00892793\n",
      "Step 500 Loss 0.320323\n",
      "Step 600 Loss 0.429646\n",
      "Step 700 Loss 0.103427\n",
      "Step 800 Loss 0.215617\n",
      "Step 900 Loss 0.198025\n",
      "Step 1000 Loss 0.0253886\n",
      "Step 1100 Loss 0.0149553\n",
      "Step 1200 Loss 0.0105145\n",
      "Step 1300 Loss 0.0102312\n",
      "Step 1400 Loss 0.20545\n",
      "Step 1500 Loss 0.0263802\n",
      "Step 1600 Loss 0.0154907\n",
      "Step 1700 Loss 0.0135839\n",
      "Step 1800 Loss 0.0103178\n",
      "Step 1900 Loss 0.00864203\n",
      "Step 2000 Loss 0.00843166\n",
      "Step 2100 Loss 0.00596378\n",
      "Step 2200 Loss 0.00722981\n",
      "Step 2300 Loss 0.00699985\n",
      "Step 2400 Loss 0.00684867\n",
      "Step 2500 Loss 0.115126\n",
      "Step 2600 Loss 0.191176\n",
      "Step 2700 Loss 0.168254\n",
      "Step 2800 Loss 0.00844288\n",
      "Step 2900 Loss 0.0134595\n",
      "Step 3000 Loss 0.169142\n",
      "Step 3100 Loss 0.00661226\n",
      "Step 3200 Loss 0.252442\n",
      "Step 3300 Loss 0.318304\n",
      "Step 3400 Loss 0.0220236\n",
      "Step 3500 Loss 0.317009\n",
      "Step 3600 Loss 0.0860223\n",
      "Step 3700 Loss 0.320636\n",
      "Step 3800 Loss 0.01486\n",
      "Step 3900 Loss 0.0132904\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 45\n",
      "Step 0 Loss 0.523741\n",
      "Step 100 Loss 0.0120676\n",
      "Step 200 Loss 0.0114308\n",
      "Step 300 Loss 0.00741262\n",
      "Step 400 Loss 0.00895926\n",
      "Step 500 Loss 0.320398\n",
      "Step 600 Loss 0.43021\n",
      "Step 700 Loss 0.103435\n",
      "Step 800 Loss 0.215784\n",
      "Step 900 Loss 0.198257\n",
      "Step 1000 Loss 0.0254062\n",
      "Step 1100 Loss 0.0150233\n",
      "Step 1200 Loss 0.0105803\n",
      "Step 1300 Loss 0.0103014\n",
      "Step 1400 Loss 0.20569\n",
      "Step 1500 Loss 0.0263852\n",
      "Step 1600 Loss 0.0155519\n",
      "Step 1700 Loss 0.0136628\n",
      "Step 1800 Loss 0.0103862\n",
      "Step 1900 Loss 0.00870628\n",
      "Step 2000 Loss 0.00849839\n",
      "Step 2100 Loss 0.00601093\n",
      "Step 2200 Loss 0.00728856\n",
      "Step 2300 Loss 0.00704893\n",
      "Step 2400 Loss 0.00688646\n",
      "Step 2500 Loss 0.115015\n",
      "Step 2600 Loss 0.191172\n",
      "Step 2700 Loss 0.168373\n",
      "Step 2800 Loss 0.00847049\n",
      "Step 2900 Loss 0.0135254\n",
      "Step 3000 Loss 0.169188\n",
      "Step 3100 Loss 0.00663563\n",
      "Step 3200 Loss 0.252785\n",
      "Step 3300 Loss 0.318698\n",
      "Step 3400 Loss 0.0219668\n",
      "Step 3500 Loss 0.319447\n",
      "Step 3600 Loss 0.0858928\n",
      "Step 3700 Loss 0.320123\n",
      "Step 3800 Loss 0.0147395\n",
      "Step 3900 Loss 0.0133016\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 46\n",
      "Step 0 Loss 0.507811\n",
      "Step 100 Loss 0.0119984\n",
      "Step 200 Loss 0.0114726\n",
      "Step 300 Loss 0.00741673\n",
      "Step 400 Loss 0.0089851\n",
      "Step 500 Loss 0.320753\n",
      "Step 600 Loss 0.430908\n",
      "Step 700 Loss 0.103464\n",
      "Step 800 Loss 0.215998\n",
      "Step 900 Loss 0.198521\n",
      "Step 1000 Loss 0.0254199\n",
      "Step 1100 Loss 0.0151017\n",
      "Step 1200 Loss 0.010655\n",
      "Step 1300 Loss 0.0103757\n",
      "Step 1400 Loss 0.205929\n",
      "Step 1500 Loss 0.0264024\n",
      "Step 1600 Loss 0.0156191\n",
      "Step 1700 Loss 0.0137532\n",
      "Step 1800 Loss 0.0104556\n",
      "Step 1900 Loss 0.00877392\n",
      "Step 2000 Loss 0.0085766\n",
      "Step 2100 Loss 0.00606543\n",
      "Step 2200 Loss 0.00735891\n",
      "Step 2300 Loss 0.00710642\n",
      "Step 2400 Loss 0.00693303\n",
      "Step 2500 Loss 0.114869\n",
      "Step 2600 Loss 0.191209\n",
      "Step 2700 Loss 0.168488\n",
      "Step 2800 Loss 0.00851016\n",
      "Step 2900 Loss 0.0136239\n",
      "Step 3000 Loss 0.169283\n",
      "Step 3100 Loss 0.00666154\n",
      "Step 3200 Loss 0.253121\n",
      "Step 3300 Loss 0.319051\n",
      "Step 3400 Loss 0.0218737\n",
      "Step 3500 Loss 0.322029\n",
      "Step 3600 Loss 0.0858077\n",
      "Step 3700 Loss 0.320015\n",
      "Step 3800 Loss 0.0146513\n",
      "Step 3900 Loss 0.0132946\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 47\n",
      "Step 0 Loss 0.499579\n",
      "Step 100 Loss 0.0119519\n",
      "Step 200 Loss 0.0114932\n",
      "Step 300 Loss 0.00741631\n",
      "Step 400 Loss 0.00900435\n",
      "Step 500 Loss 0.321152\n",
      "Step 600 Loss 0.431569\n",
      "Step 700 Loss 0.103485\n",
      "Step 800 Loss 0.216219\n",
      "Step 900 Loss 0.19877\n",
      "Step 1000 Loss 0.0254345\n",
      "Step 1100 Loss 0.0151749\n",
      "Step 1200 Loss 0.0107117\n",
      "Step 1300 Loss 0.0104475\n",
      "Step 1400 Loss 0.206152\n",
      "Step 1500 Loss 0.0264034\n",
      "Step 1600 Loss 0.0156726\n",
      "Step 1700 Loss 0.0138268\n",
      "Step 1800 Loss 0.0105195\n",
      "Step 1900 Loss 0.00883619\n",
      "Step 2000 Loss 0.00864381\n",
      "Step 2100 Loss 0.00610982\n",
      "Step 2200 Loss 0.00741804\n",
      "Step 2300 Loss 0.0071559\n",
      "Step 2400 Loss 0.00697756\n",
      "Step 2500 Loss 0.114773\n",
      "Step 2600 Loss 0.191199\n",
      "Step 2700 Loss 0.168591\n",
      "Step 2800 Loss 0.00854132\n",
      "Step 2900 Loss 0.0136847\n",
      "Step 3000 Loss 0.169351\n",
      "Step 3100 Loss 0.00668417\n",
      "Step 3200 Loss 0.253461\n",
      "Step 3300 Loss 0.319445\n",
      "Step 3400 Loss 0.0217932\n",
      "Step 3500 Loss 0.32311\n",
      "Step 3600 Loss 0.0858217\n",
      "Step 3700 Loss 0.320195\n",
      "Step 3800 Loss 0.0146407\n",
      "Step 3900 Loss 0.0133117\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 48\n",
      "Step 0 Loss 0.495818\n",
      "Step 100 Loss 0.0119546\n",
      "Step 200 Loss 0.0115182\n",
      "Step 300 Loss 0.00743013\n",
      "Step 400 Loss 0.00903058\n",
      "Step 500 Loss 0.321144\n",
      "Step 600 Loss 0.432047\n",
      "Step 700 Loss 0.103489\n",
      "Step 800 Loss 0.216379\n",
      "Step 900 Loss 0.198986\n",
      "Step 1000 Loss 0.0254382\n",
      "Step 1100 Loss 0.0152187\n",
      "Step 1200 Loss 0.0107632\n",
      "Step 1300 Loss 0.0105077\n",
      "Step 1400 Loss 0.20637\n",
      "Step 1500 Loss 0.0263933\n",
      "Step 1600 Loss 0.0157204\n",
      "Step 1700 Loss 0.0138883\n",
      "Step 1800 Loss 0.0105807\n",
      "Step 1900 Loss 0.00889346\n",
      "Step 2000 Loss 0.00869941\n",
      "Step 2100 Loss 0.00614979\n",
      "Step 2200 Loss 0.00746623\n",
      "Step 2300 Loss 0.00719759\n",
      "Step 2400 Loss 0.00701488\n",
      "Step 2500 Loss 0.114712\n",
      "Step 2600 Loss 0.191157\n",
      "Step 2700 Loss 0.168691\n",
      "Step 2800 Loss 0.00856227\n",
      "Step 2900 Loss 0.0137169\n",
      "Step 3000 Loss 0.169374\n",
      "Step 3100 Loss 0.00670416\n",
      "Step 3200 Loss 0.253785\n",
      "Step 3300 Loss 0.319866\n",
      "Step 3400 Loss 0.0217436\n",
      "Step 3500 Loss 0.323779\n",
      "Step 3600 Loss 0.0858529\n",
      "Step 3700 Loss 0.320256\n",
      "Step 3800 Loss 0.014638\n",
      "Step 3900 Loss 0.0133283\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 49\n",
      "Step 0 Loss 0.493857\n",
      "Step 100 Loss 0.0119613\n",
      "Step 200 Loss 0.0115409\n",
      "Step 300 Loss 0.00744091\n",
      "Step 400 Loss 0.00905596\n",
      "Step 500 Loss 0.32121\n",
      "Step 600 Loss 0.432473\n",
      "Step 700 Loss 0.103496\n",
      "Step 800 Loss 0.216547\n",
      "Step 900 Loss 0.199203\n",
      "Step 1000 Loss 0.0254453\n",
      "Step 1100 Loss 0.01527\n",
      "Step 1200 Loss 0.0108112\n",
      "Step 1300 Loss 0.0105688\n",
      "Step 1400 Loss 0.20658\n",
      "Step 1500 Loss 0.0263804\n",
      "Step 1600 Loss 0.0157649\n",
      "Step 1700 Loss 0.0139472\n",
      "Step 1800 Loss 0.01064\n",
      "Step 1900 Loss 0.00894919\n",
      "Step 2000 Loss 0.00875298\n",
      "Step 2100 Loss 0.00618695\n",
      "Step 2200 Loss 0.0075129\n",
      "Step 2300 Loss 0.0072378\n",
      "Step 2400 Loss 0.00705045\n",
      "Step 2500 Loss 0.114643\n",
      "Step 2600 Loss 0.191117\n",
      "Step 2700 Loss 0.168789\n",
      "Step 2800 Loss 0.00858179\n",
      "Step 2900 Loss 0.0137495\n",
      "Step 3000 Loss 0.169389\n",
      "Step 3100 Loss 0.00672408\n",
      "Step 3200 Loss 0.25409\n",
      "Step 3300 Loss 0.32028\n",
      "Step 3400 Loss 0.0217041\n",
      "Step 3500 Loss 0.324316\n",
      "Step 3600 Loss 0.0858943\n",
      "Step 3700 Loss 0.320346\n",
      "Step 3800 Loss 0.014643\n",
      "Step 3900 Loss 0.0133445\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 50\n",
      "Step 0 Loss 0.491541\n",
      "Step 100 Loss 0.0119738\n",
      "Step 200 Loss 0.011566\n",
      "Step 300 Loss 0.00745655\n",
      "Step 400 Loss 0.00908311\n",
      "Step 500 Loss 0.321131\n",
      "Step 600 Loss 0.432859\n",
      "Step 700 Loss 0.1035\n",
      "Step 800 Loss 0.216695\n",
      "Step 900 Loss 0.199411\n",
      "Step 1000 Loss 0.0254449\n",
      "Step 1100 Loss 0.0153098\n",
      "Step 1200 Loss 0.0108603\n",
      "Step 1300 Loss 0.0106252\n",
      "Step 1400 Loss 0.206787\n",
      "Step 1500 Loss 0.0263667\n",
      "Step 1600 Loss 0.015809\n",
      "Step 1700 Loss 0.0140043\n",
      "Step 1800 Loss 0.0106986\n",
      "Step 1900 Loss 0.00900364\n",
      "Step 2000 Loss 0.00880453\n",
      "Step 2100 Loss 0.00622497\n",
      "Step 2200 Loss 0.0075579\n",
      "Step 2300 Loss 0.00727687\n",
      "Step 2400 Loss 0.00708467\n",
      "Step 2500 Loss 0.114579\n",
      "Step 2600 Loss 0.191073\n",
      "Step 2700 Loss 0.168882\n",
      "Step 2800 Loss 0.00860025\n",
      "Step 2900 Loss 0.0137792\n",
      "Step 3000 Loss 0.169397\n",
      "Step 3100 Loss 0.00674375\n",
      "Step 3200 Loss 0.254381\n",
      "Step 3300 Loss 0.320692\n",
      "Step 3400 Loss 0.0216694\n",
      "Step 3500 Loss 0.324792\n",
      "Step 3600 Loss 0.0859358\n",
      "Step 3700 Loss 0.320398\n",
      "Step 3800 Loss 0.0146475\n",
      "Step 3900 Loss 0.013358\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 51\n",
      "Step 0 Loss 0.490213\n",
      "Step 100 Loss 0.0119864\n",
      "Step 200 Loss 0.0115883\n",
      "Step 300 Loss 0.00746938\n",
      "Step 400 Loss 0.00910835\n",
      "Step 500 Loss 0.3211\n",
      "Step 600 Loss 0.433232\n",
      "Step 700 Loss 0.103506\n",
      "Step 800 Loss 0.216849\n",
      "Step 900 Loss 0.19962\n",
      "Step 1000 Loss 0.0254457\n",
      "Step 1100 Loss 0.0153536\n",
      "Step 1200 Loss 0.0109066\n",
      "Step 1300 Loss 0.0106819\n",
      "Step 1400 Loss 0.206988\n",
      "Step 1500 Loss 0.0263516\n",
      "Step 1600 Loss 0.0158509\n",
      "Step 1700 Loss 0.0140597\n",
      "Step 1800 Loss 0.0107558\n",
      "Step 1900 Loss 0.00905685\n",
      "Step 2000 Loss 0.00885479\n",
      "Step 2100 Loss 0.00626123\n",
      "Step 2200 Loss 0.00760207\n",
      "Step 2300 Loss 0.00731513\n",
      "Step 2400 Loss 0.00711789\n",
      "Step 2500 Loss 0.114513\n",
      "Step 2600 Loss 0.191028\n",
      "Step 2700 Loss 0.168971\n",
      "Step 2800 Loss 0.00861833\n",
      "Step 2900 Loss 0.0138092\n",
      "Step 3000 Loss 0.169402\n",
      "Step 3100 Loss 0.00676327\n",
      "Step 3200 Loss 0.254659\n",
      "Step 3300 Loss 0.321097\n",
      "Step 3400 Loss 0.0216384\n",
      "Step 3500 Loss 0.325272\n",
      "Step 3600 Loss 0.0859773\n",
      "Step 3700 Loss 0.320455\n",
      "Step 3800 Loss 0.0146515\n",
      "Step 3900 Loss 0.0133685\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 52\n",
      "Step 0 Loss 0.488431\n",
      "Step 100 Loss 0.0119994\n",
      "Step 200 Loss 0.0116112\n",
      "Step 300 Loss 0.00748411\n",
      "Step 400 Loss 0.00913392\n",
      "Step 500 Loss 0.321007\n",
      "Step 600 Loss 0.43359\n",
      "Step 700 Loss 0.103511\n",
      "Step 800 Loss 0.216994\n",
      "Step 900 Loss 0.199823\n",
      "Step 1000 Loss 0.0254421\n",
      "Step 1100 Loss 0.0153913\n",
      "Step 1200 Loss 0.0109533\n",
      "Step 1300 Loss 0.0107358\n",
      "Step 1400 Loss 0.207184\n",
      "Step 1500 Loss 0.0263357\n",
      "Step 1600 Loss 0.015892\n",
      "Step 1700 Loss 0.0141138\n",
      "Step 1800 Loss 0.0108122\n",
      "Step 1900 Loss 0.00910912\n",
      "Step 2000 Loss 0.00890399\n",
      "Step 2100 Loss 0.0062977\n",
      "Step 2200 Loss 0.00764518\n",
      "Step 2300 Loss 0.00735266\n",
      "Step 2400 Loss 0.00715044\n",
      "Step 2500 Loss 0.114449\n",
      "Step 2600 Loss 0.190981\n",
      "Step 2700 Loss 0.169055\n",
      "Step 2800 Loss 0.008636\n",
      "Step 2900 Loss 0.0138387\n",
      "Step 3000 Loss 0.169405\n",
      "Step 3100 Loss 0.00678268\n",
      "Step 3200 Loss 0.254925\n",
      "Step 3300 Loss 0.321496\n",
      "Step 3400 Loss 0.0216102\n",
      "Step 3500 Loss 0.325733\n",
      "Step 3600 Loss 0.0860171\n",
      "Step 3700 Loss 0.320489\n",
      "Step 3800 Loss 0.0146537\n",
      "Step 3900 Loss 0.013377\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 53\n",
      "Step 0 Loss 0.48714\n",
      "Step 100 Loss 0.012012\n",
      "Step 200 Loss 0.0116319\n",
      "Step 300 Loss 0.00749694\n",
      "Step 400 Loss 0.00915825\n",
      "Step 500 Loss 0.320933\n",
      "Step 600 Loss 0.433936\n",
      "Step 700 Loss 0.103516\n",
      "Step 800 Loss 0.217141\n",
      "Step 900 Loss 0.200026\n",
      "Step 1000 Loss 0.0254386\n",
      "Step 1100 Loss 0.0154304\n",
      "Step 1200 Loss 0.010998\n",
      "Step 1300 Loss 0.0107891\n",
      "Step 1400 Loss 0.207375\n",
      "Step 1500 Loss 0.026319\n",
      "Step 1600 Loss 0.0159314\n",
      "Step 1700 Loss 0.0141663\n",
      "Step 1800 Loss 0.0108671\n",
      "Step 1900 Loss 0.00916037\n",
      "Step 2000 Loss 0.00895209\n",
      "Step 2100 Loss 0.00633308\n",
      "Step 2200 Loss 0.00768762\n",
      "Step 2300 Loss 0.00738961\n",
      "Step 2400 Loss 0.00718224\n",
      "Step 2500 Loss 0.114382\n",
      "Step 2600 Loss 0.190934\n",
      "Step 2700 Loss 0.169135\n",
      "Step 2800 Loss 0.00865368\n",
      "Step 2900 Loss 0.0138687\n",
      "Step 3000 Loss 0.169408\n",
      "Step 3100 Loss 0.00680188\n",
      "Step 3200 Loss 0.255179\n",
      "Step 3300 Loss 0.321887\n",
      "Step 3400 Loss 0.0215843\n",
      "Step 3500 Loss 0.326172\n",
      "Step 3600 Loss 0.0860576\n",
      "Step 3700 Loss 0.320521\n",
      "Step 3800 Loss 0.014656\n",
      "Step 3900 Loss 0.0133841\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 54\n",
      "Step 0 Loss 0.48564\n",
      "Step 100 Loss 0.0120246\n",
      "Step 200 Loss 0.0116527\n",
      "Step 300 Loss 0.0075106\n",
      "Step 400 Loss 0.00918228\n",
      "Step 500 Loss 0.320835\n",
      "Step 600 Loss 0.434269\n",
      "Step 700 Loss 0.103522\n",
      "Step 800 Loss 0.217283\n",
      "Step 900 Loss 0.200226\n",
      "Step 1000 Loss 0.0254324\n",
      "Step 1100 Loss 0.0154662\n",
      "Step 1200 Loss 0.0110422\n",
      "Step 1300 Loss 0.0108406\n",
      "Step 1400 Loss 0.207562\n",
      "Step 1500 Loss 0.0263015\n",
      "Step 1600 Loss 0.0159698\n",
      "Step 1700 Loss 0.0142176\n",
      "Step 1800 Loss 0.010921\n",
      "Step 1900 Loss 0.00921043\n",
      "Step 2000 Loss 0.00899916\n",
      "Step 2100 Loss 0.00636817\n",
      "Step 2200 Loss 0.00772917\n",
      "Step 2300 Loss 0.00742575\n",
      "Step 2400 Loss 0.00721338\n",
      "Step 2500 Loss 0.114317\n",
      "Step 2600 Loss 0.190886\n",
      "Step 2700 Loss 0.169211\n",
      "Step 2800 Loss 0.00867091\n",
      "Step 2900 Loss 0.0138982\n",
      "Step 3000 Loss 0.169409\n",
      "Step 3100 Loss 0.00682083\n",
      "Step 3200 Loss 0.255423\n",
      "Step 3300 Loss 0.322271\n",
      "Step 3400 Loss 0.0215602\n",
      "Step 3500 Loss 0.326582\n",
      "Step 3600 Loss 0.0860976\n",
      "Step 3700 Loss 0.32054\n",
      "Step 3800 Loss 0.0146581\n",
      "Step 3900 Loss 0.0133903\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 55\n",
      "Step 0 Loss 0.484363\n",
      "Step 100 Loss 0.0120372\n",
      "Step 200 Loss 0.0116725\n",
      "Step 300 Loss 0.00752323\n",
      "Step 400 Loss 0.00920568\n",
      "Step 500 Loss 0.32074\n",
      "Step 600 Loss 0.434592\n",
      "Step 700 Loss 0.103528\n",
      "Step 800 Loss 0.217425\n",
      "Step 900 Loss 0.200423\n",
      "Step 1000 Loss 0.0254257\n",
      "Step 1100 Loss 0.015502\n",
      "Step 1200 Loss 0.011085\n",
      "Step 1300 Loss 0.0108914\n",
      "Step 1400 Loss 0.207744\n",
      "Step 1500 Loss 0.0262834\n",
      "Step 1600 Loss 0.0160065\n",
      "Step 1700 Loss 0.0142675\n",
      "Step 1800 Loss 0.0109739\n",
      "Step 1900 Loss 0.00925974\n",
      "Step 2000 Loss 0.00904542\n",
      "Step 2100 Loss 0.00640237\n",
      "Step 2200 Loss 0.00777005\n",
      "Step 2300 Loss 0.00746134\n",
      "Step 2400 Loss 0.00724388\n",
      "Step 2500 Loss 0.114251\n",
      "Step 2600 Loss 0.190839\n",
      "Step 2700 Loss 0.169284\n",
      "Step 2800 Loss 0.00868802\n",
      "Step 2900 Loss 0.0139276\n",
      "Step 3000 Loss 0.16941\n",
      "Step 3100 Loss 0.00683954\n",
      "Step 3200 Loss 0.255657\n",
      "Step 3300 Loss 0.322647\n",
      "Step 3400 Loss 0.0215372\n",
      "Step 3500 Loss 0.326977\n",
      "Step 3600 Loss 0.0861374\n",
      "Step 3700 Loss 0.320554\n",
      "Step 3800 Loss 0.0146605\n",
      "Step 3900 Loss 0.0133969\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 56\n",
      "Step 0 Loss 0.482938\n",
      "Step 100 Loss 0.0120499\n",
      "Step 200 Loss 0.0116925\n",
      "Step 300 Loss 0.00753638\n",
      "Step 400 Loss 0.00922896\n",
      "Step 500 Loss 0.320634\n",
      "Step 600 Loss 0.434904\n",
      "Step 700 Loss 0.103534\n",
      "Step 800 Loss 0.217565\n",
      "Step 900 Loss 0.200619\n",
      "Step 1000 Loss 0.0254173\n",
      "Step 1100 Loss 0.0155359\n",
      "Step 1200 Loss 0.0111272\n",
      "Step 1300 Loss 0.010941\n",
      "Step 1400 Loss 0.207922\n",
      "Step 1500 Loss 0.0262649\n",
      "Step 1600 Loss 0.0160424\n",
      "Step 1700 Loss 0.0143165\n",
      "Step 1800 Loss 0.0110259\n",
      "Step 1900 Loss 0.00930816\n",
      "Step 2000 Loss 0.00909101\n",
      "Step 2100 Loss 0.00643646\n",
      "Step 2200 Loss 0.00781057\n",
      "Step 2300 Loss 0.00749652\n",
      "Step 2400 Loss 0.00727419\n",
      "Step 2500 Loss 0.114186\n",
      "Step 2600 Loss 0.190792\n",
      "Step 2700 Loss 0.169354\n",
      "Step 2800 Loss 0.00870512\n",
      "Step 2900 Loss 0.013957\n",
      "Step 3000 Loss 0.169411\n",
      "Step 3100 Loss 0.00685819\n",
      "Step 3200 Loss 0.255883\n",
      "Step 3300 Loss 0.323016\n",
      "Step 3400 Loss 0.0215145\n",
      "Step 3500 Loss 0.327368\n",
      "Step 3600 Loss 0.0861759\n",
      "Step 3700 Loss 0.320561\n",
      "Step 3800 Loss 0.0146625\n",
      "Step 3900 Loss 0.0134034\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 57\n",
      "Step 0 Loss 0.481546\n",
      "Step 100 Loss 0.0120621\n",
      "Step 200 Loss 0.0117119\n",
      "Step 300 Loss 0.00754898\n",
      "Step 400 Loss 0.00925164\n",
      "Step 500 Loss 0.320527\n",
      "Step 600 Loss 0.435209\n",
      "Step 700 Loss 0.103541\n",
      "Step 800 Loss 0.217705\n",
      "Step 900 Loss 0.200814\n",
      "Step 1000 Loss 0.0254082\n",
      "Step 1100 Loss 0.0155694\n",
      "Step 1200 Loss 0.0111683\n",
      "Step 1300 Loss 0.0109899\n",
      "Step 1400 Loss 0.208096\n",
      "Step 1500 Loss 0.026246\n",
      "Step 1600 Loss 0.0160772\n",
      "Step 1700 Loss 0.0143645\n",
      "Step 1800 Loss 0.0110767\n",
      "Step 1900 Loss 0.00935573\n",
      "Step 2000 Loss 0.00913595\n",
      "Step 2100 Loss 0.00646992\n",
      "Step 2200 Loss 0.00785042\n",
      "Step 2300 Loss 0.00753129\n",
      "Step 2400 Loss 0.00730431\n",
      "Step 2500 Loss 0.114123\n",
      "Step 2600 Loss 0.190747\n",
      "Step 2700 Loss 0.169421\n",
      "Step 2800 Loss 0.00872261\n",
      "Step 2900 Loss 0.0139863\n",
      "Step 3000 Loss 0.169413\n",
      "Step 3100 Loss 0.00687668\n",
      "Step 3200 Loss 0.256101\n",
      "Step 3300 Loss 0.323378\n",
      "Step 3400 Loss 0.0214912\n",
      "Step 3500 Loss 0.327761\n",
      "Step 3600 Loss 0.0862132\n",
      "Step 3700 Loss 0.320567\n",
      "Step 3800 Loss 0.0146639\n",
      "Step 3900 Loss 0.0134098\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 58\n",
      "Step 0 Loss 0.480072\n",
      "Step 100 Loss 0.0120739\n",
      "Step 200 Loss 0.0117309\n",
      "Step 300 Loss 0.00756158\n",
      "Step 400 Loss 0.00927401\n",
      "Step 500 Loss 0.320417\n",
      "Step 600 Loss 0.435507\n",
      "Step 700 Loss 0.103549\n",
      "Step 800 Loss 0.217844\n",
      "Step 900 Loss 0.201007\n",
      "Step 1000 Loss 0.0253975\n",
      "Step 1100 Loss 0.0156017\n",
      "Step 1200 Loss 0.0112087\n",
      "Step 1300 Loss 0.0110377\n",
      "Step 1400 Loss 0.208266\n",
      "Step 1500 Loss 0.0262269\n",
      "Step 1600 Loss 0.0161109\n",
      "Step 1700 Loss 0.0144113\n",
      "Step 1800 Loss 0.0111266\n",
      "Step 1900 Loss 0.00940274\n",
      "Step 2000 Loss 0.00918054\n",
      "Step 2100 Loss 0.00650333\n",
      "Step 2200 Loss 0.00789009\n",
      "Step 2300 Loss 0.00756591\n",
      "Step 2400 Loss 0.00733436\n",
      "Step 2500 Loss 0.11406\n",
      "Step 2600 Loss 0.190702\n",
      "Step 2700 Loss 0.169486\n",
      "Step 2800 Loss 0.00874018\n",
      "Step 2900 Loss 0.0140157\n",
      "Step 3000 Loss 0.169416\n",
      "Step 3100 Loss 0.00689511\n",
      "Step 3200 Loss 0.256312\n",
      "Step 3300 Loss 0.323735\n",
      "Step 3400 Loss 0.0214675\n",
      "Step 3500 Loss 0.328144\n",
      "Step 3600 Loss 0.0862495\n",
      "Step 3700 Loss 0.320571\n",
      "Step 3800 Loss 0.014665\n",
      "Step 3900 Loss 0.013416\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 59\n",
      "Step 0 Loss 0.478652\n",
      "Step 100 Loss 0.0120854\n",
      "Step 200 Loss 0.0117494\n",
      "Step 300 Loss 0.00757386\n",
      "Step 400 Loss 0.00929583\n",
      "Step 500 Loss 0.320303\n",
      "Step 600 Loss 0.435797\n",
      "Step 700 Loss 0.103557\n",
      "Step 800 Loss 0.217983\n",
      "Step 900 Loss 0.201199\n",
      "Step 1000 Loss 0.025386\n",
      "Step 1100 Loss 0.015633\n",
      "Step 1200 Loss 0.011248\n",
      "Step 1300 Loss 0.0110846\n",
      "Step 1400 Loss 0.208432\n",
      "Step 1500 Loss 0.0262073\n",
      "Step 1600 Loss 0.0161436\n",
      "Step 1700 Loss 0.0144572\n",
      "Step 1800 Loss 0.0111756\n",
      "Step 1900 Loss 0.0094489\n",
      "Step 2000 Loss 0.00922445\n",
      "Step 2100 Loss 0.00653624\n",
      "Step 2200 Loss 0.00792918\n",
      "Step 2300 Loss 0.00759998\n",
      "Step 2400 Loss 0.0073641\n",
      "Step 2500 Loss 0.113998\n",
      "Step 2600 Loss 0.190657\n",
      "Step 2700 Loss 0.169548\n",
      "Step 2800 Loss 0.00875781\n",
      "Step 2900 Loss 0.0140447\n",
      "Step 3000 Loss 0.169419\n",
      "Step 3100 Loss 0.00691338\n",
      "Step 3200 Loss 0.256517\n",
      "Step 3300 Loss 0.324087\n",
      "Step 3400 Loss 0.0214436\n",
      "Step 3500 Loss 0.328501\n",
      "Step 3600 Loss 0.0862858\n",
      "Step 3700 Loss 0.320576\n",
      "Step 3800 Loss 0.0146669\n",
      "Step 3900 Loss 0.013422\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 60\n",
      "Step 0 Loss 0.477323\n",
      "Step 100 Loss 0.0120974\n",
      "Step 200 Loss 0.0117675\n",
      "Step 300 Loss 0.00758614\n",
      "Step 400 Loss 0.00931732\n",
      "Step 500 Loss 0.320182\n",
      "Step 600 Loss 0.436078\n",
      "Step 700 Loss 0.103565\n",
      "Step 800 Loss 0.218119\n",
      "Step 900 Loss 0.201389\n",
      "Step 1000 Loss 0.0253733\n",
      "Step 1100 Loss 0.0156631\n",
      "Step 1200 Loss 0.0112864\n",
      "Step 1300 Loss 0.0111307\n",
      "Step 1400 Loss 0.208595\n",
      "Step 1500 Loss 0.0261875\n",
      "Step 1600 Loss 0.0161753\n",
      "Step 1700 Loss 0.014502\n",
      "Step 1800 Loss 0.0112238\n",
      "Step 1900 Loss 0.00949428\n",
      "Step 2000 Loss 0.00926764\n",
      "Step 2100 Loss 0.00656868\n",
      "Step 2200 Loss 0.00796775\n",
      "Step 2300 Loss 0.00763367\n",
      "Step 2400 Loss 0.00739351\n",
      "Step 2500 Loss 0.113937\n",
      "Step 2600 Loss 0.190612\n",
      "Step 2700 Loss 0.169608\n",
      "Step 2800 Loss 0.00877533\n",
      "Step 2900 Loss 0.0140734\n",
      "Step 3000 Loss 0.169422\n",
      "Step 3100 Loss 0.00693149\n",
      "Step 3200 Loss 0.256713\n",
      "Step 3300 Loss 0.324432\n",
      "Step 3400 Loss 0.0214208\n",
      "Step 3500 Loss 0.328822\n",
      "Step 3600 Loss 0.0863227\n",
      "Step 3700 Loss 0.320584\n",
      "Step 3800 Loss 0.0146695\n",
      "Step 3900 Loss 0.0134273\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 61\n",
      "Step 0 Loss 0.476174\n",
      "Step 100 Loss 0.0121098\n",
      "Step 200 Loss 0.0117849\n",
      "Step 300 Loss 0.0075982\n",
      "Step 400 Loss 0.00933846\n",
      "Step 500 Loss 0.320056\n",
      "Step 600 Loss 0.43635\n",
      "Step 700 Loss 0.103572\n",
      "Step 800 Loss 0.218254\n",
      "Step 900 Loss 0.201576\n",
      "Step 1000 Loss 0.0253596\n",
      "Step 1100 Loss 0.0156919\n",
      "Step 1200 Loss 0.0113237\n",
      "Step 1300 Loss 0.0111756\n",
      "Step 1400 Loss 0.208755\n",
      "Step 1500 Loss 0.0261669\n",
      "Step 1600 Loss 0.0162057\n",
      "Step 1700 Loss 0.0145455\n",
      "Step 1800 Loss 0.0112709\n",
      "Step 1900 Loss 0.00953876\n",
      "Step 2000 Loss 0.00930974\n",
      "Step 2100 Loss 0.00660044\n",
      "Step 2200 Loss 0.00800538\n",
      "Step 2300 Loss 0.00766662\n",
      "Step 2400 Loss 0.00742216\n",
      "Step 2500 Loss 0.113878\n",
      "Step 2600 Loss 0.190567\n",
      "Step 2700 Loss 0.169665\n",
      "Step 2800 Loss 0.00879253\n",
      "Step 2900 Loss 0.0141011\n",
      "Step 3000 Loss 0.169423\n",
      "Step 3100 Loss 0.00694938\n",
      "Step 3200 Loss 0.256902\n",
      "Step 3300 Loss 0.324773\n",
      "Step 3400 Loss 0.0213993\n",
      "Step 3500 Loss 0.329113\n",
      "Step 3600 Loss 0.0863597\n",
      "Step 3700 Loss 0.320595\n",
      "Step 3800 Loss 0.0146733\n",
      "Step 3900 Loss 0.013432\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 62\n",
      "Step 0 Loss 0.475173\n",
      "Step 100 Loss 0.0121227\n",
      "Step 200 Loss 0.011802\n",
      "Step 300 Loss 0.00761024\n",
      "Step 400 Loss 0.00935923\n",
      "Step 500 Loss 0.319928\n",
      "Step 600 Loss 0.436611\n",
      "Step 700 Loss 0.10358\n",
      "Step 800 Loss 0.218387\n",
      "Step 900 Loss 0.201761\n",
      "Step 1000 Loss 0.025345\n",
      "Step 1100 Loss 0.0157195\n",
      "Step 1200 Loss 0.01136\n",
      "Step 1300 Loss 0.0112195\n",
      "Step 1400 Loss 0.20891\n",
      "Step 1500 Loss 0.0261459\n",
      "Step 1600 Loss 0.0162353\n",
      "Step 1700 Loss 0.0145877\n",
      "Step 1800 Loss 0.0113171\n",
      "Step 1900 Loss 0.00958228\n",
      "Step 2000 Loss 0.0093509\n",
      "Step 2100 Loss 0.00663163\n",
      "Step 2200 Loss 0.00804228\n",
      "Step 2300 Loss 0.00769898\n",
      "Step 2400 Loss 0.0074502\n",
      "Step 2500 Loss 0.113821\n",
      "Step 2600 Loss 0.190521\n",
      "Step 2700 Loss 0.16972\n",
      "Step 2800 Loss 0.00880919\n",
      "Step 2900 Loss 0.0141279\n",
      "Step 3000 Loss 0.169424\n",
      "Step 3100 Loss 0.00696696\n",
      "Step 3200 Loss 0.257086\n",
      "Step 3300 Loss 0.32511\n",
      "Step 3400 Loss 0.0213791\n",
      "Step 3500 Loss 0.329391\n",
      "Step 3600 Loss 0.0863966\n",
      "Step 3700 Loss 0.320605\n",
      "Step 3800 Loss 0.0146768\n",
      "Step 3900 Loss 0.013436\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 63\n",
      "Step 0 Loss 0.474256\n",
      "Step 100 Loss 0.0121355\n",
      "Step 200 Loss 0.0118186\n",
      "Step 300 Loss 0.00762209\n",
      "Step 400 Loss 0.00937962\n",
      "Step 500 Loss 0.319798\n",
      "Step 600 Loss 0.436863\n",
      "Step 700 Loss 0.103588\n",
      "Step 800 Loss 0.218517\n",
      "Step 900 Loss 0.201944\n",
      "Step 1000 Loss 0.0253299\n",
      "Step 1100 Loss 0.0157461\n",
      "Step 1200 Loss 0.0113955\n",
      "Step 1300 Loss 0.0112627\n",
      "Step 1400 Loss 0.209063\n",
      "Step 1500 Loss 0.0261247\n",
      "Step 1600 Loss 0.0162638\n",
      "Step 1700 Loss 0.014629\n",
      "Step 1800 Loss 0.0113625\n",
      "Step 1900 Loss 0.00962517\n",
      "Step 2000 Loss 0.00939143\n",
      "Step 2100 Loss 0.00666232\n",
      "Step 2200 Loss 0.00807863\n",
      "Step 2300 Loss 0.00773087\n",
      "Step 2400 Loss 0.00747798\n",
      "Step 2500 Loss 0.113764\n",
      "Step 2600 Loss 0.190474\n",
      "Step 2700 Loss 0.169772\n",
      "Step 2800 Loss 0.00882587\n",
      "Step 2900 Loss 0.0141542\n",
      "Step 3000 Loss 0.169425\n",
      "Step 3100 Loss 0.00698443\n",
      "Step 3200 Loss 0.257262\n",
      "Step 3300 Loss 0.32544\n",
      "Step 3400 Loss 0.0213601\n",
      "Step 3500 Loss 0.329669\n",
      "Step 3600 Loss 0.0864326\n",
      "Step 3700 Loss 0.320614\n",
      "Step 3800 Loss 0.0146805\n",
      "Step 3900 Loss 0.0134398\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 64\n",
      "Step 0 Loss 0.473361\n",
      "Step 100 Loss 0.0121482\n",
      "Step 200 Loss 0.0118348\n",
      "Step 300 Loss 0.00763379\n",
      "Step 400 Loss 0.0093999\n",
      "Step 500 Loss 0.319669\n",
      "Step 600 Loss 0.437108\n",
      "Step 700 Loss 0.103596\n",
      "Step 800 Loss 0.218648\n",
      "Step 900 Loss 0.202126\n",
      "Step 1000 Loss 0.0253137\n",
      "Step 1100 Loss 0.0157717\n",
      "Step 1200 Loss 0.0114301\n",
      "Step 1300 Loss 0.0113048\n",
      "Step 1400 Loss 0.209213\n",
      "Step 1500 Loss 0.0261031\n",
      "Step 1600 Loss 0.0162913\n",
      "Step 1700 Loss 0.0146693\n",
      "Step 1800 Loss 0.0114071\n",
      "Step 1900 Loss 0.00966738\n",
      "Step 2000 Loss 0.0094315\n",
      "Step 2100 Loss 0.00669265\n",
      "Step 2200 Loss 0.00811452\n",
      "Step 2300 Loss 0.00776238\n",
      "Step 2400 Loss 0.00750525\n",
      "Step 2500 Loss 0.113708\n",
      "Step 2600 Loss 0.190428\n",
      "Step 2700 Loss 0.169824\n",
      "Step 2800 Loss 0.00884246\n",
      "Step 2900 Loss 0.0141804\n",
      "Step 3000 Loss 0.169425\n",
      "Step 3100 Loss 0.00700178\n",
      "Step 3200 Loss 0.257433\n",
      "Step 3300 Loss 0.325765\n",
      "Step 3400 Loss 0.0213413\n",
      "Step 3500 Loss 0.329951\n",
      "Step 3600 Loss 0.0864678\n",
      "Step 3700 Loss 0.320622\n",
      "Step 3800 Loss 0.0146837\n",
      "Step 3900 Loss 0.0134431\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 65\n",
      "Step 0 Loss 0.472454\n",
      "Step 100 Loss 0.0121603\n",
      "Step 200 Loss 0.0118508\n",
      "Step 300 Loss 0.00764532\n",
      "Step 400 Loss 0.00941976\n",
      "Step 500 Loss 0.319542\n",
      "Step 600 Loss 0.437346\n",
      "Step 700 Loss 0.103604\n",
      "Step 800 Loss 0.218778\n",
      "Step 900 Loss 0.202307\n",
      "Step 1000 Loss 0.0252968\n",
      "Step 1100 Loss 0.0157966\n",
      "Step 1200 Loss 0.011464\n",
      "Step 1300 Loss 0.0113463\n",
      "Step 1400 Loss 0.20936\n",
      "Step 1500 Loss 0.0260815\n",
      "Step 1600 Loss 0.0163183\n",
      "Step 1700 Loss 0.0147088\n",
      "Step 1800 Loss 0.011451\n",
      "Step 1900 Loss 0.00970908\n",
      "Step 2000 Loss 0.00947084\n",
      "Step 2100 Loss 0.0067226\n",
      "Step 2200 Loss 0.00815008\n",
      "Step 2300 Loss 0.00779356\n",
      "Step 2400 Loss 0.00753246\n",
      "Step 2500 Loss 0.113652\n",
      "Step 2600 Loss 0.190384\n",
      "Step 2700 Loss 0.169873\n",
      "Step 2800 Loss 0.00885907\n",
      "Step 2900 Loss 0.0142066\n",
      "Step 3000 Loss 0.169427\n",
      "Step 3100 Loss 0.00701906\n",
      "Step 3200 Loss 0.257598\n",
      "Step 3300 Loss 0.326086\n",
      "Step 3400 Loss 0.0213226\n",
      "Step 3500 Loss 0.330236\n",
      "Step 3600 Loss 0.0865021\n",
      "Step 3700 Loss 0.320628\n",
      "Step 3800 Loss 0.0146865\n",
      "Step 3900 Loss 0.0134461\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 66\n",
      "Step 0 Loss 0.471546\n",
      "Step 100 Loss 0.0121722\n",
      "Step 200 Loss 0.0118666\n",
      "Step 300 Loss 0.00765667\n",
      "Step 400 Loss 0.00943924\n",
      "Step 500 Loss 0.319415\n",
      "Step 600 Loss 0.437582\n",
      "Step 700 Loss 0.103613\n",
      "Step 800 Loss 0.218907\n",
      "Step 900 Loss 0.202486\n",
      "Step 1000 Loss 0.025279\n",
      "Step 1100 Loss 0.0158207\n",
      "Step 1200 Loss 0.0114972\n",
      "Step 1300 Loss 0.0113869\n",
      "Step 1400 Loss 0.209504\n",
      "Step 1500 Loss 0.0260596\n",
      "Step 1600 Loss 0.0163443\n",
      "Step 1700 Loss 0.0147476\n",
      "Step 1800 Loss 0.0114941\n",
      "Step 1900 Loss 0.00975016\n",
      "Step 2000 Loss 0.00950999\n",
      "Step 2100 Loss 0.00675236\n",
      "Step 2200 Loss 0.00818515\n",
      "Step 2300 Loss 0.00782453\n",
      "Step 2400 Loss 0.00755934\n",
      "Step 2500 Loss 0.113597\n",
      "Step 2600 Loss 0.190339\n",
      "Step 2700 Loss 0.169919\n",
      "Step 2800 Loss 0.00887583\n",
      "Step 2900 Loss 0.0142328\n",
      "Step 3000 Loss 0.169428\n",
      "Step 3100 Loss 0.00703615\n",
      "Step 3200 Loss 0.257758\n",
      "Step 3300 Loss 0.326401\n",
      "Step 3400 Loss 0.021304\n",
      "Step 3500 Loss 0.330517\n",
      "Step 3600 Loss 0.0865358\n",
      "Step 3700 Loss 0.320635\n",
      "Step 3800 Loss 0.0146893\n",
      "Step 3900 Loss 0.0134489\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 67\n",
      "Step 0 Loss 0.470656\n",
      "Step 100 Loss 0.0121838\n",
      "Step 200 Loss 0.0118822\n",
      "Step 300 Loss 0.00766783\n",
      "Step 400 Loss 0.00945844\n",
      "Step 500 Loss 0.319291\n",
      "Step 600 Loss 0.43781\n",
      "Step 700 Loss 0.103622\n",
      "Step 800 Loss 0.219036\n",
      "Step 900 Loss 0.202665\n",
      "Step 1000 Loss 0.0252605\n",
      "Step 1100 Loss 0.0158439\n",
      "Step 1200 Loss 0.0115296\n",
      "Step 1300 Loss 0.0114269\n",
      "Step 1400 Loss 0.209646\n",
      "Step 1500 Loss 0.0260376\n",
      "Step 1600 Loss 0.0163697\n",
      "Step 1700 Loss 0.0147856\n",
      "Step 1800 Loss 0.0115364\n",
      "Step 1900 Loss 0.00979044\n",
      "Step 2000 Loss 0.00954837\n",
      "Step 2100 Loss 0.00678163\n",
      "Step 2200 Loss 0.00822006\n",
      "Step 2300 Loss 0.00785508\n",
      "Step 2400 Loss 0.00758611\n",
      "Step 2500 Loss 0.113543\n",
      "Step 2600 Loss 0.190294\n",
      "Step 2700 Loss 0.169965\n",
      "Step 2800 Loss 0.00889258\n",
      "Step 2900 Loss 0.0142589\n",
      "Step 3000 Loss 0.16943\n",
      "Step 3100 Loss 0.00705326\n",
      "Step 3200 Loss 0.257913\n",
      "Step 3300 Loss 0.326711\n",
      "Step 3400 Loss 0.0212858\n",
      "Step 3500 Loss 0.330793\n",
      "Step 3600 Loss 0.086569\n",
      "Step 3700 Loss 0.320641\n",
      "Step 3800 Loss 0.0146921\n",
      "Step 3900 Loss 0.0134513\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 68\n",
      "Step 0 Loss 0.469799\n",
      "Step 100 Loss 0.0121952\n",
      "Step 200 Loss 0.0118974\n",
      "Step 300 Loss 0.00767889\n",
      "Step 400 Loss 0.00947738\n",
      "Step 500 Loss 0.319168\n",
      "Step 600 Loss 0.438033\n",
      "Step 700 Loss 0.103631\n",
      "Step 800 Loss 0.219164\n",
      "Step 900 Loss 0.202842\n",
      "Step 1000 Loss 0.0252412\n",
      "Step 1100 Loss 0.0158664\n",
      "Step 1200 Loss 0.0115615\n",
      "Step 1300 Loss 0.011466\n",
      "Step 1400 Loss 0.209784\n",
      "Step 1500 Loss 0.0260157\n",
      "Step 1600 Loss 0.0163943\n",
      "Step 1700 Loss 0.0148228\n",
      "Step 1800 Loss 0.0115779\n",
      "Step 1900 Loss 0.0098302\n",
      "Step 2000 Loss 0.00958643\n",
      "Step 2100 Loss 0.00681063\n",
      "Step 2200 Loss 0.00825436\n",
      "Step 2300 Loss 0.00788532\n",
      "Step 2400 Loss 0.00761254\n",
      "Step 2500 Loss 0.11349\n",
      "Step 2600 Loss 0.19025\n",
      "Step 2700 Loss 0.170008\n",
      "Step 2800 Loss 0.0089092\n",
      "Step 2900 Loss 0.0142848\n",
      "Step 3000 Loss 0.169432\n",
      "Step 3100 Loss 0.0070702\n",
      "Step 3200 Loss 0.258063\n",
      "Step 3300 Loss 0.327017\n",
      "Step 3400 Loss 0.0212678\n",
      "Step 3500 Loss 0.331064\n",
      "Step 3600 Loss 0.0866017\n",
      "Step 3700 Loss 0.320647\n",
      "Step 3800 Loss 0.014695\n",
      "Step 3900 Loss 0.0134536\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 69\n",
      "Step 0 Loss 0.468969\n",
      "Step 100 Loss 0.0122064\n",
      "Step 200 Loss 0.0119123\n",
      "Step 300 Loss 0.00768972\n",
      "Step 400 Loss 0.00949605\n",
      "Step 500 Loss 0.319045\n",
      "Step 600 Loss 0.438253\n",
      "Step 700 Loss 0.10364\n",
      "Step 800 Loss 0.219292\n",
      "Step 900 Loss 0.203019\n",
      "Step 1000 Loss 0.0252211\n",
      "Step 1100 Loss 0.0158879\n",
      "Step 1200 Loss 0.0115924\n",
      "Step 1300 Loss 0.0115044\n",
      "Step 1400 Loss 0.209921\n",
      "Step 1500 Loss 0.0259933\n",
      "Step 1600 Loss 0.016418\n",
      "Step 1700 Loss 0.0148592\n",
      "Step 1800 Loss 0.0116189\n",
      "Step 1900 Loss 0.00986938\n",
      "Step 2000 Loss 0.00962394\n",
      "Step 2100 Loss 0.00683928\n",
      "Step 2200 Loss 0.00828836\n",
      "Step 2300 Loss 0.00791531\n",
      "Step 2400 Loss 0.00763864\n",
      "Step 2500 Loss 0.113437\n",
      "Step 2600 Loss 0.190207\n",
      "Step 2700 Loss 0.17005\n",
      "Step 2800 Loss 0.00892593\n",
      "Step 2900 Loss 0.0143106\n",
      "Step 3000 Loss 0.169434\n",
      "Step 3100 Loss 0.00708695\n",
      "Step 3200 Loss 0.258208\n",
      "Step 3300 Loss 0.327318\n",
      "Step 3400 Loss 0.0212503\n",
      "Step 3500 Loss 0.33133\n",
      "Step 3600 Loss 0.086634\n",
      "Step 3700 Loss 0.320653\n",
      "Step 3800 Loss 0.0146979\n",
      "Step 3900 Loss 0.0134555\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 70\n",
      "Step 0 Loss 0.468169\n",
      "Step 100 Loss 0.0122174\n",
      "Step 200 Loss 0.0119271\n",
      "Step 300 Loss 0.00770044\n",
      "Step 400 Loss 0.0095144\n",
      "Step 500 Loss 0.318925\n",
      "Step 600 Loss 0.438467\n",
      "Step 700 Loss 0.10365\n",
      "Step 800 Loss 0.219419\n",
      "Step 900 Loss 0.203195\n",
      "Step 1000 Loss 0.0252005\n",
      "Step 1100 Loss 0.0159088\n",
      "Step 1200 Loss 0.0116227\n",
      "Step 1300 Loss 0.011542\n",
      "Step 1400 Loss 0.210056\n",
      "Step 1500 Loss 0.025971\n",
      "Step 1600 Loss 0.0164412\n",
      "Step 1700 Loss 0.014895\n",
      "Step 1800 Loss 0.0116591\n",
      "Step 1900 Loss 0.00990794\n",
      "Step 2000 Loss 0.00966089\n",
      "Step 2100 Loss 0.00686757\n",
      "Step 2200 Loss 0.00832195\n",
      "Step 2300 Loss 0.00794486\n",
      "Step 2400 Loss 0.00766446\n",
      "Step 2500 Loss 0.113384\n",
      "Step 2600 Loss 0.190164\n",
      "Step 2700 Loss 0.170091\n",
      "Step 2800 Loss 0.00894259\n",
      "Step 2900 Loss 0.0143362\n",
      "Step 3000 Loss 0.169437\n",
      "Step 3100 Loss 0.00710368\n",
      "Step 3200 Loss 0.258349\n",
      "Step 3300 Loss 0.327615\n",
      "Step 3400 Loss 0.021233\n",
      "Step 3500 Loss 0.331589\n",
      "Step 3600 Loss 0.0866659\n",
      "Step 3700 Loss 0.320659\n",
      "Step 3800 Loss 0.0147008\n",
      "Step 3900 Loss 0.0134571\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 71\n",
      "Step 0 Loss 0.467411\n",
      "Step 100 Loss 0.0122281\n",
      "Step 200 Loss 0.0119413\n",
      "Step 300 Loss 0.00771096\n",
      "Step 400 Loss 0.00953232\n",
      "Step 500 Loss 0.318807\n",
      "Step 600 Loss 0.438677\n",
      "Step 700 Loss 0.103659\n",
      "Step 800 Loss 0.219545\n",
      "Step 900 Loss 0.203369\n",
      "Step 1000 Loss 0.0251791\n",
      "Step 1100 Loss 0.015929\n",
      "Step 1200 Loss 0.0116522\n",
      "Step 1300 Loss 0.011579\n",
      "Step 1400 Loss 0.210188\n",
      "Step 1500 Loss 0.0259486\n",
      "Step 1600 Loss 0.0164636\n",
      "Step 1700 Loss 0.01493\n",
      "Step 1800 Loss 0.0116986\n",
      "Step 1900 Loss 0.00994603\n",
      "Step 2000 Loss 0.00969733\n",
      "Step 2100 Loss 0.00689527\n",
      "Step 2200 Loss 0.00835505\n",
      "Step 2300 Loss 0.00797404\n",
      "Step 2400 Loss 0.00769\n",
      "Step 2500 Loss 0.113333\n",
      "Step 2600 Loss 0.190122\n",
      "Step 2700 Loss 0.17013\n",
      "Step 2800 Loss 0.00895905\n",
      "Step 2900 Loss 0.0143616\n",
      "Step 3000 Loss 0.169439\n",
      "Step 3100 Loss 0.00712026\n",
      "Step 3200 Loss 0.258485\n",
      "Step 3300 Loss 0.327906\n",
      "Step 3400 Loss 0.0212162\n",
      "Step 3500 Loss 0.331835\n",
      "Step 3600 Loss 0.0866979\n",
      "Step 3700 Loss 0.320667\n",
      "Step 3800 Loss 0.014704\n",
      "Step 3900 Loss 0.0134585\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 72\n",
      "Step 0 Loss 0.466723\n",
      "Step 100 Loss 0.012239\n",
      "Step 200 Loss 0.0119555\n",
      "Step 300 Loss 0.00772133\n",
      "Step 400 Loss 0.00955013\n",
      "Step 500 Loss 0.318689\n",
      "Step 600 Loss 0.438883\n",
      "Step 700 Loss 0.103669\n",
      "Step 800 Loss 0.219671\n",
      "Step 900 Loss 0.203542\n",
      "Step 1000 Loss 0.0251573\n",
      "Step 1100 Loss 0.0159482\n",
      "Step 1200 Loss 0.011681\n",
      "Step 1300 Loss 0.0116152\n",
      "Step 1400 Loss 0.210317\n",
      "Step 1500 Loss 0.0259259\n",
      "Step 1600 Loss 0.0164852\n",
      "Step 1700 Loss 0.014964\n",
      "Step 1800 Loss 0.0117375\n",
      "Step 1900 Loss 0.00998344\n",
      "Step 2000 Loss 0.00973335\n",
      "Step 2100 Loss 0.00692283\n",
      "Step 2200 Loss 0.00838782\n",
      "Step 2300 Loss 0.0080029\n",
      "Step 2400 Loss 0.00771524\n",
      "Step 2500 Loss 0.113282\n",
      "Step 2600 Loss 0.190079\n",
      "Step 2700 Loss 0.170168\n",
      "Step 2800 Loss 0.00897534\n",
      "Step 2900 Loss 0.0143864\n",
      "Step 3000 Loss 0.169442\n",
      "Step 3100 Loss 0.00713663\n",
      "Step 3200 Loss 0.258617\n",
      "Step 3300 Loss 0.328194\n",
      "Step 3400 Loss 0.0211998\n",
      "Step 3500 Loss 0.332065\n",
      "Step 3600 Loss 0.0867298\n",
      "Step 3700 Loss 0.320677\n",
      "Step 3800 Loss 0.0147078\n",
      "Step 3900 Loss 0.0134596\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 73\n",
      "Step 0 Loss 0.466142\n",
      "Step 100 Loss 0.01225\n",
      "Step 200 Loss 0.0119692\n",
      "Step 300 Loss 0.00773164\n",
      "Step 400 Loss 0.00956769\n",
      "Step 500 Loss 0.318573\n",
      "Step 600 Loss 0.439084\n",
      "Step 700 Loss 0.103678\n",
      "Step 800 Loss 0.219795\n",
      "Step 900 Loss 0.203714\n",
      "Step 1000 Loss 0.025135\n",
      "Step 1100 Loss 0.0159667\n",
      "Step 1200 Loss 0.0117089\n",
      "Step 1300 Loss 0.0116508\n",
      "Step 1400 Loss 0.210445\n",
      "Step 1500 Loss 0.0259031\n",
      "Step 1600 Loss 0.0165062\n",
      "Step 1700 Loss 0.0149973\n",
      "Step 1800 Loss 0.0117756\n",
      "Step 1900 Loss 0.0100202\n",
      "Step 2000 Loss 0.00976851\n",
      "Step 2100 Loss 0.00694976\n",
      "Step 2200 Loss 0.00841999\n",
      "Step 2300 Loss 0.00803134\n",
      "Step 2400 Loss 0.00774008\n",
      "Step 2500 Loss 0.113231\n",
      "Step 2600 Loss 0.190037\n",
      "Step 2700 Loss 0.170204\n",
      "Step 2800 Loss 0.00899147\n",
      "Step 2900 Loss 0.0144108\n",
      "Step 3000 Loss 0.169444\n",
      "Step 3100 Loss 0.00715286\n",
      "Step 3200 Loss 0.258744\n",
      "Step 3300 Loss 0.328477\n",
      "Step 3400 Loss 0.0211845\n",
      "Step 3500 Loss 0.332276\n",
      "Step 3600 Loss 0.0867622\n",
      "Step 3700 Loss 0.32069\n",
      "Step 3800 Loss 0.0147128\n",
      "Step 3900 Loss 0.0134606\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 74\n",
      "Step 0 Loss 0.465675\n",
      "Step 100 Loss 0.0122616\n",
      "Step 200 Loss 0.011983\n",
      "Step 300 Loss 0.00774196\n",
      "Step 400 Loss 0.00958516\n",
      "Step 500 Loss 0.318456\n",
      "Step 600 Loss 0.439277\n",
      "Step 700 Loss 0.103687\n",
      "Step 800 Loss 0.219918\n",
      "Step 900 Loss 0.203883\n",
      "Step 1000 Loss 0.0251121\n",
      "Step 1100 Loss 0.0159841\n",
      "Step 1200 Loss 0.011736\n",
      "Step 1300 Loss 0.0116855\n",
      "Step 1400 Loss 0.21057\n",
      "Step 1500 Loss 0.0258801\n",
      "Step 1600 Loss 0.0165265\n",
      "Step 1700 Loss 0.0150299\n",
      "Step 1800 Loss 0.0118131\n",
      "Step 1900 Loss 0.0100563\n",
      "Step 2000 Loss 0.0098031\n",
      "Step 2100 Loss 0.00697618\n",
      "Step 2200 Loss 0.00845147\n",
      "Step 2300 Loss 0.00805913\n",
      "Step 2400 Loss 0.00776439\n",
      "Step 2500 Loss 0.113182\n",
      "Step 2600 Loss 0.189993\n",
      "Step 2700 Loss 0.170239\n",
      "Step 2800 Loss 0.00900725\n",
      "Step 2900 Loss 0.0144345\n",
      "Step 3000 Loss 0.169445\n",
      "Step 3100 Loss 0.00716892\n",
      "Step 3200 Loss 0.258867\n",
      "Step 3300 Loss 0.328754\n",
      "Step 3400 Loss 0.0211702\n",
      "Step 3500 Loss 0.332471\n",
      "Step 3600 Loss 0.0867948\n",
      "Step 3700 Loss 0.320705\n",
      "Step 3800 Loss 0.0147186\n",
      "Step 3900 Loss 0.0134613\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 75\n",
      "Step 0 Loss 0.465314\n",
      "Step 100 Loss 0.0122735\n",
      "Step 200 Loss 0.0119963\n",
      "Step 300 Loss 0.00775212\n",
      "Step 400 Loss 0.00960236\n",
      "Step 500 Loss 0.318341\n",
      "Step 600 Loss 0.439466\n",
      "Step 700 Loss 0.103696\n",
      "Step 800 Loss 0.220038\n",
      "Step 900 Loss 0.20405\n",
      "Step 1000 Loss 0.0250893\n",
      "Step 1100 Loss 0.016001\n",
      "Step 1200 Loss 0.0117623\n",
      "Step 1300 Loss 0.0117197\n",
      "Step 1400 Loss 0.210693\n",
      "Step 1500 Loss 0.0258566\n",
      "Step 1600 Loss 0.0165459\n",
      "Step 1700 Loss 0.0150613\n",
      "Step 1800 Loss 0.01185\n",
      "Step 1900 Loss 0.0100919\n",
      "Step 2000 Loss 0.00983698\n",
      "Step 2100 Loss 0.00700223\n",
      "Step 2200 Loss 0.00848256\n",
      "Step 2300 Loss 0.00808675\n",
      "Step 2400 Loss 0.0077884\n",
      "Step 2500 Loss 0.113134\n",
      "Step 2600 Loss 0.18995\n",
      "Step 2700 Loss 0.170272\n",
      "Step 2800 Loss 0.00902282\n",
      "Step 2900 Loss 0.0144576\n",
      "Step 3000 Loss 0.169446\n",
      "Step 3100 Loss 0.00718485\n",
      "Step 3200 Loss 0.258986\n",
      "Step 3300 Loss 0.329027\n",
      "Step 3400 Loss 0.0211568\n",
      "Step 3500 Loss 0.332654\n",
      "Step 3600 Loss 0.0868275\n",
      "Step 3700 Loss 0.320721\n",
      "Step 3800 Loss 0.0147248\n",
      "Step 3900 Loss 0.0134618\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 76\n",
      "Step 0 Loss 0.465035\n",
      "Step 100 Loss 0.0122855\n",
      "Step 200 Loss 0.0120095\n",
      "Step 300 Loss 0.00776232\n",
      "Step 400 Loss 0.00961949\n",
      "Step 500 Loss 0.318226\n",
      "Step 600 Loss 0.439649\n",
      "Step 700 Loss 0.103705\n",
      "Step 800 Loss 0.220158\n",
      "Step 900 Loss 0.204215\n",
      "Step 1000 Loss 0.0250658\n",
      "Step 1100 Loss 0.0160168\n",
      "Step 1200 Loss 0.0117878\n",
      "Step 1300 Loss 0.0117532\n",
      "Step 1400 Loss 0.210814\n",
      "Step 1500 Loss 0.0258334\n",
      "Step 1600 Loss 0.0165648\n",
      "Step 1700 Loss 0.0150922\n",
      "Step 1800 Loss 0.0118863\n",
      "Step 1900 Loss 0.0101271\n",
      "Step 2000 Loss 0.00987037\n",
      "Step 2100 Loss 0.00702775\n",
      "Step 2200 Loss 0.0085129\n",
      "Step 2300 Loss 0.00811372\n",
      "Step 2400 Loss 0.00781183\n",
      "Step 2500 Loss 0.113087\n",
      "Step 2600 Loss 0.189906\n",
      "Step 2700 Loss 0.170305\n",
      "Step 2800 Loss 0.00903806\n",
      "Step 2900 Loss 0.0144801\n",
      "Step 3000 Loss 0.169446\n",
      "Step 3100 Loss 0.00720047\n",
      "Step 3200 Loss 0.259101\n",
      "Step 3300 Loss 0.329296\n",
      "Step 3400 Loss 0.0211441\n",
      "Step 3500 Loss 0.332827\n",
      "Step 3600 Loss 0.0868605\n",
      "Step 3700 Loss 0.320739\n",
      "Step 3800 Loss 0.0147317\n",
      "Step 3900 Loss 0.0134621\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 77\n",
      "Step 0 Loss 0.464801\n",
      "Step 100 Loss 0.0122975\n",
      "Step 200 Loss 0.0120224\n",
      "Step 300 Loss 0.00777234\n",
      "Step 400 Loss 0.00963636\n",
      "Step 500 Loss 0.318113\n",
      "Step 600 Loss 0.439827\n",
      "Step 700 Loss 0.103714\n",
      "Step 800 Loss 0.220276\n",
      "Step 900 Loss 0.204379\n",
      "Step 1000 Loss 0.025042\n",
      "Step 1100 Loss 0.0160322\n",
      "Step 1200 Loss 0.0118127\n",
      "Step 1300 Loss 0.011786\n",
      "Step 1400 Loss 0.210932\n",
      "Step 1500 Loss 0.0258101\n",
      "Step 1600 Loss 0.0165832\n",
      "Step 1700 Loss 0.0151224\n",
      "Step 1800 Loss 0.0119221\n",
      "Step 1900 Loss 0.0101617\n",
      "Step 2000 Loss 0.00990332\n",
      "Step 2100 Loss 0.00705295\n",
      "Step 2200 Loss 0.00854302\n",
      "Step 2300 Loss 0.00814047\n",
      "Step 2400 Loss 0.00783524\n",
      "Step 2500 Loss 0.113041\n",
      "Step 2600 Loss 0.189863\n",
      "Step 2700 Loss 0.170335\n",
      "Step 2800 Loss 0.0090533\n",
      "Step 2900 Loss 0.0145025\n",
      "Step 3000 Loss 0.169445\n",
      "Step 3100 Loss 0.00721609\n",
      "Step 3200 Loss 0.25921\n",
      "Step 3300 Loss 0.329559\n",
      "Step 3400 Loss 0.0211323\n",
      "Step 3500 Loss 0.332995\n",
      "Step 3600 Loss 0.0868931\n",
      "Step 3700 Loss 0.320755\n",
      "Step 3800 Loss 0.0147389\n",
      "Step 3900 Loss 0.0134627\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 78\n",
      "Step 0 Loss 0.464584\n",
      "Step 100 Loss 0.0123096\n",
      "Step 200 Loss 0.0120352\n",
      "Step 300 Loss 0.00778245\n",
      "Step 400 Loss 0.00965316\n",
      "Step 500 Loss 0.318001\n",
      "Step 600 Loss 0.440002\n",
      "Step 700 Loss 0.103722\n",
      "Step 800 Loss 0.220393\n",
      "Step 900 Loss 0.204541\n",
      "Step 1000 Loss 0.0250181\n",
      "Step 1100 Loss 0.0160469\n",
      "Step 1200 Loss 0.0118369\n",
      "Step 1300 Loss 0.0118184\n",
      "Step 1400 Loss 0.211049\n",
      "Step 1500 Loss 0.0257863\n",
      "Step 1600 Loss 0.0166007\n",
      "Step 1700 Loss 0.0151519\n",
      "Step 1800 Loss 0.0119571\n",
      "Step 1900 Loss 0.0101957\n",
      "Step 2000 Loss 0.00993565\n",
      "Step 2100 Loss 0.00707772\n",
      "Step 2200 Loss 0.00857255\n",
      "Step 2300 Loss 0.00816674\n",
      "Step 2400 Loss 0.00785805\n",
      "Step 2500 Loss 0.112995\n",
      "Step 2600 Loss 0.18982\n",
      "Step 2700 Loss 0.170366\n",
      "Step 2800 Loss 0.00906814\n",
      "Step 2900 Loss 0.0145242\n",
      "Step 3000 Loss 0.169445\n",
      "Step 3100 Loss 0.00723137\n",
      "Step 3200 Loss 0.259319\n",
      "Step 3300 Loss 0.32982\n",
      "Step 3400 Loss 0.0211205\n",
      "Step 3500 Loss 0.333161\n",
      "Step 3600 Loss 0.0869256\n",
      "Step 3700 Loss 0.320771\n",
      "Step 3800 Loss 0.014746\n",
      "Step 3900 Loss 0.0134631\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 79\n",
      "Step 0 Loss 0.46438\n",
      "Step 100 Loss 0.0123215\n",
      "Step 200 Loss 0.012048\n",
      "Step 300 Loss 0.00779236\n",
      "Step 400 Loss 0.00966974\n",
      "Step 500 Loss 0.317891\n",
      "Step 600 Loss 0.440172\n",
      "Step 700 Loss 0.103731\n",
      "Step 800 Loss 0.22051\n",
      "Step 900 Loss 0.204703\n",
      "Step 1000 Loss 0.0249936\n",
      "Step 1100 Loss 0.016061\n",
      "Step 1200 Loss 0.0118607\n",
      "Step 1300 Loss 0.0118502\n",
      "Step 1400 Loss 0.211165\n",
      "Step 1500 Loss 0.0257628\n",
      "Step 1600 Loss 0.0166179\n",
      "Step 1700 Loss 0.015181\n",
      "Step 1800 Loss 0.0119916\n",
      "Step 1900 Loss 0.0102293\n",
      "Step 2000 Loss 0.00996769\n",
      "Step 2100 Loss 0.00710227\n",
      "Step 2200 Loss 0.00860195\n",
      "Step 2300 Loss 0.00819292\n",
      "Step 2400 Loss 0.0078809\n",
      "Step 2500 Loss 0.11295\n",
      "Step 2600 Loss 0.189777\n",
      "Step 2700 Loss 0.170395\n",
      "Step 2800 Loss 0.00908322\n",
      "Step 2900 Loss 0.0145462\n",
      "Step 3000 Loss 0.169445\n",
      "Step 3100 Loss 0.00724685\n",
      "Step 3200 Loss 0.259422\n",
      "Step 3300 Loss 0.330074\n",
      "Step 3400 Loss 0.0211091\n",
      "Step 3500 Loss 0.333322\n",
      "Step 3600 Loss 0.0869576\n",
      "Step 3700 Loss 0.320786\n",
      "Step 3800 Loss 0.0147535\n",
      "Step 3900 Loss 0.0134637\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 80\n",
      "Step 0 Loss 0.464168\n",
      "Step 100 Loss 0.0123333\n",
      "Step 200 Loss 0.0120607\n",
      "Step 300 Loss 0.00780218\n",
      "Step 400 Loss 0.00968623\n",
      "Step 500 Loss 0.317782\n",
      "Step 600 Loss 0.440339\n",
      "Step 700 Loss 0.10374\n",
      "Step 800 Loss 0.220626\n",
      "Step 900 Loss 0.204863\n",
      "Step 1000 Loss 0.0249687\n",
      "Step 1100 Loss 0.0160745\n",
      "Step 1200 Loss 0.0118839\n",
      "Step 1300 Loss 0.0118815\n",
      "Step 1400 Loss 0.211278\n",
      "Step 1500 Loss 0.0257393\n",
      "Step 1600 Loss 0.0166346\n",
      "Step 1700 Loss 0.0152093\n",
      "Step 1800 Loss 0.0120257\n",
      "Step 1900 Loss 0.0102625\n",
      "Step 2000 Loss 0.0099993\n",
      "Step 2100 Loss 0.00712651\n",
      "Step 2200 Loss 0.00863096\n",
      "Step 2300 Loss 0.00821867\n",
      "Step 2400 Loss 0.00790339\n",
      "Step 2500 Loss 0.112905\n",
      "Step 2600 Loss 0.189736\n",
      "Step 2700 Loss 0.170424\n",
      "Step 2800 Loss 0.00909808\n",
      "Step 2900 Loss 0.0145678\n",
      "Step 3000 Loss 0.169445\n",
      "Step 3100 Loss 0.007262\n",
      "Step 3200 Loss 0.259523\n",
      "Step 3300 Loss 0.330326\n",
      "Step 3400 Loss 0.0210976\n",
      "Step 3500 Loss 0.333486\n",
      "Step 3600 Loss 0.0869893\n",
      "Step 3700 Loss 0.320799\n",
      "Step 3800 Loss 0.014761\n",
      "Step 3900 Loss 0.0134643\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 81\n",
      "Step 0 Loss 0.463946\n",
      "Step 100 Loss 0.0123449\n",
      "Step 200 Loss 0.0120733\n",
      "Step 300 Loss 0.00781193\n",
      "Step 400 Loss 0.00970241\n",
      "Step 500 Loss 0.317675\n",
      "Step 600 Loss 0.440504\n",
      "Step 700 Loss 0.10375\n",
      "Step 800 Loss 0.220742\n",
      "Step 900 Loss 0.205023\n",
      "Step 1000 Loss 0.0249437\n",
      "Step 1100 Loss 0.0160877\n",
      "Step 1200 Loss 0.0119064\n",
      "Step 1300 Loss 0.0119121\n",
      "Step 1400 Loss 0.21139\n",
      "Step 1500 Loss 0.025716\n",
      "Step 1600 Loss 0.0166509\n",
      "Step 1700 Loss 0.0152373\n",
      "Step 1800 Loss 0.0120592\n",
      "Step 1900 Loss 0.0102952\n",
      "Step 2000 Loss 0.0100306\n",
      "Step 2100 Loss 0.00715056\n",
      "Step 2200 Loss 0.00865971\n",
      "Step 2300 Loss 0.00824428\n",
      "Step 2400 Loss 0.00792584\n",
      "Step 2500 Loss 0.11286\n",
      "Step 2600 Loss 0.189694\n",
      "Step 2700 Loss 0.170451\n",
      "Step 2800 Loss 0.00911303\n",
      "Step 2900 Loss 0.0145894\n",
      "Step 3000 Loss 0.169445\n",
      "Step 3100 Loss 0.00727717\n",
      "Step 3200 Loss 0.259621\n",
      "Step 3300 Loss 0.330573\n",
      "Step 3400 Loss 0.0210862\n",
      "Step 3500 Loss 0.33365\n",
      "Step 3600 Loss 0.0870204\n",
      "Step 3700 Loss 0.320812\n",
      "Step 3800 Loss 0.0147683\n",
      "Step 3900 Loss 0.0134647\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 82\n",
      "Step 0 Loss 0.463705\n",
      "Step 100 Loss 0.0123562\n",
      "Step 200 Loss 0.0120858\n",
      "Step 300 Loss 0.00782157\n",
      "Step 400 Loss 0.00971845\n",
      "Step 500 Loss 0.31757\n",
      "Step 600 Loss 0.440668\n",
      "Step 700 Loss 0.103759\n",
      "Step 800 Loss 0.220858\n",
      "Step 900 Loss 0.205183\n",
      "Step 1000 Loss 0.0249179\n",
      "Step 1100 Loss 0.0161001\n",
      "Step 1200 Loss 0.0119286\n",
      "Step 1300 Loss 0.0119423\n",
      "Step 1400 Loss 0.211501\n",
      "Step 1500 Loss 0.0256925\n",
      "Step 1600 Loss 0.0166668\n",
      "Step 1700 Loss 0.0152649\n",
      "Step 1800 Loss 0.0120924\n",
      "Step 1900 Loss 0.0103276\n",
      "Step 2000 Loss 0.0100617\n",
      "Step 2100 Loss 0.00717441\n",
      "Step 2200 Loss 0.00868834\n",
      "Step 2300 Loss 0.00826985\n",
      "Step 2400 Loss 0.00794825\n",
      "Step 2500 Loss 0.112815\n",
      "Step 2600 Loss 0.189654\n",
      "Step 2700 Loss 0.170477\n",
      "Step 2800 Loss 0.009128\n",
      "Step 2900 Loss 0.014611\n",
      "Step 3000 Loss 0.169446\n",
      "Step 3100 Loss 0.00729217\n",
      "Step 3200 Loss 0.259716\n",
      "Step 3300 Loss 0.330816\n",
      "Step 3400 Loss 0.0210748\n",
      "Step 3500 Loss 0.333816\n",
      "Step 3600 Loss 0.0870512\n",
      "Step 3700 Loss 0.320824\n",
      "Step 3800 Loss 0.0147756\n",
      "Step 3900 Loss 0.0134654\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 83\n",
      "Step 0 Loss 0.463434\n",
      "Step 100 Loss 0.0123673\n",
      "Step 200 Loss 0.0120983\n",
      "Step 300 Loss 0.00783115\n",
      "Step 400 Loss 0.00973439\n",
      "Step 500 Loss 0.317467\n",
      "Step 600 Loss 0.440827\n",
      "Step 700 Loss 0.103768\n",
      "Step 800 Loss 0.220975\n",
      "Step 900 Loss 0.205342\n",
      "Step 1000 Loss 0.024892\n",
      "Step 1100 Loss 0.0161122\n",
      "Step 1200 Loss 0.0119502\n",
      "Step 1300 Loss 0.0119719\n",
      "Step 1400 Loss 0.21161\n",
      "Step 1500 Loss 0.0256694\n",
      "Step 1600 Loss 0.016682\n",
      "Step 1700 Loss 0.0152919\n",
      "Step 1800 Loss 0.0121248\n",
      "Step 1900 Loss 0.0103595\n",
      "Step 2000 Loss 0.0100925\n",
      "Step 2100 Loss 0.00719797\n",
      "Step 2200 Loss 0.00871671\n",
      "Step 2300 Loss 0.00829505\n",
      "Step 2400 Loss 0.00797041\n",
      "Step 2500 Loss 0.112771\n",
      "Step 2600 Loss 0.189614\n",
      "Step 2700 Loss 0.170503\n",
      "Step 2800 Loss 0.00914296\n",
      "Step 2900 Loss 0.0146328\n",
      "Step 3000 Loss 0.169447\n",
      "Step 3100 Loss 0.00730717\n",
      "Step 3200 Loss 0.259808\n",
      "Step 3300 Loss 0.331055\n",
      "Step 3400 Loss 0.0210633\n",
      "Step 3500 Loss 0.333986\n",
      "Step 3600 Loss 0.0870814\n",
      "Step 3700 Loss 0.320834\n",
      "Step 3800 Loss 0.0147826\n",
      "Step 3900 Loss 0.0134659\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 84\n",
      "Step 0 Loss 0.463137\n",
      "Step 100 Loss 0.0123778\n",
      "Step 200 Loss 0.0121106\n",
      "Step 300 Loss 0.00784047\n",
      "Step 400 Loss 0.00975013\n",
      "Step 500 Loss 0.317366\n",
      "Step 600 Loss 0.440985\n",
      "Step 700 Loss 0.103778\n",
      "Step 800 Loss 0.221091\n",
      "Step 900 Loss 0.205501\n",
      "Step 1000 Loss 0.0248654\n",
      "Step 1100 Loss 0.0161236\n",
      "Step 1200 Loss 0.0119713\n",
      "Step 1300 Loss 0.012001\n",
      "Step 1400 Loss 0.211718\n",
      "Step 1500 Loss 0.025646\n",
      "Step 1600 Loss 0.0166969\n",
      "Step 1700 Loss 0.0153184\n",
      "Step 1800 Loss 0.0121568\n",
      "Step 1900 Loss 0.010391\n",
      "Step 2000 Loss 0.0101229\n",
      "Step 2100 Loss 0.00722133\n",
      "Step 2200 Loss 0.00874488\n",
      "Step 2300 Loss 0.00832015\n",
      "Step 2400 Loss 0.00799254\n",
      "Step 2500 Loss 0.112727\n",
      "Step 2600 Loss 0.189575\n",
      "Step 2700 Loss 0.170528\n",
      "Step 2800 Loss 0.00915803\n",
      "Step 2900 Loss 0.0146544\n",
      "Step 3000 Loss 0.169448\n",
      "Step 3100 Loss 0.00732208\n",
      "Step 3200 Loss 0.259899\n",
      "Step 3300 Loss 0.331292\n",
      "Step 3400 Loss 0.0210517\n",
      "Step 3500 Loss 0.33416\n",
      "Step 3600 Loss 0.0871107\n",
      "Step 3700 Loss 0.320843\n",
      "Step 3800 Loss 0.0147894\n",
      "Step 3900 Loss 0.0134665\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 85\n",
      "Step 0 Loss 0.4628\n",
      "Step 100 Loss 0.0123879\n",
      "Step 200 Loss 0.0121228\n",
      "Step 300 Loss 0.00784968\n",
      "Step 400 Loss 0.00976551\n",
      "Step 500 Loss 0.317268\n",
      "Step 600 Loss 0.44114\n",
      "Step 700 Loss 0.103788\n",
      "Step 800 Loss 0.221209\n",
      "Step 900 Loss 0.205661\n",
      "Step 1000 Loss 0.0248381\n",
      "Step 1100 Loss 0.0161346\n",
      "Step 1200 Loss 0.0119919\n",
      "Step 1300 Loss 0.0120295\n",
      "Step 1400 Loss 0.211824\n",
      "Step 1500 Loss 0.025623\n",
      "Step 1600 Loss 0.0167115\n",
      "Step 1700 Loss 0.0153447\n",
      "Step 1800 Loss 0.0121885\n",
      "Step 1900 Loss 0.0104223\n",
      "Step 2000 Loss 0.0101532\n",
      "Step 2100 Loss 0.00724461\n",
      "Step 2200 Loss 0.00877294\n",
      "Step 2300 Loss 0.0083451\n",
      "Step 2400 Loss 0.00801459\n",
      "Step 2500 Loss 0.112683\n",
      "Step 2600 Loss 0.189537\n",
      "Step 2700 Loss 0.170553\n",
      "Step 2800 Loss 0.00917314\n",
      "Step 2900 Loss 0.0146764\n",
      "Step 3000 Loss 0.169451\n",
      "Step 3100 Loss 0.00733697\n",
      "Step 3200 Loss 0.259986\n",
      "Step 3300 Loss 0.331524\n",
      "Step 3400 Loss 0.0210397\n",
      "Step 3500 Loss 0.334338\n",
      "Step 3600 Loss 0.0871398\n",
      "Step 3700 Loss 0.320851\n",
      "Step 3800 Loss 0.0147959\n",
      "Step 3900 Loss 0.0134671\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 86\n",
      "Step 0 Loss 0.462426\n",
      "Step 100 Loss 0.0123978\n",
      "Step 200 Loss 0.0121349\n",
      "Step 300 Loss 0.00785875\n",
      "Step 400 Loss 0.00978081\n",
      "Step 500 Loss 0.317172\n",
      "Step 600 Loss 0.441295\n",
      "Step 700 Loss 0.103797\n",
      "Step 800 Loss 0.221327\n",
      "Step 900 Loss 0.20582\n",
      "Step 1000 Loss 0.0248106\n",
      "Step 1100 Loss 0.0161452\n",
      "Step 1200 Loss 0.0120122\n",
      "Step 1300 Loss 0.0120576\n",
      "Step 1400 Loss 0.21193\n",
      "Step 1500 Loss 0.0256\n",
      "Step 1600 Loss 0.0167257\n",
      "Step 1700 Loss 0.0153707\n",
      "Step 1800 Loss 0.0122196\n",
      "Step 1900 Loss 0.010453\n",
      "Step 2000 Loss 0.0101833\n",
      "Step 2100 Loss 0.00726777\n",
      "Step 2200 Loss 0.00880087\n",
      "Step 2300 Loss 0.00836993\n",
      "Step 2400 Loss 0.0080366\n",
      "Step 2500 Loss 0.112639\n",
      "Step 2600 Loss 0.189499\n",
      "Step 2700 Loss 0.170576\n",
      "Step 2800 Loss 0.00918851\n",
      "Step 2900 Loss 0.0146985\n",
      "Step 3000 Loss 0.169453\n",
      "Step 3100 Loss 0.0073519\n",
      "Step 3200 Loss 0.260071\n",
      "Step 3300 Loss 0.331752\n",
      "Step 3400 Loss 0.0210277\n",
      "Step 3500 Loss 0.33452\n",
      "Step 3600 Loss 0.087168\n",
      "Step 3700 Loss 0.320857\n",
      "Step 3800 Loss 0.0148022\n",
      "Step 3900 Loss 0.0134676\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 87\n",
      "Step 0 Loss 0.462009\n",
      "Step 100 Loss 0.0124073\n",
      "Step 200 Loss 0.0121469\n",
      "Step 300 Loss 0.00786761\n",
      "Step 400 Loss 0.00979588\n",
      "Step 500 Loss 0.317078\n",
      "Step 600 Loss 0.441448\n",
      "Step 700 Loss 0.103808\n",
      "Step 800 Loss 0.221447\n",
      "Step 900 Loss 0.205981\n",
      "Step 1000 Loss 0.0247821\n",
      "Step 1100 Loss 0.0161551\n",
      "Step 1200 Loss 0.0120319\n",
      "Step 1300 Loss 0.0120852\n",
      "Step 1400 Loss 0.212034\n",
      "Step 1500 Loss 0.0255773\n",
      "Step 1600 Loss 0.0167398\n",
      "Step 1700 Loss 0.0153963\n",
      "Step 1800 Loss 0.0122503\n",
      "Step 1900 Loss 0.0104835\n",
      "Step 2000 Loss 0.0102131\n",
      "Step 2100 Loss 0.00729068\n",
      "Step 2200 Loss 0.0088287\n",
      "Step 2300 Loss 0.0083947\n",
      "Step 2400 Loss 0.00805856\n",
      "Step 2500 Loss 0.112596\n",
      "Step 2600 Loss 0.189463\n",
      "Step 2700 Loss 0.170598\n",
      "Step 2800 Loss 0.00920377\n",
      "Step 2900 Loss 0.0147206\n",
      "Step 3000 Loss 0.169457\n",
      "Step 3100 Loss 0.00736663\n",
      "Step 3200 Loss 0.260155\n",
      "Step 3300 Loss 0.331978\n",
      "Step 3400 Loss 0.0210152\n",
      "Step 3500 Loss 0.334706\n",
      "Step 3600 Loss 0.0871956\n",
      "Step 3700 Loss 0.320863\n",
      "Step 3800 Loss 0.014808\n",
      "Step 3900 Loss 0.013468\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 88\n",
      "Step 0 Loss 0.461556\n",
      "Step 100 Loss 0.0124162\n",
      "Step 200 Loss 0.0121586\n",
      "Step 300 Loss 0.0078763\n",
      "Step 400 Loss 0.00981064\n",
      "Step 500 Loss 0.316987\n",
      "Step 600 Loss 0.441602\n",
      "Step 700 Loss 0.103818\n",
      "Step 800 Loss 0.221567\n",
      "Step 900 Loss 0.206142\n",
      "Step 1000 Loss 0.0247534\n",
      "Step 1100 Loss 0.0161647\n",
      "Step 1200 Loss 0.0120511\n",
      "Step 1300 Loss 0.0121122\n",
      "Step 1400 Loss 0.212138\n",
      "Step 1500 Loss 0.0255544\n",
      "Step 1600 Loss 0.0167534\n",
      "Step 1700 Loss 0.0154215\n",
      "Step 1800 Loss 0.0122806\n",
      "Step 1900 Loss 0.0105136\n",
      "Step 2000 Loss 0.0102429\n",
      "Step 2100 Loss 0.00731348\n",
      "Step 2200 Loss 0.00885639\n",
      "Step 2300 Loss 0.00841933\n",
      "Step 2400 Loss 0.00808045\n",
      "Step 2500 Loss 0.112552\n",
      "Step 2600 Loss 0.189428\n",
      "Step 2700 Loss 0.170621\n",
      "Step 2800 Loss 0.00921927\n",
      "Step 2900 Loss 0.0147431\n",
      "Step 3000 Loss 0.169461\n",
      "Step 3100 Loss 0.00738147\n",
      "Step 3200 Loss 0.260236\n",
      "Step 3300 Loss 0.3322\n",
      "Step 3400 Loss 0.0210027\n",
      "Step 3500 Loss 0.334894\n",
      "Step 3600 Loss 0.0872227\n",
      "Step 3700 Loss 0.320867\n",
      "Step 3800 Loss 0.0148137\n",
      "Step 3900 Loss 0.0134685\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 89\n",
      "Step 0 Loss 0.461067\n",
      "Step 100 Loss 0.012425\n",
      "Step 200 Loss 0.0121705\n",
      "Step 300 Loss 0.00788496\n",
      "Step 400 Loss 0.00982535\n",
      "Step 500 Loss 0.316898\n",
      "Step 600 Loss 0.44175\n",
      "Step 700 Loss 0.103829\n",
      "Step 800 Loss 0.221689\n",
      "Step 900 Loss 0.206304\n",
      "Step 1000 Loss 0.0247239\n",
      "Step 1100 Loss 0.0161737\n",
      "Step 1200 Loss 0.0120698\n",
      "Step 1300 Loss 0.0121388\n",
      "Step 1400 Loss 0.212241\n",
      "Step 1500 Loss 0.0255317\n",
      "Step 1600 Loss 0.0167664\n",
      "Step 1700 Loss 0.0154465\n",
      "Step 1800 Loss 0.0123105\n",
      "Step 1900 Loss 0.0105435\n",
      "Step 2000 Loss 0.0102724\n",
      "Step 2100 Loss 0.00733619\n",
      "Step 2200 Loss 0.00888395\n",
      "Step 2300 Loss 0.00844385\n",
      "Step 2400 Loss 0.0081023\n",
      "Step 2500 Loss 0.112508\n",
      "Step 2600 Loss 0.189393\n",
      "Step 2700 Loss 0.170642\n",
      "Step 2800 Loss 0.00923485\n",
      "Step 2900 Loss 0.0147656\n",
      "Step 3000 Loss 0.169466\n",
      "Step 3100 Loss 0.00739621\n",
      "Step 3200 Loss 0.260316\n",
      "Step 3300 Loss 0.332419\n",
      "Step 3400 Loss 0.0209897\n",
      "Step 3500 Loss 0.335085\n",
      "Step 3600 Loss 0.0872493\n",
      "Step 3700 Loss 0.320871\n",
      "Step 3800 Loss 0.0148191\n",
      "Step 3900 Loss 0.013469\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 90\n",
      "Step 0 Loss 0.460541\n",
      "Step 100 Loss 0.0124332\n",
      "Step 200 Loss 0.0121821\n",
      "Step 300 Loss 0.00789337\n",
      "Step 400 Loss 0.00983977\n",
      "Step 500 Loss 0.316811\n",
      "Step 600 Loss 0.4419\n",
      "Step 700 Loss 0.10384\n",
      "Step 800 Loss 0.221813\n",
      "Step 900 Loss 0.206467\n",
      "Step 1000 Loss 0.0246936\n",
      "Step 1100 Loss 0.0161822\n",
      "Step 1200 Loss 0.0120881\n",
      "Step 1300 Loss 0.0121647\n",
      "Step 1400 Loss 0.212343\n",
      "Step 1500 Loss 0.0255089\n",
      "Step 1600 Loss 0.0167793\n",
      "Step 1700 Loss 0.015471\n",
      "Step 1800 Loss 0.0123397\n",
      "Step 1900 Loss 0.0105728\n",
      "Step 2000 Loss 0.0103016\n",
      "Step 2100 Loss 0.0073587\n",
      "Step 2200 Loss 0.00891141\n",
      "Step 2300 Loss 0.00846817\n",
      "Step 2400 Loss 0.00812409\n",
      "Step 2500 Loss 0.112464\n",
      "Step 2600 Loss 0.189359\n",
      "Step 2700 Loss 0.170663\n",
      "Step 2800 Loss 0.0092506\n",
      "Step 2900 Loss 0.0147883\n",
      "Step 3000 Loss 0.169472\n",
      "Step 3100 Loss 0.00741095\n",
      "Step 3200 Loss 0.260392\n",
      "Step 3300 Loss 0.332634\n",
      "Step 3400 Loss 0.0209766\n",
      "Step 3500 Loss 0.335276\n",
      "Step 3600 Loss 0.0872753\n",
      "Step 3700 Loss 0.320873\n",
      "Step 3800 Loss 0.0148242\n",
      "Step 3900 Loss 0.0134694\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 91\n",
      "Step 0 Loss 0.459986\n",
      "Step 100 Loss 0.0124411\n",
      "Step 200 Loss 0.0121938\n",
      "Step 300 Loss 0.00790167\n",
      "Step 400 Loss 0.00985406\n",
      "Step 500 Loss 0.316726\n",
      "Step 600 Loss 0.442048\n",
      "Step 700 Loss 0.103851\n",
      "Step 800 Loss 0.221938\n",
      "Step 900 Loss 0.206631\n",
      "Step 1000 Loss 0.0246628\n",
      "Step 1100 Loss 0.0161901\n",
      "Step 1200 Loss 0.0121058\n",
      "Step 1300 Loss 0.0121901\n",
      "Step 1400 Loss 0.212444\n",
      "Step 1500 Loss 0.0254864\n",
      "Step 1600 Loss 0.0167919\n",
      "Step 1700 Loss 0.0154954\n",
      "Step 1800 Loss 0.0123688\n",
      "Step 1900 Loss 0.0106019\n",
      "Step 2000 Loss 0.0103307\n",
      "Step 2100 Loss 0.0073811\n",
      "Step 2200 Loss 0.00893876\n",
      "Step 2300 Loss 0.00849252\n",
      "Step 2400 Loss 0.00814576\n",
      "Step 2500 Loss 0.11242\n",
      "Step 2600 Loss 0.189326\n",
      "Step 2700 Loss 0.170683\n",
      "Step 2800 Loss 0.00926628\n",
      "Step 2900 Loss 0.014811\n",
      "Step 3000 Loss 0.169478\n",
      "Step 3100 Loss 0.00742567\n",
      "Step 3200 Loss 0.260468\n",
      "Step 3300 Loss 0.332847\n",
      "Step 3400 Loss 0.0209632\n",
      "Step 3500 Loss 0.335468\n",
      "Step 3600 Loss 0.087301\n",
      "Step 3700 Loss 0.320875\n",
      "Step 3800 Loss 0.0148295\n",
      "Step 3900 Loss 0.01347\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 92\n",
      "Step 0 Loss 0.459405\n",
      "Step 100 Loss 0.0124489\n",
      "Step 200 Loss 0.0122053\n",
      "Step 300 Loss 0.00790986\n",
      "Step 400 Loss 0.00986812\n",
      "Step 500 Loss 0.316643\n",
      "Step 600 Loss 0.442196\n",
      "Step 700 Loss 0.103862\n",
      "Step 800 Loss 0.222065\n",
      "Step 900 Loss 0.206797\n",
      "Step 1000 Loss 0.0246314\n",
      "Step 1100 Loss 0.0161977\n",
      "Step 1200 Loss 0.0121231\n",
      "Step 1300 Loss 0.0122152\n",
      "Step 1400 Loss 0.212544\n",
      "Step 1500 Loss 0.0254639\n",
      "Step 1600 Loss 0.0168042\n",
      "Step 1700 Loss 0.0155194\n",
      "Step 1800 Loss 0.0123974\n",
      "Step 1900 Loss 0.0106307\n",
      "Step 2000 Loss 0.0103597\n",
      "Step 2100 Loss 0.00740342\n",
      "Step 2200 Loss 0.00896611\n",
      "Step 2300 Loss 0.00851657\n",
      "Step 2400 Loss 0.00816747\n",
      "Step 2500 Loss 0.112376\n",
      "Step 2600 Loss 0.189294\n",
      "Step 2700 Loss 0.170702\n",
      "Step 2800 Loss 0.00928218\n",
      "Step 2900 Loss 0.014834\n",
      "Step 3000 Loss 0.169484\n",
      "Step 3100 Loss 0.00744033\n",
      "Step 3200 Loss 0.260542\n",
      "Step 3300 Loss 0.333057\n",
      "Step 3400 Loss 0.0209492\n",
      "Step 3500 Loss 0.33566\n",
      "Step 3600 Loss 0.0873263\n",
      "Step 3700 Loss 0.320877\n",
      "Step 3800 Loss 0.0148344\n",
      "Step 3900 Loss 0.0134704\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 93\n",
      "Step 0 Loss 0.458801\n",
      "Step 100 Loss 0.0124562\n",
      "Step 200 Loss 0.0122167\n",
      "Step 300 Loss 0.00791793\n",
      "Step 400 Loss 0.00988195\n",
      "Step 500 Loss 0.316563\n",
      "Step 600 Loss 0.442343\n",
      "Step 700 Loss 0.103873\n",
      "Step 800 Loss 0.222194\n",
      "Step 900 Loss 0.206964\n",
      "Step 1000 Loss 0.024599\n",
      "Step 1100 Loss 0.0162046\n",
      "Step 1200 Loss 0.0121397\n",
      "Step 1300 Loss 0.0122396\n",
      "Step 1400 Loss 0.212644\n",
      "Step 1500 Loss 0.0254415\n",
      "Step 1600 Loss 0.0168161\n",
      "Step 1700 Loss 0.0155432\n",
      "Step 1800 Loss 0.0124256\n",
      "Step 1900 Loss 0.0106591\n",
      "Step 2000 Loss 0.0103884\n",
      "Step 2100 Loss 0.00742556\n",
      "Step 2200 Loss 0.00899324\n",
      "Step 2300 Loss 0.0085407\n",
      "Step 2400 Loss 0.00818912\n",
      "Step 2500 Loss 0.112332\n",
      "Step 2600 Loss 0.189262\n",
      "Step 2700 Loss 0.170721\n",
      "Step 2800 Loss 0.00929808\n",
      "Step 2900 Loss 0.0148572\n",
      "Step 3000 Loss 0.169491\n",
      "Step 3100 Loss 0.00745514\n",
      "Step 3200 Loss 0.260613\n",
      "Step 3300 Loss 0.333263\n",
      "Step 3400 Loss 0.0209356\n",
      "Step 3500 Loss 0.33585\n",
      "Step 3600 Loss 0.0873513\n",
      "Step 3700 Loss 0.320877\n",
      "Step 3800 Loss 0.0148396\n",
      "Step 3900 Loss 0.0134709\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 94\n",
      "Step 0 Loss 0.458177\n",
      "Step 100 Loss 0.0124635\n",
      "Step 200 Loss 0.0122281\n",
      "Step 300 Loss 0.00792594\n",
      "Step 400 Loss 0.00989573\n",
      "Step 500 Loss 0.316484\n",
      "Step 600 Loss 0.442488\n",
      "Step 700 Loss 0.103884\n",
      "Step 800 Loss 0.222324\n",
      "Step 900 Loss 0.207131\n",
      "Step 1000 Loss 0.0245661\n",
      "Step 1100 Loss 0.016211\n",
      "Step 1200 Loss 0.012156\n",
      "Step 1300 Loss 0.0122636\n",
      "Step 1400 Loss 0.212743\n",
      "Step 1500 Loss 0.0254193\n",
      "Step 1600 Loss 0.0168277\n",
      "Step 1700 Loss 0.0155667\n",
      "Step 1800 Loss 0.0124533\n",
      "Step 1900 Loss 0.0106872\n",
      "Step 2000 Loss 0.010417\n",
      "Step 2100 Loss 0.0074475\n",
      "Step 2200 Loss 0.00902033\n",
      "Step 2300 Loss 0.00856465\n",
      "Step 2400 Loss 0.00821069\n",
      "Step 2500 Loss 0.112288\n",
      "Step 2600 Loss 0.189231\n",
      "Step 2700 Loss 0.17074\n",
      "Step 2800 Loss 0.00931407\n",
      "Step 2900 Loss 0.0148803\n",
      "Step 3000 Loss 0.169499\n",
      "Step 3100 Loss 0.00746977\n",
      "Step 3200 Loss 0.260684\n",
      "Step 3300 Loss 0.333468\n",
      "Step 3400 Loss 0.0209214\n",
      "Step 3500 Loss 0.336039\n",
      "Step 3600 Loss 0.0873762\n",
      "Step 3700 Loss 0.320879\n",
      "Step 3800 Loss 0.014845\n",
      "Step 3900 Loss 0.0134713\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 95\n",
      "Step 0 Loss 0.457552\n",
      "Step 100 Loss 0.0124707\n",
      "Step 200 Loss 0.0122394\n",
      "Step 300 Loss 0.00793383\n",
      "Step 400 Loss 0.0099094\n",
      "Step 500 Loss 0.316405\n",
      "Step 600 Loss 0.442633\n",
      "Step 700 Loss 0.103895\n",
      "Step 800 Loss 0.222457\n",
      "Step 900 Loss 0.2073\n",
      "Step 1000 Loss 0.0245323\n",
      "Step 1100 Loss 0.0162167\n",
      "Step 1200 Loss 0.0121715\n",
      "Step 1300 Loss 0.0122868\n",
      "Step 1400 Loss 0.212842\n",
      "Step 1500 Loss 0.0253966\n",
      "Step 1600 Loss 0.0168388\n",
      "Step 1700 Loss 0.0155896\n",
      "Step 1800 Loss 0.0124805\n",
      "Step 1900 Loss 0.0107148\n",
      "Step 2000 Loss 0.0104451\n",
      "Step 2100 Loss 0.00746922\n",
      "Step 2200 Loss 0.00904701\n",
      "Step 2300 Loss 0.00858835\n",
      "Step 2400 Loss 0.00823201\n",
      "Step 2500 Loss 0.112245\n",
      "Step 2600 Loss 0.189201\n",
      "Step 2700 Loss 0.170758\n",
      "Step 2800 Loss 0.00933011\n",
      "Step 2900 Loss 0.0149033\n",
      "Step 3000 Loss 0.169508\n",
      "Step 3100 Loss 0.00748445\n",
      "Step 3200 Loss 0.260752\n",
      "Step 3300 Loss 0.333669\n",
      "Step 3400 Loss 0.0209072\n",
      "Step 3500 Loss 0.336225\n",
      "Step 3600 Loss 0.0874011\n",
      "Step 3700 Loss 0.320881\n",
      "Step 3800 Loss 0.0148507\n",
      "Step 3900 Loss 0.0134718\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 96\n",
      "Step 0 Loss 0.45693\n",
      "Step 100 Loss 0.012478\n",
      "Step 200 Loss 0.0122507\n",
      "Step 300 Loss 0.00794165\n",
      "Step 400 Loss 0.00992289\n",
      "Step 500 Loss 0.316328\n",
      "Step 600 Loss 0.442778\n",
      "Step 700 Loss 0.103906\n",
      "Step 800 Loss 0.22259\n",
      "Step 900 Loss 0.207471\n",
      "Step 1000 Loss 0.0244981\n",
      "Step 1100 Loss 0.0162219\n",
      "Step 1200 Loss 0.0121867\n",
      "Step 1300 Loss 0.0123097\n",
      "Step 1400 Loss 0.212941\n",
      "Step 1500 Loss 0.0253743\n",
      "Step 1600 Loss 0.0168498\n",
      "Step 1700 Loss 0.0156124\n",
      "Step 1800 Loss 0.0125076\n",
      "Step 1900 Loss 0.0107422\n",
      "Step 2000 Loss 0.0104732\n",
      "Step 2100 Loss 0.00749093\n",
      "Step 2200 Loss 0.00907392\n",
      "Step 2300 Loss 0.00861217\n",
      "Step 2400 Loss 0.00825346\n",
      "Step 2500 Loss 0.112201\n",
      "Step 2600 Loss 0.189171\n",
      "Step 2700 Loss 0.170775\n",
      "Step 2800 Loss 0.00934622\n",
      "Step 2900 Loss 0.0149266\n",
      "Step 3000 Loss 0.169517\n",
      "Step 3100 Loss 0.00749913\n",
      "Step 3200 Loss 0.26082\n",
      "Step 3300 Loss 0.333867\n",
      "Step 3400 Loss 0.0208931\n",
      "Step 3500 Loss 0.336407\n",
      "Step 3600 Loss 0.0874258\n",
      "Step 3700 Loss 0.320884\n",
      "Step 3800 Loss 0.0148574\n",
      "Step 3900 Loss 0.0134725\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 97\n",
      "Step 0 Loss 0.456321\n",
      "Step 100 Loss 0.0124852\n",
      "Step 200 Loss 0.012262\n",
      "Step 300 Loss 0.00794956\n",
      "Step 400 Loss 0.00993647\n",
      "Step 500 Loss 0.316251\n",
      "Step 600 Loss 0.44292\n",
      "Step 700 Loss 0.103917\n",
      "Step 800 Loss 0.222724\n",
      "Step 900 Loss 0.207642\n",
      "Step 1000 Loss 0.024463\n",
      "Step 1100 Loss 0.0162265\n",
      "Step 1200 Loss 0.0122011\n",
      "Step 1300 Loss 0.012332\n",
      "Step 1400 Loss 0.213038\n",
      "Step 1500 Loss 0.0253518\n",
      "Step 1600 Loss 0.0168603\n",
      "Step 1700 Loss 0.0156347\n",
      "Step 1800 Loss 0.0125341\n",
      "Step 1900 Loss 0.0107693\n",
      "Step 2000 Loss 0.010501\n",
      "Step 2100 Loss 0.00751235\n",
      "Step 2200 Loss 0.00910039\n",
      "Step 2300 Loss 0.00863557\n",
      "Step 2400 Loss 0.00827468\n",
      "Step 2500 Loss 0.112158\n",
      "Step 2600 Loss 0.189142\n",
      "Step 2700 Loss 0.170792\n",
      "Step 2800 Loss 0.00936216\n",
      "Step 2900 Loss 0.0149497\n",
      "Step 3000 Loss 0.169526\n",
      "Step 3100 Loss 0.00751379\n",
      "Step 3200 Loss 0.260885\n",
      "Step 3300 Loss 0.334064\n",
      "Step 3400 Loss 0.0208788\n",
      "Step 3500 Loss 0.336586\n",
      "Step 3600 Loss 0.087451\n",
      "Step 3700 Loss 0.320889\n",
      "Step 3800 Loss 0.0148646\n",
      "Step 3900 Loss 0.0134729\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 98\n",
      "Step 0 Loss 0.455742\n",
      "Step 100 Loss 0.0124927\n",
      "Step 200 Loss 0.0122734\n",
      "Step 300 Loss 0.00795747\n",
      "Step 400 Loss 0.00994989\n",
      "Step 500 Loss 0.316174\n",
      "Step 600 Loss 0.443061\n",
      "Step 700 Loss 0.103928\n",
      "Step 800 Loss 0.222859\n",
      "Step 900 Loss 0.207813\n",
      "Step 1000 Loss 0.0244279\n",
      "Step 1100 Loss 0.0162307\n",
      "Step 1200 Loss 0.0122153\n",
      "Step 1300 Loss 0.0123538\n",
      "Step 1400 Loss 0.213135\n",
      "Step 1500 Loss 0.0253295\n",
      "Step 1600 Loss 0.0168707\n",
      "Step 1700 Loss 0.0156567\n",
      "Step 1800 Loss 0.0125603\n",
      "Step 1900 Loss 0.0107959\n",
      "Step 2000 Loss 0.0105285\n",
      "Step 2100 Loss 0.0075336\n",
      "Step 2200 Loss 0.0091267\n",
      "Step 2300 Loss 0.00865892\n",
      "Step 2400 Loss 0.00829566\n",
      "Step 2500 Loss 0.112115\n",
      "Step 2600 Loss 0.189113\n",
      "Step 2700 Loss 0.170809\n",
      "Step 2800 Loss 0.00937821\n",
      "Step 2900 Loss 0.0149727\n",
      "Step 3000 Loss 0.169534\n",
      "Step 3100 Loss 0.00752835\n",
      "Step 3200 Loss 0.260948\n",
      "Step 3300 Loss 0.334257\n",
      "Step 3400 Loss 0.0208651\n",
      "Step 3500 Loss 0.336759\n",
      "Step 3600 Loss 0.0874767\n",
      "Step 3700 Loss 0.320896\n",
      "Step 3800 Loss 0.0148729\n",
      "Step 3900 Loss 0.0134732\n",
      "The TAG dictionary is\n",
      "{'houseNumber': 0, 'streetType': 1, 'streetName': 2, 'postalCode': 3, 'city': 4, 'provCode': 5, 'area': 6}\n",
      "New data, epoch 99\n",
      "Step 0 Loss 0.455197\n",
      "Step 100 Loss 0.0125004\n",
      "Step 200 Loss 0.0122846\n",
      "Step 300 Loss 0.00796544\n",
      "Step 400 Loss 0.0099633\n",
      "Step 500 Loss 0.316097\n",
      "Step 600 Loss 0.443202\n",
      "Step 700 Loss 0.103937\n",
      "Step 800 Loss 0.222994\n",
      "Step 900 Loss 0.207984\n",
      "Step 1000 Loss 0.0243917\n",
      "Step 1100 Loss 0.0162339\n",
      "Step 1200 Loss 0.0122286\n",
      "Step 1300 Loss 0.0123751\n",
      "Step 1400 Loss 0.213231\n",
      "Step 1500 Loss 0.0253071\n",
      "Step 1600 Loss 0.0168806\n",
      "Step 1700 Loss 0.0156784\n",
      "Step 1800 Loss 0.0125862\n",
      "Step 1900 Loss 0.0108225\n",
      "Step 2000 Loss 0.0105558\n",
      "Step 2100 Loss 0.00755459\n",
      "Step 2200 Loss 0.00915277\n",
      "Step 2300 Loss 0.008682\n",
      "Step 2400 Loss 0.00831647\n",
      "Step 2500 Loss 0.112072\n",
      "Step 2600 Loss 0.189085\n",
      "Step 2700 Loss 0.170824\n",
      "Step 2800 Loss 0.00939406\n",
      "Step 2900 Loss 0.0149953\n",
      "Step 3000 Loss 0.169543\n",
      "Step 3100 Loss 0.00754285\n",
      "Step 3200 Loss 0.26101\n",
      "Step 3300 Loss 0.334449\n",
      "Step 3400 Loss 0.0208515\n",
      "Step 3500 Loss 0.336929\n",
      "Step 3600 Loss 0.0875025\n",
      "Step 3700 Loss 0.320907\n",
      "Step 3800 Loss 0.0148824\n",
      "Step 3900 Loss 0.0134736\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8e99cbf0f0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    plt.ion()\n",
    "    plt.figure()\n",
    "    plt.show()\n",
    "    loss_list = []\n",
    "\n",
    "    for epoch_idx in range(num_epochs):\n",
    "        x,y = generateData()\n",
    "        \n",
    "        _current_state = np.zeros((batch_size, state_size))\n",
    "\n",
    "        print(\"New data, epoch\", epoch_idx)\n",
    "\n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * truncated_backprop_length\n",
    "            end_idx = start_idx + truncated_backprop_length\n",
    "            \n",
    "            batchX = np.array(x)[:,start_idx:end_idx]\n",
    "            batchY = np.array(y)[:,start_idx:end_idx]\n",
    "            \n",
    "            _total_loss, _train_step, _current_state, _predictions_series = sess.run(\n",
    "                [total_loss, train_step, current_state, predictions_series],\n",
    "                feed_dict={\n",
    "                    batchX_placeholder:batchX,\n",
    "                    batchY_placeholder:batchY,\n",
    "                    init_state:_current_state\n",
    "                })\n",
    "            \n",
    "            loss_list.append(_total_loss)\n",
    "            \n",
    "            if batch_idx%100 == 0:\n",
    "                print(\"Step\",batch_idx, \"Loss\", _total_loss)\n",
    "                plot(loss_list, _predictions_series, batchX, batchY)\n",
    "            \n",
    "\n",
    "#plt.ioff()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_hmm_model_kfold_sm(smoothing=1e-3):\n",
    "    X, Y, word2idx, tag2idx = load_dataset('dataset_v4/training_hmm.csv', split_sequences=True)\n",
    "    X, Y = shuffle(X, Y)\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    print('Samples in whole dataset =', len(X), len(Y))\n",
    "    V = len(word2idx)\n",
    "    print('V =', V)\n",
    "    \n",
    "    # find hidden state transition matrix and pi\n",
    "    M = len(tag2idx)\n",
    "    print('M =', M)\n",
    "    states = set([str(i) for i in range(len(tag2idx))])\n",
    "    symbols = set([str(s) for key, s in word2idx.items()])\n",
    "    A = np.ones((M, M))*smoothing # add-one smoothing\n",
    "    pi = np.zeros(M)\n",
    "    \n",
    "    # observation matrix\n",
    "    B = np.ones((M, V))*smoothing # add-one smoothing\n",
    "    #print(B)\n",
    "    kf = KFold(n_splits = 10, shuffle = True)\n",
    "    sum_acc_train = 0\n",
    "    sum_acc_test = 0\n",
    "    sum_f1_score_train = 0\n",
    "    sum_f1_score_test = 0\n",
    "    counter = 0\n",
    "    for train_ind, test_ind in kf.split(X):\n",
    "        Xtrain, Xtest = X[train_ind], X[test_ind]\n",
    "        Ytrain, Ytest = Y[train_ind], Y[test_ind]\n",
    "        \n",
    "        for y in Ytrain:\n",
    "            pi[y[0]] += 1\n",
    "            for i in range(len(y)-1):\n",
    "                A[y[i], y[i+1]] += 1\n",
    "        \n",
    "        A /= A.sum(axis=1, keepdims=True)\n",
    "        pi /= pi.sum()\n",
    "        \n",
    "        \n",
    "        for x, y in zip(Xtrain, Ytrain):\n",
    "            for xi, yi in zip(x, y):\n",
    "                B[yi, xi] += 1    # Do Absolute discounting here \n",
    "            else:\n",
    "                B[yi, xi] = 0.001\n",
    "        B /= B.sum(axis=1, keepdims=True)\n",
    "        hmm = Model(states, symbols)\n",
    "        hmm._start_prob = pi\n",
    "        hmm._trans_prob = A\n",
    "        hmm._emit_prob = B\n",
    "\n",
    "        \n",
    "        Ptrain = []\n",
    "        for x in Xtrain:\n",
    "            x = [str(i) for i in x]\n",
    "            p = hmm.decode(np.array(x)) # F.P. this method can be used to tag a new address\n",
    "            Ptrain.append(p)\n",
    "            #print(len(Ptrain))\n",
    "        Ptest = []\n",
    "        for x in Xtest:\n",
    "            x = [str(i) for i in x]\n",
    "            p = hmm.decode(np.array(x))\n",
    "            Ptest.append(p)\n",
    "        # print results\n",
    "        sum_acc_train += accuracy(Ytrain, Ptrain)\n",
    "        sum_acc_test += accuracy(Ytest, Ptest)\n",
    "        sum_f1_score_train += total_f1_score(Ytrain, Ptrain)\n",
    "        sum_f1_score_test += total_f1_score(Ytest, Ptest)\n",
    "        counter += 1\n",
    "        print(\"RUN NOW IS: {} OF 10\".format(counter))\n",
    "    print(\"train accuracy: {}\".format(sum_acc_train/10))\n",
    "    print(\"test accuracy: {}\".format(sum_acc_test/10))\n",
    "    print(\"train F1_score: {}\".format(sum_f1_score_train/10))\n",
    "    print(\"test F1_score: {}\".format(sum_f1_score_test/10))\n",
    "    \n",
    "    return hmm, word2idx, tag2idx      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "python OpenNMT-py/preprocess.py \\\n",
    " -train_src data/src-train.txt \\\n",
    " -train_tgt data/tgt-train.txt \\\n",
    " -valid_src data/src-val.txt \\\n",
    " -valid_tgt data/tgt-val.txt \\\n",
    " -save_data data/train/preprocessed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "python -u train.py \\\n",
    " -save_model data/train/models_v2/ \\\n",
    " -data data/train/preprocessed \\\n",
    " -global_attention mlp \\\n",
    " -word_vec_size 13745 \\\n",
    " -rnn_size 512 \\\n",
    " -layers 4 \\\n",
    " -encoder_type brnn \\\n",
    " -train_steps 50000 \\\n",
    " -report_every 10000 \\\n",
    " -valid_steps 5000 \\\n",
    " -valid_batch_size 64 \\\n",
    " -max_generator_batches 128 \\\n",
    " -save_checkpoint_steps 10000 \\\n",
    " -max_grad_norm 2 \\\n",
    " -dropout 0.1 \\\n",
    " -batch_size 16 \\\n",
    " -optim adagrad \\\n",
    " -learning_rate 0.15 \\\n",
    " -start_decay_steps 10000 \\\n",
    " -decay_steps 1000 \\\n",
    " -adagrad_accumulator_init 0.1 \\\n",
    " -copy_loss_by_seqlength \\\n",
    " -bridge \\\n",
    " -seed 919 \\\n",
    " -log_file train.v2.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mkdir nmt_model\n",
    "python -m nmt.nmt \\\n",
    "    -src=en --tgt=vi \\\n",
    "    -vocab_prefix= /home/gift/Pictures/nmt/nmt/NMT_data/vocab  \\\n",
    "    -train_prefix= /home/gift/Pictures/nmt/nmt/NMT_data/train \\\n",
    "    -dev_prefix=/home/gift/Pictures/nmt/nmt/NMT_data/tst2012  \\\n",
    "    --test_prefix=/home/gift/Pictures/nmt/nmt/NMT_data/tst2013 \\\n",
    "    --out_dir=nmt_model \\\n",
    "    --num_train_steps=12000 \\\n",
    "    --steps_per_stats=100 \\\n",
    "    --num_layers=2 \\\n",
    "    --num_units=128 \\\n",
    "    --dropout=0.2 \\\n",
    "    --metrics=bleu\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "python -m nmt.nmt \\\n",
    "    --src=en --tgt=vi \\\n",
    "    --vocab_prefix=/tmp/nmt_data/vocab \\\n",
    "    --train_prefix=/tmp/nmt_data/train \\\n",
    "    --dev_prefix=/tmp/nmt_data/tst2012 \\\n",
    "    --test_prefix=/tmp/nmt_data/tst2013 \\ \n",
    "    -out_dir=/tmp/nmt_model \\\n",
    "    -num_train_steps=12000 \\   \n",
    "    -steps_per_stats=100  \\   \n",
    "    -num_layers=6 \\\n",
    "    -num_units=128 \\\n",
    "    -dropout=0.2 \\\n",
    "    -log_file train.v2.log \\\n",
    "    -metrics=bleu\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
